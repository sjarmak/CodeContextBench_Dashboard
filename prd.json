{
  "project": "CodeContextBench_Dashboard",
  "branchName": "ralph/10figure-cross-repo-tasks",
  "description": "10Figure Cross-Repo Benchmark Tasks - Expand the 10Figure benchmark with 4 cross-repository tasks testing Sourcegraph MCP cross-repo code intelligence value across Kubernetes, Envoy, Django, and TensorFlow",
  "userStories": [
    {
      "id": "US-001",
      "title": "Resolve open design questions in existing PRD",
      "description": "As a benchmark developer, I want the 4 open questions in prd-10figure-benchmark-expansion.md resolved and documented so that implementation can proceed without ambiguity.",
      "acceptanceCriteria": [
        "Open Questions section in existing PRD replaced with Resolved Decisions referencing the decisions table in this document",
        "Commit SHAs identified for all 4 repos by checking sg-benchmarks org mirrors",
        "Decisions documented in both PRDs for traceability"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Resolved 2026-01-31: All 4 open questions resolved in prd-10figure-benchmark-expansion.md. Commit SHAs: kubernetes=ef274e86, envoy=782c6cae, django=c4e07f94, tensorflow=420baf67. Patch format: single combined. Image: existing corpus-copy. Scoring: per-repo weighted."
    },
    {
      "id": "US-002",
      "title": "Multi-stage base Dockerfile for cross-repo support",
      "description": "As a benchmark runner, I want the base Docker image updated with a multi-stage build containing all 4 repos at pinned commits so that cross-repo tasks can execute in a single container.",
      "acceptanceCriteria": [
        "benchmarks/10figure/base/Dockerfile uses multi-stage build (build stage with full clones, runtime stage with source trees only)",
        "All 4 repos (Kubernetes, Django, Envoy, TensorFlow) present at /10figure/src/{repo}/",
        "Each repo checked out at the pinned commit SHA",
        "Runtime image size < 8GB (documented in build output)",
        "base/build.sh updated with new repo clone commands",
        "docker build completes without errors",
        "Typecheck passes (N/A -- Dockerfile, shell script only)"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Implemented 2026-01-31: Multi-stage Dockerfile (builder strips .git, copies only 4 target repos; runtime installs deps). build.sh validates all 4 repos, reports image size. Cannot verify docker build without Docker daemon."
    },
    {
      "id": "US-003",
      "title": "Cross-repo test script template",
      "description": "As a benchmark developer, I want a test script template that validates patches spanning multiple repos so that cross-repo tasks can be scored correctly.",
      "acceptanceCriteria": [
        "New Jinja2 template at benchmarks/10figure/templates/cross_repo_test.sh.j2",
        "Template accepts a list of repo directories to validate against",
        "Template applies the combined patch and runs validate_patch.py per repo",
        "Template computes weighted reward: sum(repo_score * repo_weight) where weight = expected_changes_in_repo / total_expected_changes",
        "Template writes single combined reward to /logs/verifier/reward.txt",
        "Template writes per-repo breakdown to /logs/verifier/validation_result.json",
        "Typecheck passes (N/A -- shell script template)"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Implemented 2026-01-31: Jinja2 template at templates/cross_repo_test.sh.j2. Uses repo_weights variable (e.g., 'kubernetes:0.6 envoy:0.4'). Splits combined patch by repo, validates per-repo, computes weighted score."
    },
    {
      "id": "US-004",
      "title": "Task package for cross_api_tracing_01",
      "description": "As a benchmark runner, I want the API Contract Tracing task (Kubernetes + Envoy) packaged as a Harbor-compatible task directory.",
      "acceptanceCriteria": [
        "Directory benchmarks/10figure/cross_api_tracing_01/ created with: instruction.md, task.toml, task.yaml, environment/Dockerfile, tests/test.sh, tests/expected_changes.json",
        "instruction.md describes the appProtocol field tracing task with clear success criteria",
        "task.toml lists both kubernetes and envoy in repo metadata",
        "task.yaml contains ground truth symbol chain for scoring",
        "environment/Dockerfile inherits from harbor-10figure:base",
        "tests/test.sh validates output against expected symbol chain in both repos",
        "Task passes structural validation (valid TOML, valid YAML, Dockerfile builds)",
        "Typecheck passes (N/A -- config and shell files only)"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Implemented 2026-01-31: Traces appProtocol from K8s ServicePort/EndpointPort through to Envoy FilterChainMatch/ClusterProtocolSelection. 7 ground truth symbols across both repos."
    },
    {
      "id": "US-005",
      "title": "Task package for cross_dependency_impact_01",
      "description": "As a benchmark runner, I want the Dependency Impact Analysis task (Django + TensorFlow) packaged as a Harbor-compatible task directory.",
      "acceptanceCriteria": [
        "Directory benchmarks/10figure/cross_dependency_impact_01/ created with standard task structure",
        "instruction.md describes the JSON serialization impact analysis task",
        "task.toml lists both django and tensorflow in repo metadata",
        "task.yaml contains ground truth: affected files and functions in both repos",
        "tests/test.sh scores completeness of identified impact sites (partial credit for subset)",
        "Task passes structural validation",
        "Typecheck passes (N/A -- config and shell files only)"
      ],
      "priority": 5,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-006",
      "title": "Task package for cross_bug_localization_01",
      "description": "As a benchmark runner, I want the Cross-Project Bug Localization task (Kubernetes + Django) packaged as a Harbor-compatible task directory.",
      "acceptanceCriteria": [
        "Directory benchmarks/10figure/cross_bug_localization_01/ created with standard task structure",
        "instruction.md describes the race condition pattern with K8s error trace",
        "task.toml lists both kubernetes and django in repo metadata",
        "task.yaml contains ground truth: bug locations and expected fix patterns in both repos",
        "tests/test.sh validates patch fixes the bug in both repos using the cross-repo template",
        "Per-repo scoring weights defined in expected_changes.json (proportional to change count)",
        "Task passes structural validation",
        "Typecheck passes (N/A -- config and shell files only)"
      ],
      "priority": 6,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-007",
      "title": "Task package for cross_refactor_01",
      "description": "As a benchmark runner, I want the Multi-Repo Refactor task (Envoy + Kubernetes) packaged as a Harbor-compatible task directory.",
      "acceptanceCriteria": [
        "Directory benchmarks/10figure/cross_refactor_01/ created with standard task structure",
        "instruction.md describes the protobuf field rename with clear scope",
        "task.toml lists both envoy and kubernetes in repo metadata",
        "task.yaml contains ground truth: all proto field references in both repos",
        "tests/test.sh validates rename completeness across both repos using cross-repo template",
        "Per-repo scoring weights defined (Envoy weighted higher as proto source)",
        "Task passes structural validation",
        "Typecheck passes (N/A -- config and shell files only)"
      ],
      "priority": 7,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-008",
      "title": "Per-repo weighted scoring in validate_patch.py",
      "description": "As a benchmark developer, I want validate_patch.py to support per-repo weighted scoring so that cross-repo tasks produce a single reward that reflects contribution from each repo.",
      "acceptanceCriteria": [
        "validate_patch.py accepts a --weights argument (JSON mapping repo name to weight, e.g., {\"kubernetes\": 0.6, \"envoy\": 0.4})",
        "When --weights is provided, scoring is computed per-repo and combined: reward = sum(repo_reward * weight)",
        "When --weights is omitted, behavior is unchanged (single-repo scoring)",
        "Weights must sum to 1.0; script exits with error if they don't",
        "Output JSON includes both overall_score and per_repo_scores fields",
        "Typecheck passes (if Python with type hints)",
        "Unit tests cover: single-repo fallback, two-repo weighted, weight validation error"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Implemented 2026-01-31 as validate_patch_cross_repo.py (wraps base validator). 11 unit tests pass covering all AC scenarios."
    },
    {
      "id": "US-009",
      "title": "Smoke test for cross-repo tasks",
      "description": "As a benchmark developer, I want a smoke test that validates all cross-repo task packages have correct structure and build successfully.",
      "acceptanceCriteria": [
        "tests/smoke_test_cross_repo.py validates each cross-repo task directory",
        "Checks: instruction.md exists and references multiple repos, task.toml is valid TOML with multi-repo metadata, task.yaml is valid YAML with ground truth, environment/Dockerfile exists and references base image, tests/test.sh exists and is executable",
        "Checks that expected_changes.json contains entries for 2+ repos",
        "Checks that scoring weights sum to 1.0",
        "Script exits 0 if all checks pass, non-zero with details on failure",
        "Typecheck passes",
        "Can be run via python -m pytest tests/smoke_test_cross_repo.py"
      ],
      "priority": 8,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-010",
      "title": "Nop agent validation run",
      "description": "As a benchmark developer, I want to run the nop agent against all 4 cross-repo tasks to verify the Harbor framework executes and scores them correctly (expected reward: 0.0).",
      "acceptanceCriteria": [
        "All 4 cross-repo tasks execute via Harbor with the nop agent without errors",
        "Each task produces reward.txt with value 0.0 (nop agent produces no output)",
        "Each task produces validation_result.json with per-repo breakdown",
        "No container build failures or missing file errors",
        "Results documented in benchmarks/10figure/VALIDATION.md"
      ],
      "priority": 9,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-011",
      "title": "Baseline vs MCP agent validation run",
      "description": "As a benchmark developer, I want to run both a baseline agent and an MCP agent against all 4 cross-repo tasks to verify scoring differentiates between them.",
      "acceptanceCriteria": [
        "Baseline agent (no MCP) run completes on all 4 cross-repo tasks",
        "MCP agent (Sourcegraph) run completes on all 4 cross-repo tasks",
        "MCP agent achieves higher average reward than baseline on cross-repo tasks",
        "Per-repo scoring breakdown shows MCP advantage is larger in the non-primary repo (the one requiring cross-repo search)",
        "Results documented with raw rewards and statistical summary"
      ],
      "priority": 10,
      "passes": false,
      "notes": ""
    }
  ]
}
