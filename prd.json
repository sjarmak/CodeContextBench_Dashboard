{
  "project": "CodeContextBench",
  "branchName": "ralph/analysis-pipeline-redesign",
  "description": "Analysis Pipeline Redesign - CLI pipeline from raw runs to publication-ready artifacts + simplified 5-view dashboard for the CodeContextBench paper",
  "userStories": [
    {
      "id": "US-001",
      "title": "Add AgentConfig enum and .mcp.json config detector",
      "description": "As a researcher, I want the pipeline to auto-detect Baseline/MCP-Base/MCP-Full from .mcp.json so I don't need manual mapping.",
      "acceptanceCriteria": [
        "New module src/ingest/config_detector.py with detect_agent_config(trial_dir: Path) -> AgentConfig",
        "AgentConfig is a string enum with values BASELINE, MCP_BASE, MCP_FULL",
        "Detection: no .mcp.json or empty servers list = BASELINE; has sourcegraph server but no deepsearch endpoint = MCP_BASE; has deepsearch endpoint in URL = MCP_FULL",
        "Checks both {trial}/agent/.mcp.json and {trial}/agent/sessions/.mcp.json",
        "Falls back to folder name convention (baseline/ deepsearch/ sourcegraph/) if .mcp.json missing",
        "All dataclasses are frozen/immutable",
        "Unit tests for each config type plus missing file and malformed JSON edge cases",
        "Typecheck passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-002",
      "title": "Build tool utilization extractor from claude-code.txt JSONL",
      "description": "As a researcher, I want to extract MCP tool call frequency, search queries, Deep Search vs keyword ratio, and context window fill rate from claude-code.txt.",
      "acceptanceCriteria": [
        "New module src/ingest/transcript_tool_extractor.py",
        "Parses JSONL events from claude-code.txt line by line",
        "Extracts tool call counts by name and by category (MCP/DEEP_SEARCH/LOCAL/OTHER) using existing categorization from transcript_parser.py",
        "Extracts search query text from MCP tool calls (keyword_search, nls_search, deep_search parameters)",
        "Computes Deep Search vs keyword search ratio per trial",
        "Computes context window fill rate as cumulative input tokens / 200000 (Claude Haiku 4.5 context limit)",
        "Returns frozen dataclass ToolUtilizationMetrics with all fields",
        "Skips malformed JSONL lines with logged warning, does not raise",
        "Unit tests with sample JSONL fixture covering tool_use, tool_result, and system init events",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-003",
      "title": "Build unified metrics extraction pipeline (extract.py)",
      "description": "As a researcher, I want a single script that reads a runs directory and produces experiment_metrics.json with all metrics per trial.",
      "acceptanceCriteria": [
        "New module scripts/ccb_pipeline/extract.py with __main__ entry point",
        "Walks runs directory (official/, experiment/, troubleshooting/ subfolders)",
        "For each trial discovers: reward.txt or result.json for reward, result.json for timing, task_metrics.json or result.json for tokens",
        "Calls detect_agent_config() from US-001 for config classification",
        "Calls ToolUtilizationMetrics extractor from US-002 for tool usage",
        "Outputs experiment_metrics.json with schema: {run_category, experiments: [{experiment_id, trials: [{trial_id, task_name, benchmark, agent_config, metrics: {reward, pass_fail, duration_seconds, input_tokens, output_tokens, cached_tokens, tool_utilization}}]}]}",
        "Skips incomplete trials (missing result.json) with warning to stderr",
        "CLI: python -m scripts.ccb_pipeline.extract --runs-dir <path> --output <path>",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-004",
      "title": "Build benchmark-to-SDLC phase mapping",
      "description": "As a researcher, I want a static mapping from benchmark names to SDLC phases so the analysis can group by phase.",
      "acceptanceCriteria": [
        "New module scripts/ccb_pipeline/sdlc_mapping.py",
        "Dict mapping benchmark name patterns to SDLC phases from Table 2 of the paper",
        "Covers: ccb-kubernetes-docs -> Documentation, ccb-pytorch-issues -> Implementation, ccb-crossrepo -> Implementation+Maintenance, ccb-largerepo -> Implementation, swe-bench-pro -> Implementation+Testing, locobench -> Implementation+Code Review, repoqa -> Requirements, di-bench -> Maintenance, swe-perf -> Implementation+Testing, dependeval -> Maintenance, the-agent-company -> Multiple",
        "Function get_sdlc_phases(benchmark_name: str) -> list[str] with fuzzy matching on benchmark name",
        "Function get_benchmark_name(task_path: str) -> str that extracts benchmark from task path",
        "Typecheck passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-005",
      "title": "Build statistical analysis step (analyze.py) - aggregate + pairwise",
      "description": "As a researcher, I want the pipeline to compute aggregate metrics per config and pairwise statistical tests for paper sections 4.1 and 4.3.",
      "acceptanceCriteria": [
        "New module scripts/ccb_pipeline/analyze.py with __main__ entry point",
        "Consumes experiment_metrics.json from US-003",
        "Computes per-configuration (Baseline/MCP-Base/MCP-Full): mean reward, pass rate, median duration, mean input tokens, mean output tokens",
        "Includes standard error for mean reward and pass rate",
        "Runs pairwise statistical tests for all 3 pairs using scipy (t-test for continuous, proportion z-test for pass rate)",
        "Computes effect sizes (Cohen's d for continuous metrics)",
        "Outputs analysis_results.json with sections: aggregate_metrics, pairwise_tests, effect_sizes",
        "CLI: python -m scripts.ccb_pipeline.analyze --input experiment_metrics.json --output analysis_results.json",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-006",
      "title": "Add per-benchmark and per-SDLC breakdown to analyze.py",
      "description": "As a researcher, I want performance broken down by benchmark and SDLC phase for paper section 4.2.",
      "acceptanceCriteria": [
        "analyze.py groups trials by benchmark name using get_benchmark_name() from US-004",
        "Maps benchmarks to SDLC phases using get_sdlc_phases() from US-004",
        "Computes per-benchmark: pass rate per config, mean reward per config, pairwise significance tests",
        "Computes per-SDLC-phase: aggregated pass rate per config, mean reward delta",
        "Flags benchmarks/phases where MCP shows statistically significant improvement (p < 0.05)",
        "Results added to analysis_results.json under per_benchmark and per_sdlc_phase sections",
        "Typecheck passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-007",
      "title": "Add efficiency analysis to analyze.py",
      "description": "As a researcher, I want token consumption and timing trade-off analysis for paper section 4.4.",
      "acceptanceCriteria": [
        "analyze.py computes per-configuration: total tokens (input/output/cached), input-to-output ratio, median wall-clock time",
        "Computes tokens-per-success (total tokens / number of passing tasks) per config",
        "Computes MCP token overhead (mean tokens for MCP configs minus mean tokens for baseline)",
        "Includes standard error for all means",
        "Results added to analysis_results.json under efficiency section",
        "Typecheck passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-008",
      "title": "Add tool utilization analysis to analyze.py",
      "description": "As a researcher, I want tool usage pattern analysis for paper section 4.5.",
      "acceptanceCriteria": [
        "analyze.py computes per-configuration: mean MCP calls, mean Deep Search calls, Deep Search vs keyword ratio",
        "Computes correlation between MCP tool call count and task reward (Pearson r with p-value)",
        "Computes mean context window fill rate per config",
        "Groups tool usage by benchmark to identify where MCP is most/least used",
        "Results added to analysis_results.json under tool_utilization section",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-009",
      "title": "Build LaTeX table generator (publish.py - tables)",
      "description": "As a researcher, I want the pipeline to produce LaTeX tables I can paste into the paper.",
      "acceptanceCriteria": [
        "New module scripts/ccb_pipeline/publish.py with __main__ entry point",
        "Consumes analysis_results.json",
        "Generates table_aggregate.tex: config | pass rate | mean reward | median duration | mean tokens with SE",
        "Generates table_per_benchmark.tex: benchmark | config | pass rate | reward | significance marker",
        "Generates table_significance.tex: pair | metric | p-value | effect size | interpretation",
        "Generates table_efficiency.tex: config | input tokens | output tokens | tokens per success | fill rate",
        "Tables use booktabs package formatting (toprule/midrule/bottomrule)",
        "All tables written to output/tables/ directory (created if not exists)",
        "CLI: python -m scripts.ccb_pipeline.publish --input analysis_results.json --output-dir output/",
        "Typecheck passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-010",
      "title": "Build matplotlib figure generator (publish.py - figures)",
      "description": "As a researcher, I want publication-quality figures exported as PDF and SVG.",
      "acceptanceCriteria": [
        "publish.py generates fig_pass_rate.pdf/svg: grouped bar chart of pass rate by config (3 bars) with SE error bars",
        "publish.py generates fig_benchmark_heatmap.pdf/svg: heatmap of reward delta (MCP-Full minus Baseline) per benchmark",
        "publish.py generates fig_token_overhead.pdf/svg: scatter plot of token overhead vs reward improvement per task",
        "publish.py generates fig_tool_utilization.pdf/svg: stacked bar chart of tool call distribution by category per config",
        "All figures use seaborn-v0_8-paper style, colorblind-friendly palette (tableau-colorblind10), no emojis",
        "Figures exported as both PDF and SVG to output/figures/",
        "Typecheck passes"
      ],
      "priority": 10,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-011",
      "title": "Build LLM judge pipeline step (judge.py)",
      "description": "As a researcher, I want the pipeline to optionally run LLM judge evaluation with a 3-model voting ensemble.",
      "acceptanceCriteria": [
        "New module scripts/ccb_pipeline/judge.py with __main__ entry point",
        "Runs LLM judge with default rubric template per benchmark type",
        "Uses 3-model voting ensemble: GPT-5.2, Claude Sonnet, Gemini (3 rounds each)",
        "Reads trial agent output from claude-code.txt or agent solution files",
        "Writes judge_scores to experiment_metrics.json under judge_scores field per trial",
        "CLI flags: --skip-judge to skip, --judge-template <path> to override default template",
        "Handles API errors gracefully (retry with backoff, log failures, continue to next task)",
        "CLI: python -m scripts.ccb_pipeline.judge --input experiment_metrics.json --runs-dir <path>",
        "Typecheck passes"
      ],
      "priority": 11,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-012",
      "title": "Build end-to-end pipeline CLI (__main__.py)",
      "description": "As a researcher, I want one command that chains extract -> judge -> analyze -> publish.",
      "acceptanceCriteria": [
        "New module scripts/ccb_pipeline/__main__.py",
        "CLI: python -m scripts.ccb_pipeline --runs-dir <path> --output-dir output/ [--skip-judge] [--judge-template <path>]",
        "Runs stages in order: extract -> judge (unless --skip-judge) -> analyze -> publish",
        "Each stage reads from previous stage output in output-dir",
        "Prints progress: 'Stage 1/4: Extracting metrics...' etc.",
        "Prints summary on completion: trials processed, configs detected, artifacts generated",
        "Non-zero exit code on failure with clear error message",
        "Each stage also runnable independently via its own __main__",
        "Typecheck passes"
      ],
      "priority": 12,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-013",
      "title": "Simplify dashboard app.py to 5 views",
      "description": "As a researcher, I want the dashboard reduced from 14 views to 5: Home, Results Explorer, Comparison, LLM Judge, Export.",
      "acceptanceCriteria": [
        "Update dashboard/app.py navigation to show exactly 5 sidebar buttons: Home, Results Explorer, Comparison, LLM Judge, Export",
        "Remove imports and routing for: benchmark_manager, deep_search, analysis_hub, analysis_ir, analysis_failure, analysis_cost, analysis_statistical, analysis_timeseries, analysis_comparison",
        "Home page retains run scanner showing official/experiment/troubleshooting groups with task counts",
        "Add 'Run Analysis Pipeline' button on Home that triggers scripts.ccb_pipeline via subprocess",
        "Show pipeline progress and output path when complete",
        "Existing view files NOT deleted (just unlinked from navigation for now)",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 13,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-014",
      "title": "Build Results Explorer dashboard view",
      "description": "As a researcher, I want a single view to explore per-trial results with filtering by benchmark, config, and outcome.",
      "acceptanceCriteria": [
        "New view dashboard/views/results_explorer.py replacing run_results + task_comparison",
        "Loads experiment_metrics.json from output/ directory",
        "Filter bar in columns: benchmark multiselect, config multiselect (Baseline/MCP-Base/MCP-Full), outcome (pass/fail/partial)",
        "Results table: task, benchmark, config, reward, duration, input tokens, output tokens, judge score (if available)",
        "Click row to expand with st.expander: shows tool call summary and file change list from the trial",
        "CSV export button for filtered results",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 14,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-015",
      "title": "Build Comparison dashboard view with paper section tabs",
      "description": "As a researcher, I want an interactive comparison view with tabs matching paper sections 4.1-4.5.",
      "acceptanceCriteria": [
        "New view dashboard/views/comparison_analysis.py",
        "Loads analysis_results.json from output/ directory",
        "5 tabs: Aggregate (4.1), Per-Benchmark (4.2), Config Sensitivity (4.3), Efficiency (4.4), Tool Utilization (4.5)",
        "Aggregate tab: metrics table with config rows, Plotly bar chart of pass rates with error bars",
        "Per-Benchmark tab: heatmap of reward deltas, per-benchmark table with significance markers",
        "Config Sensitivity tab: pairwise test results table, effect size visualization",
        "Efficiency tab: token distribution bar chart, tokens-per-success comparison",
        "Tool Utilization tab: tool call distribution chart, correlation scatter plot",
        "Export button (PNG for charts, CSV for tables) on each tab",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 15,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-016",
      "title": "Build simplified LLM Judge dashboard view",
      "description": "As a researcher, I want a view to review judge results and re-run with different templates for specific tasks.",
      "acceptanceCriteria": [
        "New view dashboard/views/judge_viewer.py",
        "Loads judge scores from experiment_metrics.json",
        "Aggregated scores table: benchmark, config, mean judge score, vote confidence",
        "Per-task breakdown with expandable rows showing vote distribution per dimension",
        "Template selector dropdown loading from configs/judge_templates/",
        "Re-run Judge button for selected tasks with chosen template (calls judge.py via subprocess)",
        "Side-by-side score comparison if multiple template runs exist",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 16,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-017",
      "title": "Build Export dashboard view",
      "description": "As a researcher, I want a view to browse and download all publication artifacts.",
      "acceptanceCriteria": [
        "New view dashboard/views/export_view.py",
        "Lists files in output/tables/ and output/figures/ with file size and modification time",
        "Inline preview for figures (st.image for SVG/PNG)",
        "Inline preview for .tex files (rendered in st.code block with latex syntax highlighting)",
        "Download button per file using st.download_button",
        "Download All button that creates a zip archive of output/ and serves it",
        "Shows 'No artifacts found - run the pipeline first' if output/ is empty",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 17,
      "passes": false,
      "notes": ""
    }
  ]
}
