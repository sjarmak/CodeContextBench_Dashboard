{
  "project": "CodeContextBench",
  "branchName": "ralph/locobench-agent-adapter",
  "description": "LoCoBench-Agent Harbor Adapter - Select ~50 high-complexity tasks and create adapter for benchmark execution",
  "userStories": [
    {
      "id": "US-001",
      "title": "Download LoCoBench-Agent dataset",
      "description": "As a developer, I need to download the LoCoBench-Agent data from Google Drive so I can analyze its structure.",
      "acceptanceCriteria": [
        "Download LoCoBench-Agent data using gdown from https://drive.google.com/uc?id=1HwPztd0bipUUi8zs7Pxo3StZCOnJBwVR",
        "Unzip data.zip to benchmarks/locobench_agent/data/",
        "Verify data directory exists with expected contents"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Data manually downloaded and extracted. Located at benchmarks/locobench_agent/data/ with subfolders: generated/ (1000 synthetic projects), output/scenarios/ (8000 task JSON files)"
    },
    {
      "id": "US-002",
      "title": "Explore and document dataset structure",
      "description": "As a developer, I need to understand the LoCoBench-Agent data format so I can build the extraction script.",
      "acceptanceCriteria": [
        "Create benchmarks/locobench_agent/DATA_EXPLORATION.md documenting directory structure",
        "Document that scenarios are in data/output/scenarios/*.json (8000 files)",
        "Document key fields: id, task_category, difficulty, title, description, context_files, context_length, task_prompt, ground_truth, evaluation_criteria",
        "Document the 8 task categories: architectural_understanding, bug_investigation, code_comprehension, cross_file_refactoring, feature_implementation, integration_testing, multi_session_development, security_analysis",
        "Include sample scenario JSON showing actual field names and types"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Created DATA_EXPLORATION.md documenting dataset structure, scenarios format, 8 task categories, 10 languages, and key fields for task selection."
    },
    {
      "id": "US-003",
      "title": "Define task selection criteria",
      "description": "As a benchmark designer, I want documented criteria for selecting tasks that maximize MCP value demonstration.",
      "acceptanceCriteria": [
        "Create benchmarks/locobench_agent/docs/TASK_SELECTION_CRITERIA.md",
        "Define quantitative metrics: min context_length (>50K tokens), min files_count (>5)",
        "Define scoring weights: context_length (0.3), files_count (0.3), task_category_bonus (0.4)",
        "Define task category bonuses: architectural_understanding=1.0, cross_file_refactoring=0.9, bug_investigation=0.8, security_analysis=0.7, feature_implementation=0.5",
        "Document that selection is complexity-driven, not language-driven"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Created TASK_SELECTION_CRITERIA.md with scoring formula, weights, category bonuses, and complexity-driven selection philosophy."
    },
    {
      "id": "US-004",
      "title": "Create dataset extraction script",
      "description": "As a developer, I need to extract LoCoBench-Agent scenarios into normalized JSONL format.",
      "acceptanceCriteria": [
        "Create benchmarks/locobench_agent/extract_dataset.py",
        "Script reads all JSON files from data/output/scenarios/*.json",
        "Outputs normalized JSONL with fields: id, task_category, difficulty, title, context_length, files_count, task_prompt, ground_truth, evaluation_criteria, language (parsed from id)",
        "Handles missing fields gracefully with sensible defaults",
        "Outputs to benchmarks/locobench_agent/locobench_dataset.jsonl",
        "Typecheck passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Created extract_dataset.py that reads all 8000 scenarios, normalizes fields, parses language from ID, and outputs to locobench_dataset.jsonl. Successfully extracted all scenarios with category/language distribution stats."
    },
    {
      "id": "US-005",
      "title": "Implement task scoring and selection script",
      "description": "As a benchmark designer, I need to score and rank tasks to select the top 50 by MCP value.",
      "acceptanceCriteria": [
        "Create benchmarks/locobench_agent/select_tasks.py",
        "Implement scoring function using weights from TASK_SELECTION_CRITERIA.md",
        "Score based on: context_length, files_count, task_category bonus",
        "Output ranked list with scores to benchmarks/locobench_agent/selected_tasks.json",
        "Select top 50 tasks and log selection rationale to stdout",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Created select_tasks.py implementing scoring formula from criteria docs. Selected top 50 tasks: 34 architectural_understanding, 13 cross_file_refactoring, 3 bug_investigation. Average context: 968K tokens, average files: 81. Score range: 0.87-0.98."
    },
    {
      "id": "US-006",
      "title": "Create Harbor task templates",
      "description": "As a developer, I need templates for consistent Harbor task generation.",
      "acceptanceCriteria": [
        "Create benchmarks/locobench_agent/templates/task.toml with metadata placeholders",
        "Create benchmarks/locobench_agent/templates/instruction.md with task description format",
        "Create benchmarks/locobench_agent/templates/environment/Dockerfile that copies synthetic project files",
        "Create benchmarks/locobench_agent/templates/tests/test.sh that invokes verifier",
        "Templates use {placeholder} syntax for: id, task_category, difficulty, context_length, task_prompt"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Created all templates: task.toml (metadata with placeholders), instruction.md (task description format), Dockerfile (multi-language support for all 10 LoCoBench languages), test.sh (invokes verify.py), verify.py (keyword-based semantic similarity verifier)."
    },
    {
      "id": "US-007",
      "title": "Create LoCoBenchTask dataclass and loader",
      "description": "As a developer, I need data models to represent LoCoBench tasks.",
      "acceptanceCriteria": [
        "Create benchmarks/locobench_agent/adapter.py",
        "Implement LoCoBenchTask dataclass with fields: id, task_category, difficulty, title, description, context_files, context_length, task_prompt, ground_truth, evaluation_criteria, language",
        "Implement LoCoBenchTask.from_dict() class method",
        "Implement LoCoBenchLoader class to load from JSONL file",
        "Implement LoCoBenchLoader.load(), all_ids(), filter_by_task_category(), filter_by_language() methods",
        "Typecheck passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Created adapter.py with LoCoBenchTask dataclass (all 11 fields), from_dict() class method, and LoCoBenchLoader with load(), all_ids(), filter_by_task_category(), filter_by_language(), filter_by_difficulty(), and get_all_tasks() methods. Successfully tested loading all 8000 tasks."
    },
    {
      "id": "US-008",
      "title": "Implement LoCoBenchAdapter class",
      "description": "As a developer, I need the adapter class that generates Harbor task directories.",
      "acceptanceCriteria": [
        "Add LoCoBenchAdapter class to benchmarks/locobench_agent/adapter.py",
        "Import Harbor models (TaskConfig, TaskPaths) using dynamic import pattern from repoqa/dibench",
        "Implement _render_template() for placeholder substitution",
        "Implement _create_instruction() to generate instruction.md content",
        "Implement _prepare_task_from_template() to create full task directory structure",
        "Implement generate_task(task_id, local_task_id) public method",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Implemented LoCoBenchAdapter with _render_template(), _create_instruction(), _prepare_task_from_template(), generate_task(). Imports Harbor TaskConfig and TaskPaths from installed package. LoCoBenchLoader supplements missing JSONL fields from raw scenario files. Added expected_approach field to LoCoBenchTask dataclass."
    },
    {
      "id": "US-009",
      "title": "Create run_adapter.py CLI",
      "description": "As a developer, I need a CLI to generate Harbor tasks from selected LoCoBench tasks.",
      "acceptanceCriteria": [
        "Create benchmarks/locobench_agent/run_adapter.py",
        "Support arguments: --dataset_path, --output_dir, --task_ids (optional), --limit (optional)",
        "Load tasks from dataset_path JSONL file",
        "Generate Harbor task directories to output_dir",
        "Log progress: [N/M] task_id for each generated task",
        "Handle and log errors gracefully without stopping entire run",
        "Typecheck passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Created run_adapter.py CLI with argparse. Supports --dataset_path, --output_dir, --task_ids, --limit, --selected_tasks (bonus), --data_dir. Logs progress as [N/M] task_id, handles errors gracefully continuing with next task. Tested successfully generating tasks from both direct IDs and selected_tasks.json."
    },
    {
      "id": "US-010",
      "title": "Generate 50 selected tasks",
      "description": "As a developer, I need to run the adapter to create the 50 task directories.",
      "acceptanceCriteria": [
        "Run extract_dataset.py to create locobench_dataset.jsonl",
        "Run select_tasks.py to create selected_tasks.json with top 50 tasks",
        "Run run_adapter.py with selected task IDs to generate task directories",
        "Tasks saved to benchmarks/locobench_agent/tasks/",
        "Each task directory contains: instruction.md, task.toml, tests/ground_truth.json, tests/test.sh, environment/Dockerfile",
        "No generation errors for selected tasks"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Generated all 50 selected tasks to benchmarks/locobench_agent/tasks/. Each task has: instruction.md, task.toml, tests/ground_truth.json, tests/test.sh, tests/verify.py, tests/task_metadata.json, environment/Dockerfile, environment/project/. No generation errors occurred."
    },
    {
      "id": "US-011",
      "title": "Smoke test with baseline agent",
      "description": "As a developer, I need to validate the adapter works by running 1-2 tasks with the baseline agent.",
      "acceptanceCriteria": [
        "Select 2 diverse tasks from the 50 (different task categories)",
        "Run harbor run with baseline agent on each task",
        "Tasks execute without adapter/framework errors (agent success/failure is acceptable)",
        "Create benchmarks/locobench_agent/SMOKE_TEST_RESULTS.md documenting: tasks tested, commands run, results, any issues found"
      ],
      "priority": 11,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-012",
      "title": "Create benchmark documentation",
      "description": "As a developer, I need documentation so others can understand and use the LoCoBench adapter.",
      "acceptanceCriteria": [
        "Create benchmarks/locobench_agent/README.md with overview, setup instructions, and usage examples",
        "Create benchmarks/locobench_agent/DESIGN.md explaining adapter architecture and data flow",
        "Include example commands for: extracting data, selecting tasks, generating tasks, running with harbor",
        "Document task selection criteria summary and MCP value hypothesis"
      ],
      "priority": 12,
      "passes": false,
      "notes": ""
    }
  ]
}
