{
  "project": "CodeContextBench Dashboard",
  "branchName": "ralph/dashboard-filtering-analytics",
  "description": "Dashboard Filtering and Analytics Enhancement - Enable cross-config comparison, task filtering, and pattern discovery across baseline/sourcegraph_base/sourcegraph_full experiment configs",
  "userStories": [
    {
      "id": "US-001",
      "title": "Create comparison data loader",
      "description": "As a dashboard backend, I need to load task_metrics.json files for all 3 configs (baseline, sourcegraph_base, sourcegraph_full) and join on (benchmark, task_id) so comparison views can work.",
      "acceptanceCriteria": [
        "Create new file dashboard/utils/comparison_loader.py",
        "Implement load_comparison_data(runs_dir: Path) that walks runs/official/<run_name>/<config>/<batch_timestamp>/<task_id__hash>/task_metrics.json",
        "Config directory names are: 'baseline', 'sourcegraph_base', 'sourcegraph_full' (use _KNOWN_MODE_DIRS pattern from home_run_scanner.py)",
        "runs_dir comes from CCB_EXTERNAL_RUNS_DIR env var or defaults to ~/evals/custom_agents/agents/claudecode/runs (same as home_run_scanner.py)",
        "Join on (benchmark, task_id) â€” return dict: {task_id: {'baseline': dict, 'sourcegraph_base': dict, 'sourcegraph_full': dict}}",
        "Handle missing tasks in any config gracefully (set to None)",
        "Handle null/None values using data.get('key') or default pattern (per CLAUDE.md gotcha about dict.get returning None)",
        "Cache result in st.session_state['comparison_data'] to avoid re-reading files on page interaction",
        "Typecheck passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Follow patterns from dashboard/utils/home_run_scanner.py for directory walking and RunSummary. Use json.loads(path.read_text()) for file reading. Config names must match _KNOWN_MODE_DIRS exactly."
    },
    {
      "id": "US-002",
      "title": "Create comparison metrics computation",
      "description": "As a dashboard view, I need computed metrics (benefit_pct, efficiency, error_rate) derived from paired task_metrics data without modifying source data.",
      "acceptanceCriteria": [
        "Create new file dashboard/utils/comparison_metrics.py",
        "Implement compute_mcp_benefit(baseline: dict, variant: dict) returning dict with: benefit_pct, token_cost_pct, efficiency",
        "benefit_pct = (variant_reward - baseline_reward) / max(baseline_reward, 0.001) * 100",
        "token_cost_pct = (variant_tokens - baseline_tokens) / max(baseline_tokens, 1) * 100 where tokens = input_tokens + output_tokens",
        "efficiency = benefit_pct / max(token_cost_pct, 1)",
        "All divisions use max(denominator, threshold) to avoid divide-by-zero",
        "Return None for any metric where inputs are None",
        "Implement compute_task_ratios(task: dict) returning dict with: tool_error_rate_pct, backtrack_ratio, execution_phase_breakdown",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "All task_metrics.json fields can be None. Use immutable return dicts (don't mutate inputs). Reward field is 'reward', token fields are 'input_tokens' and 'output_tokens'."
    },
    {
      "id": "US-003",
      "title": "Create comparison filter functions",
      "description": "As a dashboard filter, I need to filter comparison task data by category, difficulty, size, error type, and search strategy.",
      "acceptanceCriteria": [
        "Create new file dashboard/utils/comparison_filters.py",
        "Implement filter_comparison_tasks(tasks: dict, benchmark=None, category=None, difficulty=None, size_bucket=None, error_severity=None, search_strategy_type=None) returning filtered dict",
        "All filter params optional (None = no filter on that dimension)",
        "size_bucket bucketing: small (<300K), medium (300K-1M), large (>1M) using task_context_length field, fall back to instruction_length_chars if task_context_length is None",
        "Multi-value filters: category=['architectural_understanding', 'feature_implementation'] includes tasks matching either (OR within dimension)",
        "AND across dimensions (e.g., category=X AND difficulty=Y)",
        "Return new dict without modifying input (immutable pattern per coding style rules)",
        "Reference existing patterns in dashboard/utils/filters.py and dashboard/utils/filter_ui.py for consistency",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Existing dashboard/utils/filters.py has filtering patterns for task metadata. Category values include: architectural_understanding, bug_investigation, cross_file_refactoring, feature_implementation. Difficulty values: easy, medium, hard, expert."
    },
    {
      "id": "US-004",
      "title": "Create config comparison view with raw values table",
      "description": "As a researcher, I want to see baseline vs sourcegraph_base vs sourcegraph_full metrics side-by-side for the same task so I can evaluate MCP impact directly.",
      "acceptanceCriteria": [
        "Create new file dashboard/views/config_comparison.py with function show_config_comparison()",
        "Register the view in dashboard/app.py: add 'Config Comparison' to nav_results list and add routing in the if/elif page chain",
        "Use load_comparison_data() from dashboard/utils/comparison_loader.py to load data",
        "Display st.dataframe with columns: task_id, benchmark, baseline_reward, sg_base_reward, sg_full_reward, baseline_tokens, sg_base_tokens, sg_full_tokens",
        "Use agent display names from dashboard/utils/agent_labels.py AGENT_DISPLAY_NAMES for column headers",
        "Display dash character for null values in the dataframe",
        "Use st.columns() for layout (avoid st.sidebar per CLAUDE.md gotcha about sidebar content being hidden)",
        "Prefix all widget keys with 'ccmp_' to avoid StreamlitDuplicateElementKey errors",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Follow existing view patterns: see dashboard/views/task_comparison.py for reference. Import pattern: 'from dashboard.utils.comparison_loader import load_comparison_data'. App.py routing uses dynamic imports: 'from views.config_comparison import show_config_comparison'."
    },
    {
      "id": "US-005",
      "title": "Add computed effectiveness columns to comparison table",
      "description": "As a researcher, I want to see how much MCP helped (or hurt) on each task via benefit_pct, token_cost_pct, and efficiency columns.",
      "acceptanceCriteria": [
        "Import compute_mcp_benefit from dashboard/utils/comparison_metrics.py in config_comparison.py",
        "Add computed columns to dataframe: sg_base_benefit_pct, sg_full_benefit_pct, sg_full_token_cost_pct, sg_full_efficiency",
        "Format percentages to 1 decimal (e.g., '+41.2%', '-5.3%')",
        "Display dash character for null/missing computed values",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 5,
      "passes": true,
      "notes": "compute_mcp_benefit returns None for metrics where inputs are None. Format in the view layer only (keep raw floats in the data)."
    },
    {
      "id": "US-006",
      "title": "Add anomaly highlighting to comparison table",
      "description": "As an analyst, I want to visually identify tasks where MCP had outsized impact or regressions so pattern-finding is faster.",
      "acceptanceCriteria": [
        "Use Pandas Styler or st.dataframe column_config to apply row background colors based on sg_full_benefit_pct",
        "Green background for rows where sg_full_benefit_pct > +50% (label: Win)",
        "Red background for rows where sg_full_benefit_pct < -20% (label: Regression)",
        "Yellow background for rows where sg_full_token_cost_pct > +200% (label: Token spike)",
        "Add st.checkbox('Show highlights', value=True, key='ccmp_highlights') toggle to enable/disable highlighting",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Streamlit st.dataframe supports Pandas Styler objects. Use .applymap() or .apply() for conditional formatting. Keep widget key prefix 'ccmp_'."
    },
    {
      "id": "US-007",
      "title": "Add filter panel to comparison view",
      "description": "As a researcher, I want to filter the comparison to specific task categories, difficulties, or sizes to answer 'does MCP help with complex tasks?'",
      "acceptanceCriteria": [
        "Add filter controls above the comparison table using st.columns() layout (not sidebar)",
        "Create multiselect dropdowns for: Benchmark, Category, Difficulty, SDLC Phase, Search Strategy Type",
        "Add radio button for Task Size bucket: All, Small (<300K), Medium (300K-1M), Large (>1M)",
        "Use filter_comparison_tasks() from dashboard/utils/comparison_filters.py to apply filters",
        "Display 'Showing X of Y tasks' summary above the table",
        "Show helpful empty state message when no tasks match filters",
        "All widget keys use 'ccmp_filter_' prefix",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Populate filter dropdowns dynamically from the loaded data (e.g., unique categories present). Use st.multiselect for multi-value, st.radio for single-value. Reference dashboard/utils/filter_ui.py for existing patterns."
    },
    {
      "id": "US-008",
      "title": "Add CSV export to comparison view",
      "description": "As a researcher, I want to export the filtered comparison table to CSV for custom analysis in Excel or Python.",
      "acceptanceCriteria": [
        "Add st.download_button with label 'Export CSV' to the comparison view",
        "Export all visible (filtered) rows with all columns including computed metrics",
        "Use Pandas to_csv() with index=False for clean output",
        "Filename format: ccb-comparison-{timestamp}.csv",
        "Include header row with human-readable column names",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Streamlit st.download_button(data=csv_string, file_name='...', mime='text/csv') handles the download. Generate CSV from the filtered Pandas DataFrame."
    },
    {
      "id": "US-009",
      "title": "Add search strategy effectiveness section",
      "description": "As a researcher, I want to see how search strategies (keyword_only, nls_focused, deepsearch_heavy, mixed) correlate with success across task categories.",
      "acceptanceCriteria": [
        "Add 'Search Effectiveness' tab or expander section to config_comparison.py view",
        "Display aggregation table: Category | Strategy | Avg Reward | Avg Tokens | Task Count",
        "Aggregate across sourcegraph_base and sourcegraph_full configs only (skip baseline which has no search)",
        "Group by (category, search_strategy_type) from task_metrics.json fields",
        "Show 'N tasks' count column for statistical significance context",
        "Sort by Avg Reward descending within each category",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 9,
      "passes": true,
      "notes": "search_strategy_type field is null on baseline configs. Values: keyword_only, nls_focused, deepsearch_heavy, mixed. Use Pandas groupby + agg for aggregation."
    },
    {
      "id": "US-010",
      "title": "Add search effectiveness by task size section",
      "description": "As a researcher, I want to see if search strategy effectiveness varies with task complexity so I can optimize MCP deployment per scenario.",
      "acceptanceCriteria": [
        "Add 'Search by Size' sub-section within the search effectiveness area",
        "Display pivot table: rows = Task Size Bucket (small/medium/large), columns = search_strategy_type, values = Avg Reward",
        "Size buckets use task_context_length: small (<300K), medium (300K-1M), large (>1M)",
        "Show task count per cell for significance assessment",
        "Apply background gradient coloring: red (low reward) to green (high reward) using Pandas Styler",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Use pd.pivot_table() for the cross-tabulation. Pandas Styler .background_gradient(cmap='RdYlGn') for heatmap coloring."
    },
    {
      "id": "US-011",
      "title": "Add error severity overview section",
      "description": "As an operator, I want to see error distribution by type (infra, api, task, mcp, verifier, setup) so I can prioritize debugging.",
      "acceptanceCriteria": [
        "Add 'Error Analysis' tab or expander section to config_comparison.py view",
        "Display table: Error Severity | Count | % of Total | Avg Reward | Config Distribution",
        "Extract severity from error_fingerprint field in task_metrics.json (it contains {fingerprint_id, label, severity})",
        "Group across all configs to show which configs have which error types",
        "Handle tasks with no error_fingerprint (null = no error, show as 'No Error' row)",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 11,
      "passes": false,
      "notes": "error_fingerprint is a dict with keys: fingerprint_id, label, severity. Severity values from scripts/status_fingerprints.py: infra, api, task, mcp, verifier, setup. Many tasks will have null error_fingerprint (no error)."
    },
    {
      "id": "US-012",
      "title": "Add tool error breakdown section",
      "description": "As an operator, I want to drill into which tools are failing most so I can identify API/integration issues.",
      "acceptanceCriteria": [
        "Add 'Tool Errors' sub-section within the error analysis area",
        "Display table: Tool Name | Error Count | Affected Task Count | Avg Error Rate",
        "Source data from tool_errors_by_name dict field in task_metrics.json (maps tool name to error count)",
        "Compute error rate = sum(errors for tool) / sum(tool_calls_by_name[tool] or 1) across tasks",
        "Sort by error count descending",
        "Add config filter (st.multiselect with baseline/sourcegraph_base/sourcegraph_full options)",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 12,
      "passes": false,
      "notes": "tool_errors_by_name and tool_calls_by_name are dicts in task_metrics.json mapping tool name strings to integer counts. Both can be None."
    }
  ]
}
