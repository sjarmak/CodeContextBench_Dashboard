```rust
//! CanvasChain Symphony â€“ Node Service gRPC server implementation.
//!
//! This server exposes a thin, authenticated faÃ§ade over the internal
//! `NodeRuntime`, allowing other micro-services (composition, minting,
//! governance, etc.) to submit blocks, query state and stream events.
//!
//! Security & Hardening
//! --------------------
//! * All requests must carry a valid HMAC-SHA256 signature in the
//!   `authorization` metadata header (`Bearer <token>`).
//! * Requests are rate-limited per peer (bucketed in memory; flushed
//!   every minute).
//! * Server metrics are exported via Prometheus on `/metrics`.
//!
//! Failure semantics
//! -----------------
//! The service returns structured gRPC `Status` errors carrying an
//! error-code domain that can be programmatically inspected by clients.
//!
//! Concurrency model
//! -----------------
//! The `NodeRuntime` is wrapped in an `Arc` and internally guarded by a
//! `RwLock`, allowing cheap concurrent reads while serialising writes.
//! Expensive, blocking operations are off-loaded to a dedicated
//! CPU-bound thread-pool via `tokio::task::spawn_blocking`.

use std::net::SocketAddr;
use std::sync::Arc;
use std::time::{Duration, SystemTime};

use dashmap::DashMap;
use metrics_exporter_prometheus::PrometheusBuilder;
use prost::Message;
use thiserror::Error;
use tokio::sync::RwLock;
use tokio::time::Instant;
use tonic::codegen::InterceptedService;
use tonic::metadata::MetadataValue;
use tonic::service::Interceptor;
use tonic::{transport::Server, Request, Response, Status};
use tracing::{debug, error, info, warn};

/// Generated by `tonic-build` from `proto/node_rpc.proto`.
/// See `build.rs` at the workspace root.
pub(crate) mod proto {
    tonic::include_proto!("node_rpc");
}

use proto::node_rpc_server::{NodeRpc, NodeRpcServer};
use proto::*;

/// Domain-level runtime that owns the chain state.
#[derive(Debug, Default)]
pub struct NodeRuntime {
    // NOTE: Simplified â€“ in reality this would be a complex state
    // machine plus storage back-end.
    chain_height: u64,
}

impl NodeRuntime {
    pub fn new() -> Self {
        Self { chain_height: 0 }
    }

    /// Verify and append a new block.
    async fn submit_block(&mut self, block: Vec<u8>) -> Result<(), RuntimeError> {
        // Heavy cryptographic verification off-loaded to blocking pool
        tokio::task::spawn_blocking(move || {
            // TODO: validate PoI signature, execute state transition, etc.
            debug!("Verifying {} bytes of block data", block.len());
        })
        .await
        .map_err(|e| RuntimeError::Internal(e.to_string()))??;

        self.chain_height += 1;
        Ok(())
    }

    /// Query an arbitrary key from the state.
    async fn query_state(&self, key: &str) -> Result<Vec<u8>, RuntimeError> {
        // Placeholder for RocksDB/Parquet lookup.
        Ok(format!("value_for_{key}").into_bytes())
    }
}

#[derive(Debug, Error)]
pub enum RuntimeError {
    #[error("unauthorised")]
    Unauthorised,
    #[error("rate limit exceeded")]
    RateLimited,
    #[error("internal error: {0}")]
    Internal(String),
}

impl From<RuntimeError> for Status {
    fn from(err: RuntimeError) -> Self {
        match err {
            RuntimeError::Unauthorised => Status::unauthenticated(err.to_string()),
            RuntimeError::RateLimited => Status::resource_exhausted(err.to_string()),
            RuntimeError::Internal(e) => Status::internal(e),
        }
    }
}

/// Simple in-memory token bucket per peer.
struct RateLimiter {
    buckets: DashMap<String, (u32, Instant)>,
    capacity: u32,
    refill_interval: Duration,
}

impl RateLimiter {
    fn new(capacity: u32, refill_interval: Duration) -> Self {
        Self {
            buckets: DashMap::new(),
            capacity,
            refill_interval,
        }
    }

    /// Attempt to consume one unit. Returns `true` if allowed.
    fn allow(&self, peer: &str) -> bool {
        let now = Instant::now();
        let mut entry = self
            .buckets
            .entry(peer.to_owned())
            .or_insert((self.capacity, now));

        // Refill if needed
        if now.duration_since(entry.1) >= self.refill_interval {
            *entry = (self.capacity, now);
        }

        if entry.0 == 0 {
            return false;
        }

        entry.0 -= 1;
        true
    }
}

/// Interceptor performing auth + rate-limit.
#[derive(Clone)]
struct AuthInterceptor {
    secret: Arc<Vec<u8>>,
    limiter: Arc<RateLimiter>,
}

impl Interceptor for AuthInterceptor {
    fn call(&mut self, req: Request<()>) -> Result<Request<()>, Status> {
        // Rate-limit by peer addr string
        let peer = req
            .remote_addr()
            .map(|a| a.to_string())
            .unwrap_or_else(|| "unknown".into());

        if !self.limiter.allow(&peer) {
            return Err(RuntimeError::RateLimited.into());
        }

        // Verify `authorization` header
        let expected = MetadataValue::from_bytes(&self.secret);
        match req.metadata().get("authorization") {
            Some(header) if header == expected => Ok(req),
            _ => Err(RuntimeError::Unauthorised.into()),
        }
    }
}

/// Concrete gRPC implementation.
///
/// Wraps an `Arc<RwLock<NodeRuntime>>` so we can mutate state safely.
#[derive(Clone)]
struct NodeRpcImpl {
    runtime: Arc<RwLock<NodeRuntime>>,
}

#[tonic::async_trait]
impl NodeRpc for NodeRpcImpl {
    async fn ping(
        &self,
        request: Request<PingRequest>,
    ) -> Result<Response<PingResponse>, Status> {
        debug!("Ping from {:?}", request.remote_addr());
        let resp = PingResponse {
            message: format!("pong @ {}", SystemTime::now().duration_since(SystemTime::UNIX_EPOCH).unwrap().as_secs()),
        };
        Ok(Response::new(resp))
    }

    async fn submit_block(
        &self,
        request: Request<SubmitBlockRequest>,
    ) -> Result<Response<SubmitBlockResponse>, Status> {
        let data = request.into_inner().block;
        let mut guard = self.runtime.write().await;
        match guard.submit_block(data).await {
            Ok(_) => Ok(Response::new(SubmitBlockResponse {
                accepted: true,
                error: "".into(),
            })),
            Err(e) => Ok(Response::new(SubmitBlockResponse {
                accepted: false,
                error: e.to_string(),
            })),
        }
    }

    async fn query_state(
        &self,
        request: Request<QueryStateRequest>,
    ) -> Result<Response<QueryStateResponse>, Status> {
        let key = request.into_inner().key;
        let guard = self.runtime.read().await;
        let value = guard.query_state(&key).await?;
        Ok(Response::new(QueryStateResponse { value }))
    }
}

/// Start the gRPC server and the Prometheus metrics exporter.
pub async fn run_rpc_server(
    bind_addr: SocketAddr,
    auth_secret: Vec<u8>,
) -> anyhow::Result<()> {
    // Initial logs
    info!("ðŸŽ¼  Starting CanvasChain Node RPC server on {bind_addr}");

    // Metrics endpoint
    let (rec, exporter) = PrometheusBuilder::new()
        .install_recorder()
        .map_err(|e| anyhow::anyhow!("metrics recorder: {e}"))?;
    tokio::spawn(async move {
        hyper::Server::bind(&([0, 0, 0, 0], 9898).into())
            .serve(exporter)
            .await
            .expect("Prometheus exporter");
    });
    info!("Prometheus metrics exported on 0.0.0.0:9898/metrics");

    // Shared state
    let runtime = Arc::new(RwLock::new(NodeRuntime::new()));

    // Interceptor
    let interceptor = AuthInterceptor {
        secret: Arc::new(auth_secret),
        limiter: Arc::new(RateLimiter::new(64, Duration::from_secs(60))),
    };

    // Compose the service
    let svc = NodeRpcServer::with_interceptor(NodeRpcImpl { runtime }, interceptor);

    Server::builder()
        .trace_fn(|h| {
            tracing::info_span!(
                "grpc",
                method = %h.method().full_name(),
                peer = ?h.remote_addr()
            )
        })
        .add_service(svc)
        .serve(bind_addr)
        .await?;

    Ok(())
}
```