```rust
//! src/module_68.rs
//!
//! Adaptive performance–monitoring module that observes model–serving
//! statistics and triggers automated retraining when configurable
//! thresholds are violated.
//!
//! The design follows the Observer pattern: `Monitor` is the `Subject`,
//! and any type implementing `RetrainNotifier` is an `Observer`.
//!
//! Threshold evaluation is pluggable through the `Policy` trait,
//! demonstrating the Strategy pattern.
//!
//! ───────────────────────────────────────────────────────────────────────────
//! Public interface summary
//!
//!   • `Metric`                – Enumerates supported KPIs.
//!   • `PerformanceSnapshot`   – Point-in-time collection of metrics.
//!   • `Policy` / `ThresholdPolicy`
//!                             – Strategy for deciding when to retrain.
//!   • `RetrainNotifier`       – Observer trait for side-effectful actions.
//!   • `Monitor`               – Coordinates evaluation and notification.
//!   • `MonitorError`          – Domain-specific error handling.
//!
//! ───────────────────────────────────────────────────────────────────────────

use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;

use async_trait::async_trait;
use chrono::{DateTime, Utc};
use log::{debug, error, info, warn};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::sync::RwLock;
use tokio::time;

//////////////////////////////////////////////////////////////////////////////
/// Domain types
//////////////////////////////////////////////////////////////////////////////

/// Domain-specific key performance indicators.
///
/// New KPIs can be added without touching existing logic; only the policy
/// needs to be aware of them.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum Metric {
    Accuracy,
    Precision,
    Recall,
    F1Score,
    Latency, // milliseconds
}

/// Point-in-time model-performance statistics.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceSnapshot {
    pub model_version: String,
    pub collected_at: DateTime<Utc>,
    pub values: HashMap<Metric, f64>,
}

impl PerformanceSnapshot {
    /// Convenience constructor.
    pub fn new(model_version: impl Into<String>, values: HashMap<Metric, f64>) -> Self {
        Self {
            model_version: model_version.into(),
            collected_at: Utc::now(),
            values,
        }
    }

    /// Returns KPI if present.
    pub fn metric(&self, m: Metric) -> Option<f64> {
        self.values.get(&m).copied()
    }
}

//////////////////////////////////////////////////////////////////////////////
/// Policy Strategy
//////////////////////////////////////////////////////////////////////////////

/// Any evaluation strategy that decides whether a snapshot warrants
/// retraining must implement this trait.
#[async_trait]
pub trait Policy: Send + Sync {
    /// Returns `true` if retraining should be triggered.
    async fn should_retrain(&self, snapshot: &PerformanceSnapshot) -> Result<bool, MonitorError>;
}

/// Concrete policy based on static KPI thresholds.
///
///     Accuracy  ≥ min_accuracy
///     Latency   ≤ max_latency
///
#[derive(Debug, Clone)]
pub struct ThresholdPolicy {
    min_accuracy: f64,
    max_latency_ms: f64,
}

impl ThresholdPolicy {
    pub fn new(min_accuracy: f64, max_latency_ms: f64) -> Self {
        Self {
            min_accuracy,
            max_latency_ms,
        }
    }
}

#[async_trait]
impl Policy for ThresholdPolicy {
    async fn should_retrain(&self, snapshot: &PerformanceSnapshot) -> Result<bool, MonitorError> {
        let accuracy_below =
            snapshot.metric(Metric::Accuracy).map_or(true, |v| v < self.min_accuracy);
        let latency_above =
            snapshot.metric(Metric::Latency).map_or(false, |v| v > self.max_latency_ms);

        Ok(accuracy_below || latency_above)
    }
}

//////////////////////////////////////////////////////////////////////////////
/// Observer trait
//////////////////////////////////////////////////////////////////////////////

/// Observer that receives notifications whenever a retraining event is fired.
#[async_trait]
pub trait RetrainNotifier: Send + Sync {
    async fn notify(&self, snapshot: &PerformanceSnapshot) -> Result<(), MonitorError>;
}

/// No-op notifier useful for unit tests.
pub struct NullNotifier;

#[async_trait]
impl RetrainNotifier for NullNotifier {
    async fn notify(&self, _snapshot: &PerformanceSnapshot) -> Result<(), MonitorError> {
        Ok(())
    }
}

/// Example notifier that publishes a JSON payload to a Kafka topic.
///
/// In production, this might leverage `rdkafka` or a messaging SDK.
/// For brevity, we only simulate the I/O with a log statement.
pub struct KafkaNotifier {
    topic: String,
}

impl KafkaNotifier {
    pub fn new(topic: impl Into<String>) -> Self {
        Self { topic: topic.into() }
    }
}

#[async_trait]
impl RetrainNotifier for KafkaNotifier {
    async fn notify(&self, snapshot: &PerformanceSnapshot) -> Result<(), MonitorError> {
        // In real code, serialize and push to Kafka.
        let payload = serde_json::to_string(snapshot)?;
        info!("KafkaNotifier › publishing to topic={}: {}", self.topic, payload);
        Ok(())
    }
}

//////////////////////////////////////////////////////////////////////////////
/// Monitor: Subject in the Observer pattern
//////////////////////////////////////////////////////////////////////////////

/// Errors emitted during monitoring.
#[derive(Error, Debug)]
pub enum MonitorError {
    #[error("serialization error: {0}")]
    Serde(#[from] serde_json::Error),

    #[error("policy violation: {0}")]
    Policy(String),

    #[error("system time error: {0}")]
    Time(#[from] std::time::SystemTimeError),
}

/// Coordinates metric ingestion, policy evaluation and notifications.
///
/// The struct is `Clone` because all inner fields are behind `Arc`.
#[derive(Clone)]
pub struct Monitor {
    policy: Arc<dyn Policy>,
    notifiers: Arc<RwLock<Vec<Arc<dyn RetrainNotifier>>>>,
}

impl Monitor {
    /// Creates a new monitor with a given policy and a list of notifiers.
    pub fn new<P: Policy + 'static>(policy: P) -> Self {
        Self {
            policy: Arc::new(policy),
            notifiers: Arc::new(RwLock::new(Vec::new())),
        }
    }

    /// Registers an additional notifier at runtime.
    pub async fn register_notifier<N>(&self, notifier: N)
    where
        N: RetrainNotifier + 'static,
    {
        self.notifiers.write().await.push(Arc::new(notifier));
    }

    /// Synchronous helper when only a single snapshot needs evaluation.
    pub async fn evaluate_snapshot(
        &self,
        snapshot: PerformanceSnapshot,
    ) -> Result<(), MonitorError> {
        if self.policy.should_retrain(&snapshot).await? {
            warn!(
                "Performance threshold violated for model={} at {}.",
                snapshot.model_version, snapshot.collected_at
            );
            let notifiers = self.notifiers.read().await.clone();
            for ntf in notifiers {
                if let Err(e) = ntf.notify(&snapshot).await {
                    error!("Notifier error: {}", e);
                }
            }
        } else {
            debug!(
                "Snapshot for model={} within acceptable thresholds.",
                snapshot.model_version
            );
        }

        Ok(())
    }

    /// Continuously consumes snapshots from an async stream source.
    ///
    /// The caller passes an async closure that yields the next snapshot.
    /// If the closure returns `None`, the loop terminates gracefully.
    pub async fn run<F, Fut>(&self, mut next_snapshot: F, poll_interval: Duration)
    where
        F: FnMut() -> Fut + Send,
        Fut: std::future::Future<Output = Option<PerformanceSnapshot>> + Send,
    {
        loop {
            // Block for the configured interval.
            time::sleep(poll_interval).await;

            if let Some(snapshot) = next_snapshot().await {
                if let Err(e) = self.evaluate_snapshot(snapshot).await {
                    error!("Evaluation failed: {}", e);
                }
            } else {
                info!("Snapshot stream exhausted; monitor terminating.");
                break;
            }
        }
    }
}

//////////////////////////////////////////////////////////////////////////////
/// Example usage
///
/// ```no_run
/// use ml_computer_vision::module_68::{Monitor, ThresholdPolicy, KafkaNotifier, Metric, PerformanceSnapshot};
/// use std::collections::HashMap;
/// use std::time::Duration;
/// use tokio;
///
/// # #[tokio::main]
/// # async fn main() {
/// let policy = ThresholdPolicy::new(0.93, 50.0);
/// let monitor = Monitor::new(policy);
/// monitor.register_notifier(KafkaNotifier::new("cv-model-alerts")).await;
///
/// // Simulation: asynchronously yield a snapshot every second.
/// let mut accuracy = 0.95;
/// let mut latency = 30.0;
///
/// monitor
///     .run(
///         move || {
///             accuracy -= 0.005;
///             latency += 2.0;
///             let mut values = HashMap::new();
///             values.insert(Metric::Accuracy, accuracy);
///             values.insert(Metric::Latency, latency);
///             let snap = PerformanceSnapshot::new("v1.2.3", values);
///             async move { Some(snap) }
///         },
///         Duration::from_secs(1),
///     )
///     .await;
/// # }
/// ```
//////////////////////////////////////////////////////////////////////////////

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};

    struct CounterNotifier {
        count: Arc<AtomicUsize>,
    }

    #[async_trait]
    impl RetrainNotifier for CounterNotifier {
        async fn notify(&self, _snapshot: &PerformanceSnapshot) -> Result<(), MonitorError> {
            self.count.fetch_add(1, Ordering::SeqCst);
            Ok(())
        }
    }

    #[tokio::test]
    async fn triggers_only_when_threshold_violated() {
        let policy = ThresholdPolicy::new(0.90, 100.0);
        let monitor = Monitor::new(policy);

        let counter = Arc::new(AtomicUsize::new(0));
        monitor
            .register_notifier(CounterNotifier {
                count: counter.clone(),
            })
            .await;

        // Snapshot 1: within threshold → no trigger
        let mut ok_values = HashMap::new();
        ok_values.insert(Metric::Accuracy, 0.95);
        ok_values.insert(Metric::Latency, 50.0);
        monitor
            .evaluate_snapshot(PerformanceSnapshot::new("ok_model", ok_values))
            .await
            .unwrap();

        // Snapshot 2: accuracy below threshold → trigger
        let mut bad_values = HashMap::new();
        bad_values.insert(Metric::Accuracy, 0.85);
        bad_values.insert(Metric::Latency, 50.0);
        monitor
            .evaluate_snapshot(PerformanceSnapshot::new("bad_model", bad_values))
            .await
            .unwrap();

        assert_eq!(counter.load(Ordering::SeqCst), 1);
    }
}
```