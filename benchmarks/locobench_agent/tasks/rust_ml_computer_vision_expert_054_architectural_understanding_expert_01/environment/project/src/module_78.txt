```rust
//! src/module_78.txt
//!
//! Adaptive Hyper-Parameter Tuning & Observer Bus
//! ---------------------------------------------
//! This module lives in the `Feature Ops` → `Model Ops` seam and provides an
//! implementation of an adaptive hyper-parameter tuning engine that emits
//! tuning events on an internal bus.  Down-stream workers (e.g. automated
//! retraining jobs, dashboards, or alerting sinks) can subscribe to those
//! events via an `async` stream without tight coupling.
//!
//! Key Design Choices
//! ------------------
//! * Strategy Pattern:  The `HyperParamTuner` trait allows multiple search
//!   strategies to coexist (Bayesian, Grid, Evolutionary, …).
//! * Observer Pattern:  Observers implement `TunerObserver` and register on the
//!   `EventBus`, receiving real-time updates.
//! * Thread Safety:     Everything is `Send + Sync`, and the event bus is
//!   backed by Tokio’s broadcast channel for multi-consumer fan-out.
//! * Error Handling:    Rich `TunerError` type with source errors and context.

use rand::Rng;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fmt::{self, Display, Formatter};
use std::sync::Arc;
use thiserror::Error;
use tokio::sync::{broadcast, RwLock};

/// A flattened representation of a hyper-parameter configuration.
///
/// Keys are logical path segments—e.g. `"optimizer.lr"` or `"model.depth"`.
/// Values are JSON blobs to preserve type information (numeric, categorical,
/// string, …).  Using `serde_json::Value` keeps the design flexible without
/// forcing consumers into yet another strongly typed DSL.
pub type HyperParams = HashMap<String, serde_json::Value>;

/// Result metrics returned by a single trial.
///
/// The orchestration layer downstream decides which metric is primary.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrialResult {
    pub experiment_id: uuid::Uuid,
    pub params: HyperParams,
    pub metrics: HashMap<String, f64>,
    pub succeeded: bool,
}

/// Trait defining the contract for an adaptive hyper-parameter tuner.
#[async_trait::async_trait]
pub trait HyperParamTuner: Send + Sync {
    /// Returns a new candidate configuration for evaluation.
    async fn suggest(&self) -> Result<HyperParams, TunerError>;

    /// Updates the tuner’s internal state with the trial outcome so it can
    /// refine subsequent suggestions.
    async fn update(&self, result: TrialResult) -> Result<(), TunerError>;
}

/// An event emitted by the tuning engine.
#[derive(Debug, Clone)]
pub enum TunerEvent {
    /// Emitted when a new suggestion is produced.
    Suggestion {
        experiment_id: uuid::Uuid,
        params: HyperParams,
    },
    /// Emitted when a trial result has been ingested.
    TrialCompleted(TrialResult),
}

impl Display for TunerEvent {
    fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
        match self {
            TunerEvent::Suggestion {
                experiment_id,
                params,
            } => write!(
                f,
                "New suggestion {experiment_id}: {} params",
                params.len()
            ),
            TunerEvent::TrialCompleted(result) => write!(
                f,
                "Trial {} completed — success: {}",
                result.experiment_id, result.succeeded
            ),
        }
    }
}

/// Observer trait enabling plug-ins to watch the tuner in real-time.
///
/// Example implementations:
/// * `DashboardSink`  – pushes events to a web socket.
/// * `AlertingSink`   – triggers alerts on regression.
/// * `RegistrySink`   – logs completed trials to the Model Registry.
#[async_trait::async_trait]
pub trait TunerObserver: Send + Sync {
    async fn on_event(&self, event: TunerEvent);
}

/// Concrete bus implementation used for decoupled fan-out.
#[derive(Clone)]
pub struct EventBus {
    sender: broadcast::Sender<TunerEvent>,
}

impl EventBus {
    pub fn new(capacity: usize) -> Self {
        let (sender, _) = broadcast::channel(capacity);
        Self { sender }
    }

    pub fn subscribe(&self) -> broadcast::Receiver<TunerEvent> {
        self.sender.subscribe()
    }

    pub fn publish(&self, event: TunerEvent) -> Result<(), TunerError> {
        self.sender
            .send(event)
            .map(|_| ())
            .map_err(|e| TunerError::PublishError(e.into()))
    }
}

/// Simple Bayesian-like tuner with random exploration.
//
/// NOTE: Production systems would integrate libraries like GPyTorch or
/// optuna-rs.  This stub is *probabilistic* for illustration while remaining
/// self-contained.
pub struct BayesianTuner {
    state: RwLock<TunerState>,
    event_bus: EventBus,
}

/// Internal mutable state protected by an `RwLock`.
struct TunerState {
    trials: Vec<TrialResult>,
}

impl BayesianTuner {
    pub fn new(event_bus: EventBus) -> Self {
        Self {
            state: RwLock::new(TunerState { trials: vec![] }),
            event_bus,
        }
    }

    /// Register a new observer that receives all subsequent events.
    pub fn register_observer<O>(&self, observer: Arc<O>)
    where
        O: TunerObserver + 'static,
    {
        let mut rx = self.event_bus.subscribe();
        tokio::spawn(async move {
            while let Ok(event) = rx.recv().await {
                observer.on_event(event).await;
            }
        });
    }

    /// Internal helper to emit and swallow any publication error.
    async fn emit(&self, event: TunerEvent) {
        if let Err(e) = self.event_bus.publish(event.clone()) {
            tracing::error!("Failed to publish event {event:?}: {e}");
        }
    }
}

#[async_trait::async_trait]
impl HyperParamTuner for BayesianTuner {
    async fn suggest(&self) -> Result<HyperParams, TunerError> {
        // Very naïve: choose random learning rate and depth.
        let mut rng = rand::thread_rng();
        let lr: f64 = rng.gen_range(1e-5..1e-1);
        let depth: u64 = rng.gen_range(18..152); // ResNet-like depths.

        let mut params = HyperParams::new();
        params.insert("optimizer.lr".into(), serde_json::json!(lr));
        params.insert("model.depth".into(), serde_json::json!(depth));

        let experiment_id = uuid::Uuid::new_v4();
        self.emit(TunerEvent::Suggestion {
            experiment_id,
            params: params.clone(),
        })
        .await;
        Ok(params)
    }

    async fn update(&self, result: TrialResult) -> Result<(), TunerError> {
        {
            let mut state = self.state.write().await;
            state.trials.push(result.clone());
        }
        self.emit(TunerEvent::TrialCompleted(result)).await;
        Ok(())
    }
}

/// Rich domain errors that bubble up to callers.
#[derive(Debug, Error)]
pub enum TunerError {
    #[error("failed to fetch suggestion: {0}")]
    SuggestionError(String),

    #[error("failed to ingest trial result: {0}")]
    UpdateError(String),

    #[error("event bus publish error: {0}")]
    PublishError(#[source] anyhow::Error),
}

/// Convenience alias for a shared tuner handle.
pub type SharedTuner = Arc<dyn HyperParamTuner>;

/// Example observer that logs every event via `tracing`.
///
/// In production, this could push to a Prometheus endpoint or persistent
/// storage.
pub struct LoggingObserver;

#[async_trait::async_trait]
impl TunerObserver for LoggingObserver {
    async fn on_event(&self, event: TunerEvent) {
        match &event {
            TunerEvent::Suggestion { .. } => tracing::info!("{event}"),
            TunerEvent::TrialCompleted(result) => {
                tracing::info!("{event}; primary metric: {:?}", result.metrics)
            }
        }
    }
}

// -----------------------------------------------------------------------------
// Integration Helper
// -----------------------------------------------------------------------------

/// Create a fully wired tuner with a default logging observer.
///
/// This helper can be invoked by orchestration code to keep the call-sites
/// lean.
pub fn provision_default_tuner() -> SharedTuner {
    let bus = EventBus::new(128);
    let tuner = Arc::new(BayesianTuner::new(bus.clone())) as SharedTuner;

    // Attach default observer.
    let observer = Arc::new(LoggingObserver);
    // Downcast to concrete type to call extension method.
    if let Some(concrete) = Arc::clone(&tuner)
        .downcast_arc::<BayesianTuner>()
        .ok()
    {
        concrete.register_observer(observer);
    }

    tuner
}

// -----------------------------------------------------------------------------
// Unit Tests
// -----------------------------------------------------------------------------

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;

    #[tokio::test]
    async fn smoke_test_tuner() {
        tracing_subscriber::fmt::init();
        let tuner = provision_default_tuner();

        // Generate suggestion and feed a dummy result.
        let params = tuner.suggest().await.unwrap();
        assert!(params.contains_key("optimizer.lr"));

        let trial = TrialResult {
            experiment_id: uuid::Uuid::new_v4(),
            params,
            metrics: [("accuracy".into(), 0.75)].into(),
            succeeded: true,
        };
        tuner.update(trial).await.unwrap();

        // Wait briefly to allow observer to log.
        tokio::time::sleep(Duration::from_millis(50)).await;
    }
}
```