```rust
// src/module_62.txt
//
//! Hyper-parameter Tuning, Model-Monitoring and Automated-Retraining
//! ================================================================
//! This module glues together three critical concerns of the VisuTility
//! Orchestrator platform:
//!
//! 1. Strategy Pattern‐based hyper-parameter tuning
//! 2. Observer Pattern‐based model monitoring
//! 3. Automated retraining triggers
//!
//! The code is completely self-contained and can be compiled and unit-tested
//! in isolation.  In the full platform the public interfaces defined here
//! are re-exported by the *Model Ops* layer.
//!
//! Dependencies
//! ------------
//! The following third-party crates are required in `Cargo.toml`.
//!
//! ```toml
//! [dependencies]
//! async-trait = "0.1"
//! rand        = "0.8"
//! thiserror   = "1.0"
//! tokio       = { version = "1", features = ["full"] }
//! log         = "0.4"
//! ```
//!
//! Note: In production, all blocking‐IO should be pushed to separate worker
//! pools; here tokio is used for concise asynchronous examples.

use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use std::time::{Duration, SystemTime};

use async_trait::async_trait;
use log::{debug, error, info, warn};
use rand::distributions::{Distribution, Uniform};
use rand::rngs::ThreadRng;
use thiserror::Error;
use tokio::task;
use tokio::time::sleep;

//
// Domain Types
// ------------

/// A unique model identifier.
pub type ModelId = String;

/// A simplistic representation of metric values keyed by their names.
pub type Metrics = HashMap<String, f64>;

/// Hyper-parameter key/value store.  A real implementation would likely use
/// strongly-typed parameters and ranges.
pub type HyperParams = HashMap<String, f64>;

/// Raw training data handle.  In production this would wrap an Arrow table,
/// Parquet reader, or a stream of image tensors.
#[derive(Debug, Clone)]
pub struct DataBatch {
    pub num_samples: usize,
}

//
// Error Handling
// --------------

#[derive(Debug, Error)]
pub enum ModelOpsError {
    #[error("model `{0}` not found")]
    ModelNotFound(ModelId),

    #[error("failed to spawn background task: {0}")]
    TaskSpawnError(String),

    #[error("training failed: {0}")]
    TrainingFailed(String),

    #[error("registry error: {0}")]
    Registry(String),
}

//
//  Model Registry
// ---------------

/// Metadata for a single, immutable model version.
#[derive(Debug, Clone)]
pub struct ModelVersion {
    pub version: u64,
    pub trained_at: SystemTime,
    pub hyper_params: HyperParams,
    pub metrics: Metrics,
}

/// Thread-safe, in-memory implementation of a model registry.
///
/// In production this would be backed by a SQL/NoSQL database with audit
/// logging. All writes are versioned and immutable.
#[derive(Debug, Default, Clone)]
pub struct ModelRegistry {
    inner: Arc<RwLock<HashMap<ModelId, Vec<ModelVersion>>>>,
}

impl ModelRegistry {
    /// Registers a new model version and returns the assigned version number.
    pub fn register(
        &self,
        id: &ModelId,
        hyper_params: HyperParams,
        metrics: Metrics,
    ) -> Result<u64, ModelOpsError> {
        let mut guard = self
            .inner
            .write()
            .map_err(|e| ModelOpsError::Registry(e.to_string()))?;

        let versions = guard.entry(id.clone()).or_default();
        let version_num = versions
            .last()
            .map(|v| v.version + 1)
            .unwrap_or(1);

        versions.push(ModelVersion {
            version: version_num,
            trained_at: SystemTime::now(),
            hyper_params,
            metrics,
        });

        info!("Registered model `{id}` version {version_num}");
        Ok(version_num)
    }

    /// Returns the latest model version.
    pub fn latest(&self, id: &ModelId) -> Result<ModelVersion, ModelOpsError> {
        let guard = self
            .inner
            .read()
            .map_err(|e| ModelOpsError::Registry(e.to_string()))?;

        guard
            .get(id)
            .and_then(|v| v.last().cloned())
            .ok_or_else(|| ModelOpsError::ModelNotFound(id.clone()))
    }
}

//
// Strategy Pattern: Hyper-Parameter Tuners
// ---------------------------------------

/// Strategies must implement asynchronous tuning given a model id and data.
#[async_trait]
pub trait HyperParamTuner: Send + Sync {
    async fn tune(
        &self,
        model_id: &ModelId,
        data: &DataBatch,
    ) -> Result<HyperParams, ModelOpsError>;
}

/// A simple grid search tuner.
pub struct GridSearchTuner {
    learning_rates: Vec<f64>,
    num_layers: Vec<f64>,
}

impl GridSearchTuner {
    pub fn new() -> Self {
        Self {
            learning_rates: vec![1e-4, 1e-3, 1e-2],
            num_layers: vec![2.0, 3.0, 4.0],
        }
    }
}

#[async_trait]
impl HyperParamTuner for GridSearchTuner {
    async fn tune(
        &self,
        model_id: &ModelId,
        data: &DataBatch,
    ) -> Result<HyperParams, ModelOpsError> {
        info!(
            "Running grid search for model `{}` on {} samples",
            model_id, data.num_samples
        );

        // Dummy evaluation metric: random value ∈ [0,1]
        let mut best_metric = f64::MIN;
        let mut best_params = HyperParams::new();

        for lr in &self.learning_rates {
            for layers in &self.num_layers {
                let simulated_metric = 1.0 / (lr * layers); // deterministic but arbitrary
                if simulated_metric > best_metric {
                    best_metric = simulated_metric;
                    best_params.insert("learning_rate".into(), *lr);
                    best_params.insert("num_layers".into(), *layers);
                }
            }
        }

        info!("Grid search selected params: {:?}", best_params);
        Ok(best_params)
    }
}

/// A random search tuner.
pub struct RandomSearchTuner {
    trials: usize,
    rng: ThreadRng,
}

impl RandomSearchTuner {
    pub fn new(trials: usize) -> Self {
        Self {
            trials,
            rng: rand::thread_rng(),
        }
    }
}

#[async_trait]
impl HyperParamTuner for RandomSearchTuner {
    async fn tune(
        &self,
        model_id: &ModelId,
        data: &DataBatch,
    ) -> Result<HyperParams, ModelOpsError> {
        info!(
            "Running random search for model `{}` with {} trials",
            model_id, self.trials
        );

        let lr_dist = Uniform::new(1e-4, 1e-2);
        let layer_dist = Uniform::new_inclusive(2.0, 6.0);

        let mut best_metric = f64::MIN;
        let mut best_params = HyperParams::new();

        for _ in 0..self.trials {
            let lr = lr_dist.sample(&mut self.rng);
            let layers = layer_dist.sample(&mut self.rng);
            let simulated_metric = 1.0 / (lr * layers);

            if simulated_metric > best_metric {
                best_metric = simulated_metric;
                best_params.insert("learning_rate".into(), lr);
                best_params.insert("num_layers".into(), layers);
            }
        }

        info!("Random search selected params: {:?}", best_params);
        Ok(best_params)
    }
}

/// Factory for obtaining the right tuner based on contextual information.
///
/// In production this might inspect the model type, data modality, or
/// tenancy metadata.
pub enum TunerKind {
    GridSearch,
    RandomSearch { trials: usize },
}

pub fn tuner_factory(kind: TunerKind) -> Arc<dyn HyperParamTuner> {
    match kind {
        TunerKind::GridSearch => Arc::new(GridSearchTuner::new()),
        TunerKind::RandomSearch { trials } => Arc::new(RandomSearchTuner::new(trials)),
    }
}

//
// Observer Pattern: Metrics Events & Subscriptions
// -----------------------------------------------

/// A metrics event emitted by the serving layer.
#[derive(Debug, Clone)]
pub struct MetricsEvent {
    pub model_id: ModelId,
    pub metrics: Metrics,
}

/// Observer trait used by any consumer interested in metrics events.
#[async_trait]
pub trait Observer: Send + Sync {
    async fn update(&self, event: MetricsEvent);
}

/// Subject trait implemented by event producers.
#[async_trait]
pub trait Subject {
    async fn add_observer(&self, observer: Arc<dyn Observer>);
    async fn notify(&self, event: MetricsEvent);
}

/// Model monitor aggregates runtime metrics and notifies observers when
/// degradation is detected.
#[derive(Default)]
pub struct ModelMonitor {
    observers: Arc<RwLock<Vec<Arc<dyn Observer>>>>,
}

#[async_trait]
impl Subject for ModelMonitor {
    async fn add_observer(&self, observer: Arc<dyn Observer>) {
        let mut guard = self
            .observers
            .write()
            .expect("poisoned lock in ModelMonitor");
        guard.push(observer);
    }

    async fn notify(&self, event: MetricsEvent) {
        debug!("Notifying {} observers", self.observers.read().unwrap().len());
        for obs in self.observers.read().unwrap().iter() {
            obs.update(event.clone()).await;
        }
    }
}

//
// Automated Retrainer
// -------------------

/// Retrains a model when metrics fall below a dynamic threshold.
pub struct AutoRetrainer {
    registry: ModelRegistry,
    tuner: Arc<dyn HyperParamTuner>,
    degradation_threshold: f64,
}

impl AutoRetrainer {
    pub fn new(
        registry: ModelRegistry,
        tuner: Arc<dyn HyperParamTuner>,
        degradation_threshold: f64,
    ) -> Self {
        Self {
            registry,
            tuner,
            degradation_threshold,
        }
    }

    /// Actual training routine.  In production this would call into a training
    /// service, ship GPU jobs, etc.
    async fn train_model(
        &self,
        model_id: &ModelId,
        hyper_params: HyperParams,
        data: DataBatch,
    ) -> Result<Metrics, ModelOpsError> {
        info!("Training model `{model_id}` with params {:?}", hyper_params);
        // Fake training latency
        sleep(Duration::from_millis(500)).await;

        // Return fake metrics inversely proportional to learning rate
        let val_accuracy = 0.85
            + 0.1 * (1e-2 - hyper_params["learning_rate"]).clamp(0.0, 1e-2) / 1e-2;

        Ok(HashMap::from([("val_accuracy".into(), val_accuracy)]))
    }
}

#[async_trait]
impl Observer for AutoRetrainer {
    async fn update(&self, event: MetricsEvent) {
        let acc = event.metrics.get("val_accuracy").cloned().unwrap_or(1.0);
        if acc >= self.degradation_threshold {
            debug!(
                "Model `{}` accuracy {:.3} above threshold {:.3}, no action.",
                event.model_id, acc, self.degradation_threshold
            );
            return;
        }

        warn!(
            "Model `{}` accuracy {:.3} below threshold {:.3}, initiating retraining.",
            event.model_id, acc, self.degradation_threshold
        );

        let registry_clone = self.registry.clone();
        let tuner_clone = self.tuner.clone();
        let model_id = event.model_id.clone();

        // Fire-and-forget background task
        if let Err(e) = task::spawn(async move {
            // Step 1: Pull fresh data — here we fake it
            let data = DataBatch { num_samples: 10_000 };

            // Step 2: Hyper-parameter tuning
            let tuned_params = tuner_clone.tune(&model_id, &data).await?;

            // Step 3: Train
            let new_metrics =
                AutoRetrainer::train_model_static(&model_id, tuned_params.clone(), data).await?;

            // Step 4: Register new version
            registry_clone
                .register(&model_id, tuned_params, new_metrics)
                .map(|v| info!("Model `{}` retrained, new version {}", model_id, v))
        })
        .await
        {
            error!("Failed to spawn retraining task: {e}");
        }
    }
}

impl AutoRetrainer {
    /// A static helper so we can move it into the spawned async task easily.
    async fn train_model_static(
        model_id: &ModelId,
        hyper_params: HyperParams,
        data: DataBatch,
    ) -> Result<Metrics, ModelOpsError> {
        // Use a dummy retrainer to reuse the `train_model` method logic.
        AutoRetrainer {
            registry: ModelRegistry::default(),
            tuner: Arc::new(GridSearchTuner::new()),
            degradation_threshold: 0.0,
        }
        .train_model(model_id, hyper_params, data)
        .await
    }
}

//
// Example Usage
// -------------
//! To keep the single-file constraint, an example `main` is included behind
//! the `demo` feature flag.  Compile via `cargo run --features demo`.
#[cfg(feature = "demo")]
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    env_logger::init();

    // Registry bootstrap
    let registry = ModelRegistry::default();
    registry.register(
        &"detector_rgb".into(),
        HashMap::from([
            ("learning_rate".into(), 1e-3),
            ("num_layers".into(), 3.0),
        ]),
        HashMap::from([("val_accuracy".into(), 0.91)]),
    )?;

    // Monitoring & automated retraining
    let monitor = ModelMonitor::default();
    let tuner = tuner_factory(TunerKind::RandomSearch { trials: 20 });
    let retrainer = Arc::new(AutoRetrainer::new(
        registry.clone(),
        tuner,
        0.92, // Threshold
    ));
    monitor.add_observer(retrainer).await;

    // Emit some events – normally this comes from the serving layer.
    let events = vec![
        MetricsEvent {
            model_id: "detector_rgb".into(),
            metrics: HashMap::from([("val_accuracy".into(), 0.93)]),
        },
        MetricsEvent {
            model_id: "detector_rgb".into(),
            metrics: HashMap::from([("val_accuracy".into(), 0.89)]),
        },
    ];

    for ev in events {
        monitor.notify(ev).await;
        sleep(Duration::from_millis(100)).await;
    }

    // Wait for background retraining
    sleep(Duration::from_secs(2)).await;

    // Check registry
    let latest = registry.latest(&"detector_rgb".into())?;
    println!(
        "Latest version: v{} (acc={})",
        latest.version,
        latest.metrics["val_accuracy"]
    );

    Ok(())
}
```