```rust
//! src/module_59.rs
//!
//! Model-Monitoring & Automated-Retraining Trigger
//! ==============================================
//! This module is responsible for *observing* live model–performance
//! metrics, detecting statistically significant drifts or degradations,
//! and broadcasting actionable events to downstream subsystems (e.g. the
//! automated–retraining service).
//!
//! The implementation follows an *Observer Pattern* so that multiple,
//! possibly heterogeneous, components can subscribe to performance events
//! without forming tight couplings.  
//!
//! Usage (simplified):
//! ```ignore
//! let monitor = ModelPerformanceMonitor::new(Default::default());
//! monitor.spawn()?;
//!
//! let retrainer = AutoRetrainObserver::new(…);
//! monitor.register(Box::new(retrainer));
//!
//! // Somewhere else, live metrics are pushed in:
//! monitor.push_metric(MetricEvent::new("model-A", Metric::Accuracy, 0.79));
//! ```
//!
//! The `ModelPerformanceMonitor` performs windowed statistical analysis
//! (rolling mean + stdev) and automatically triggers `MonitorEvent::PerformanceDegraded`
//! when the latest metric deviates beyond an acceptable bound.
//!
//! NOTE: In a full system this component would likely consume metrics
//! over gRPC, Kafka, or a shared in-process channel.  For portability and
//! to keep the module self-contained, we use an internal, bounded
//! `crossbeam_channel`.

use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::thread;
use std::time::{Duration, Instant};

use crossbeam_channel::{bounded, Receiver, RecvTimeoutError, Sender};
use thiserror::Error;

/// The metric type that is being monitored.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum Metric {
    Accuracy,
    Precision,
    Recall,
    LatencyMs,
    ThroughputFps,
}

/// Incoming metric sample for a specific model/version.
#[derive(Debug, Clone)]
pub struct MetricEvent {
    pub model_id: String,
    pub metric: Metric,
    pub value: f64,
    pub timestamp: Instant,
}

impl MetricEvent {
    pub fn new<S: Into<String>>(model_id: S, metric: Metric, value: f64) -> Self {
        Self {
            model_id: model_id.into(),
            metric,
            value,
            timestamp: Instant::now(),
        }
    }
}

/// High-level events generated by the monitor.
#[derive(Debug, Clone)]
pub enum MonitorEvent {
    PerformanceDegraded {
        model_id: String,
        metric: Metric,
        current: f64,
        baseline_mean: f64,
    },
    PerformanceRecovered {
        model_id: String,
        metric: Metric,
        current: f64,
        baseline_mean: f64,
    },
    /// Raw pass-through metric, useful for dashboards.
    MetricSample(MetricEvent),
}

/// Observer interface for downstream components.
pub trait Observer: Send + Sync {
    fn on_event(&self, event: &MonitorEvent);
}

/// Configurable parameters for the monitor.
#[derive(Debug, Clone)]
pub struct MonitorConfig {
    /// How many most-recent samples to keep per (model, metric).
    pub window_size: usize,
    /// Threshold (number of standard deviations) that constitutes degradation.
    pub z_score_threshold: f64,
    /// Max amount of time to block waiting for metrics before the
    /// background thread checks for shutdown.
    pub pull_timeout: Duration,
}

impl Default for MonitorConfig {
    fn default() -> Self {
        Self {
            window_size: 100,
            z_score_threshold: 3.0,
            pull_timeout: Duration::from_millis(500),
        }
    }
}

/// Convenience alias.
type Result<T, E = MonitorError> = std::result::Result<T, E>;

/// Error type for monitor operations.
#[derive(Debug, Error)]
pub enum MonitorError {
    #[error("monitor is already running")]
    AlreadyRunning,
    #[error("monitor thread join error: {0}")]
    ThreadJoin(#[from] Box<dyn std::any::Any + Send + 'static>),
    #[error("channel send failed: {0}")]
    ChannelSend(#[from] crossbeam_channel::SendError<MetricEvent>),
}

/// Rolling window statistics for one (model, metric) tuple.
#[derive(Debug, Clone)]
struct RollingWindow {
    values: Vec<f64>,
    capacity: usize,
}

impl RollingWindow {
    fn new(capacity: usize) -> Self {
        Self {
            values: Vec::with_capacity(capacity),
            capacity,
        }
    }

    fn push(&mut self, v: f64) {
        if self.values.len() == self.capacity {
            self.values.remove(0);
        }
        self.values.push(v);
    }

    fn mean(&self) -> f64 {
        if self.values.is_empty() {
            0.0
        } else {
            self.values.iter().copied().sum::<f64>() / self.values.len() as f64
        }
    }

    fn std_dev(&self) -> f64 {
        let mean = self.mean();
        let variance = self
            .values
            .iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f64>()
            / self.values.len().max(1) as f64;
        variance.sqrt()
    }
}

/// Public façade object.
pub struct ModelPerformanceMonitor {
    cfg: MonitorConfig,
    /// Map keyed by (model_id, metric) -> rolling stats window.
    windows: Arc<Mutex<HashMap<(String, Metric), RollingWindow>>>,
    observers: Arc<Mutex<Vec<Box<dyn Observer>>>>,
    tx: Sender<MetricEvent>,
    rx: Receiver<MetricEvent>,
    background_handle: Arc<Mutex<Option<thread::JoinHandle<()>>>>,
}

impl ModelPerformanceMonitor {
    /// Returns a new, *not yet running* monitor.
    pub fn new(cfg: MonitorConfig) -> Self {
        let (tx, rx) = bounded::<MetricEvent>(10_000);
        Self {
            cfg,
            windows: Arc::new(Mutex::new(HashMap::new())),
            observers: Arc::new(Mutex::new(Vec::new())),
            tx,
            rx,
            background_handle: Arc::new(Mutex::new(None)),
        }
    }

    /// Start the background monitoring thread.
    pub fn spawn(&self) -> Result<()> {
        let mut guard = self.background_handle.lock().unwrap();
        if guard.is_some() {
            return Err(MonitorError::AlreadyRunning);
        }

        let rx = self.rx.clone();
        let cfg = self.cfg.clone();
        let windows = Arc::clone(&self.windows);
        let observers = Arc::clone(&self.observers);

        let handle = thread::Builder::new()
            .name("model-performance-monitor".into())
            .spawn(move || loop {
                match rx.recv_timeout(cfg.pull_timeout) {
                    Ok(metric_event) => {
                        // Push raw sample to observers regardless of statistics.
                        Self::notify(&observers, &MonitorEvent::MetricSample(metric_event.clone()));

                        let mut win_guard = windows.lock().unwrap();
                        let window = win_guard
                            .entry((metric_event.model_id.clone(), metric_event.metric))
                            .or_insert_with(|| RollingWindow::new(cfg.window_size));

                        window.push(metric_event.value);

                        if window.values.len() >= 5 {
                            let mean = window.mean();
                            let std = window.std_dev();
                            let z_score = if std > 0.0 {
                                (metric_event.value - mean) / std
                            } else {
                                0.0
                            };

                            if z_score.abs() >= cfg.z_score_threshold {
                                let event = MonitorEvent::PerformanceDegraded {
                                    model_id: metric_event.model_id.clone(),
                                    metric: metric_event.metric,
                                    current: metric_event.value,
                                    baseline_mean: mean,
                                };
                                Self::notify(&observers, &event);
                            } else if z_score.abs() <= cfg.z_score_threshold / 2.0 {
                                // Performance back to normal range.
                                let event = MonitorEvent::PerformanceRecovered {
                                    model_id: metric_event.model_id.clone(),
                                    metric: metric_event.metric,
                                    current: metric_event.value,
                                    baseline_mean: mean,
                                };
                                Self::notify(&observers, &event);
                            }
                        }
                    }
                    Err(RecvTimeoutError::Timeout) => {
                        // check for shutdown signal here in a full impl
                        continue;
                    }
                    Err(RecvTimeoutError::Disconnected) => break,
                }
            })?;

        *guard = Some(handle);
        Ok(())
    }

    /// Register an observer for monitor events.
    pub fn register(&self, observer: Box<dyn Observer>) {
        let mut obs = self.observers.lock().unwrap();
        obs.push(observer);
    }

    /// Push a live metric into the monitor.
    pub fn push_metric(&self, evt: MetricEvent) -> Result<()> {
        Ok(self.tx.send(evt)?)
    }

    /// Notify all registered observers of an event.
    fn notify(observers: &Arc<Mutex<Vec<Box<dyn Observer>>>>, evt: &MonitorEvent) {
        let obs_guard = observers.lock().unwrap();
        for obs in obs_guard.iter() {
            // In production we might want to protect each call with catch_unwind
            // to prevent a single observer from crashing the monitor.
            obs.on_event(evt);
        }
    }
}

/// A sample observer that triggers automated retraining upon degradation.
pub struct AutoRetrainObserver {
    retrain_threshold: usize,
    degradation_counter: Mutex<HashMap<(String, Metric), usize>>,
}

impl AutoRetrainObserver {
    pub fn new(retrain_threshold: usize) -> Self {
        Self {
            retrain_threshold,
            degradation_counter: Mutex::new(HashMap::new()),
        }
    }

    /// Placeholder stub for kicking off a retraining pipeline.
    fn trigger_retrain(model_id: &str) {
        // In reality this would enqueue a job in the orchestrator, call
        // the Kubeflow pipeline, etc.
        log::info!(
            "[AutoRetrainObserver] Triggering retrain for model `{}`",
            model_id
        );
    }
}

impl Observer for AutoRetrainObserver {
    fn on_event(&self, event: &MonitorEvent) {
        match event {
            MonitorEvent::PerformanceDegraded {
                model_id,
                metric,
                ..
            } => {
                let mut guard = self.degradation_counter.lock().unwrap();
                let key = (model_id.clone(), *metric);
                let count = guard.entry(key).or_insert(0);
                *count += 1;
                if *count >= self.retrain_threshold {
                    Self::trigger_retrain(model_id);
                    *count = 0; // reset counter after retrain
                }
            }
            MonitorEvent::PerformanceRecovered { model_id, metric, .. } => {
                // Reset degradation counter
                let mut guard = self.degradation_counter.lock().unwrap();
                guard.insert((model_id.clone(), *metric), 0);
            }
            MonitorEvent::MetricSample(_) => {} // Ignored
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};

    struct TestObserver {
        events: AtomicUsize,
    }

    impl TestObserver {
        fn new() -> Self {
            Self {
                events: AtomicUsize::new(0),
            }
        }
    }

    impl Observer for TestObserver {
        fn on_event(&self, _event: &MonitorEvent) {
            self.events.fetch_add(1, Ordering::SeqCst);
        }
    }

    #[test]
    fn monitor_push_and_observe() {
        let monitor = ModelPerformanceMonitor::new(MonitorConfig {
            window_size: 10,
            z_score_threshold: 2.0,
            pull_timeout: Duration::from_millis(50),
        });

        monitor.spawn().unwrap();

        let obs = Box::new(TestObserver::new());
        let ptr = &*obs as *const TestObserver as usize; // hacky but fine for test
        monitor.register(obs);

        for _ in 0..20 {
            monitor
                .push_metric(MetricEvent::new(
                    "model-X",
                    Metric::Accuracy,
                    0.90,
                ))
                .unwrap();
        }

        std::thread::sleep(Duration::from_millis(200));

        // Safety: we know the observer still lives.
        let obs_ref: &TestObserver = unsafe { &*(ptr as *const TestObserver) };
        assert!(obs_ref.events.load(Ordering::SeqCst) > 0);
    }
}
```