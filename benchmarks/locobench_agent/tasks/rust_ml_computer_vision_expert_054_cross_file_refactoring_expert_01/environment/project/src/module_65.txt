```rust
//! Hyperparameter tuning sub–module (Strategy + Observer Patterns)
//
// This module belongs to the VisuTility Orchestrator project and is responsible
// for end-to-end hyper-parameter tuning and automated retraining triggers.  The
// module demonstrates the Strategy Pattern for pluggable tuning algorithms and
// the Observer Pattern for notifying interested parties when a new set of
// hyper-parameters has been found.
//
// The implementation is `async`/`tokio` friendly and uses `tracing` for
// structured logging so it can be embedded in the larger, distributed runtime
// of the Orchestrator.

// ────────────────────────────────────────────────────────────────────────────
// External dependencies
// ────────────────────────────────────────────────────────────────────────────
use std::{collections::HashMap, sync::Arc, time::Duration};

use anyhow::{anyhow, Result};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use tokio::{
    select,
    sync::{broadcast, mpsc, Mutex},
    task,
    time::sleep,
};
use tracing::{debug, error, info, instrument, warn};

// ────────────────────────────────────────────────────────────────────────────
// Core types
// ────────────────────────────────────────────────────────────────────────────

/// Type-alias for a generic hyper-parameter set.
/// In practice, each algorithm will parse the `HashMap<String, f64>`
/// into its own strongly-typed domain object.
pub type HyperParameters = HashMap<String, f64>;

/// A domain-level identifier for the training task being tuned.
/// Keeps the module decoupled from upper-layer identifiers.
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct TuningJobId(pub String);

// ────────────────────────────────────────────────────────────────────────────
// Strategy Pattern — pluggable tuning algorithms
// ────────────────────────────────────────────────────────────────────────────

/// Generic interface for hyper-parameter tuning strategies.
#[async_trait]
pub trait HyperparameterTuningStrategy: Send + Sync {
    /// Receives feedback on the last trial and suggests the next candidate.
    async fn next_candidates(
        &self,
        job_id: &TuningJobId,
        last_score: Option<f64>,
    ) -> Result<Vec<HyperParameters>>;

    /// Provides a human-readable identifier.
    fn name(&self) -> &'static str;
}

/// SIMPLE GRID SEARCH STRATEGY
///
/// Used for low-dimensional search spaces or as a fallback when
/// Bayesian optimisation cannot converge.
pub struct GridSearchStrategy {
    /// Discrete search grid per parameter key.
    search_grid: HashMap<String, Vec<f64>>,
    /// The current index inspected per parameter key.
    cursor: Arc<Mutex<HashMap<String, usize>>>,
}

impl GridSearchStrategy {
    pub fn new(search_grid: HashMap<String, Vec<f64>>) -> Self {
        Self {
            cursor: Arc::new(Mutex::new(HashMap::default())),
            search_grid,
        }
    }
}

#[async_trait]
impl HyperparameterTuningStrategy for GridSearchStrategy {
    #[instrument(skip_all, fields(strategy = "grid_search"))]
    async fn next_candidates(
        &self,
        _job_id: &TuningJobId,
        _last_score: Option<f64>,
    ) -> Result<Vec<HyperParameters>> {
        // For demo purposes we emit a single candidate at a time.
        // In production you might batch to exploit parallel hardware.
        let mut candidate = HyperParameters::default();
        let mut cursor_lock = self.cursor.lock().await;

        for (key, values) in &self.search_grid {
            let idx = cursor_lock.entry(key.clone()).or_insert(0);
            if *idx >= values.len() {
                // We exhausted the grid.
                return Err(anyhow!("Search grid exhausted for key={}", key));
            }
            candidate.insert(key.clone(), values[*idx]);
            // Advance cursor for the next call.
            *idx += 1;
        }

        Ok(vec![candidate])
    }

    fn name(&self) -> &'static str {
        "grid_search"
    }
}

/// BAYESIAN OPTIMISATION STRATEGY (mocked for brevity)
///
/// In a real-world implementation we would wrap a C-FFI binding
/// to a BO library such as `bayes-opt` or write our own GP.
/// Here we approximate behaviour with random sampling
/// augmented by an exploration/exploitation trade-off parameter.
pub struct BayesianOptimizationStrategy {
    search_space: HashMap<String, (f64, f64)>, // (low, high)
    exploration_weight: f64,
}

impl BayesianOptimizationStrategy {
    pub fn new(search_space: HashMap<String, (f64, f64)>, exploration_weight: f64) -> Self {
        Self {
            search_space,
            exploration_weight,
        }
    }
}

#[async_trait]
impl HyperparameterTuningStrategy for BayesianOptimizationStrategy {
    #[instrument(skip_all, fields(strategy = "bayesian_optimisation"))]
    async fn next_candidates(
        &self,
        _job_id: &TuningJobId,
        last_score: Option<f64>,
    ) -> Result<Vec<HyperParameters>> {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        let mut hp = HyperParameters::with_capacity(self.search_space.len());

        for (key, (low, high)) in &self.search_space {
            let sample = rng.gen_range(*low..=*high);
            hp.insert(key.clone(), sample);
        }

        if let Some(score) = last_score {
            debug!(
                strategy = self.name(),
                ?hp,
                %score,
                "Received feedback, adjusting exploration weight"
            );
            // Real implementation would update GP model here.
        }
        Ok(vec![hp])
    }

    fn name(&self) -> &'static str {
        "bayesian_optimisation"
    }
}

// ────────────────────────────────────────────────────────────────────────────
// Factory Pattern — build a strategy from declarative config
// ────────────────────────────────────────────────────────────────────────────

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum StrategyConfig {
    GridSearch {
        grid: HashMap<String, Vec<f64>>,
    },
    Bayesian {
        space: HashMap<String, (f64, f64)>,
        exploration_weight: f64,
    },
}

impl TryFrom<StrategyConfig> for Box<dyn HyperparameterTuningStrategy> {
    type Error = anyhow::Error;

    fn try_from(cfg: StrategyConfig) -> Result<Self, Self::Error> {
        Ok(match cfg {
            StrategyConfig::GridSearch { grid } => Box::new(GridSearchStrategy::new(grid)),
            StrategyConfig::Bayesian {
                space,
                exploration_weight,
            } => Box::new(BayesianOptimizationStrategy::new(
                space,
                exploration_weight,
            )),
        })
    }
}

// ────────────────────────────────────────────────────────────────────────────
// Observer Pattern — notify when new hyper-parameters discovered
// ────────────────────────────────────────────────────────────────────────────

#[async_trait]
pub trait TuningObserver: Send + Sync {
    async fn on_new_parameters(
        &self,
        job_id: &TuningJobId,
        params: Vec<HyperParameters>,
    ) -> Result<()>;
}

/// Simple observer that triggers retraining via an async channel.
/// In a full orchestration, this would fan-out into the model mesh.
pub struct RetrainTrigger {
    sender: mpsc::Sender<(TuningJobId, HyperParameters)>,
}

impl RetrainTrigger {
    pub fn new(sender: mpsc::Sender<(TuningJobId, HyperParameters)>) -> Self {
        Self { sender }
    }
}

#[async_trait]
impl TuningObserver for RetrainTrigger {
    #[instrument(skip_all, fields(observer = "retrain_trigger"))]
    async fn on_new_parameters(
        &self,
        job_id: &TuningJobId,
        params: Vec<HyperParameters>,
    ) -> Result<()> {
        for hp in params {
            self.sender
                .send((job_id.clone(), hp))
                .await
                .map_err(|e| anyhow!("Failed to send retrain request: {e}"))?;
        }
        Ok(())
    }
}

// ────────────────────────────────────────────────────────────────────────────
// Orchestrator — coordinates strategy & observers
// ────────────────────────────────────────────────────────────────────────────

/// Public facade for hyper-parameter tuning jobs.
pub struct TuningOrchestrator {
    strategy: Box<dyn HyperparameterTuningStrategy>,
    observers: Vec<Arc<dyn TuningObserver>>,
    /// Broadcast channel used for shutdown signals.
    shutdown: broadcast::Sender<()>,
}

impl TuningOrchestrator {
    /// Creates a new orchestrator using the provided strategy configuration.
    pub fn new(strategy_cfg: StrategyConfig) -> Result<Self> {
        let strategy: Box<dyn HyperparameterTuningStrategy> = strategy_cfg.try_into()?;
        let (shutdown, _) = broadcast::channel(4);
        Ok(Self {
            strategy,
            observers: Vec::new(),
            shutdown,
        })
    }

    /// Registers an observer that will be notified on every new candidate set.
    pub fn register_observer(&mut self, obs: Arc<dyn TuningObserver>) {
        self.observers.push(obs);
    }

    /// Starts the tuning loop in a background task.
    ///
    /// The loop terminates when a shutdown signal is received
    /// or when the strategy raises an unrecoverable error.
    pub fn start(
        &self,
        job_id: TuningJobId,
        feedback_rx: mpsc::Receiver<f64>,
    ) -> Result<task::JoinHandle<()>> {
        let mut observers = self.observers.clone();
        let strategy = self.strategy.clone();
        let mut shutdown_rx = self.shutdown.subscribe();

        let handle = task::spawn(async move {
            let mut feedback_rx = feedback_rx;
            let mut last_score: Option<f64> = None;

            loop {
                select! {
                    biased;

                    _ = shutdown_rx.recv() => {
                        info!(%job_id, "Shutdown signal received — terminating tuning loop");
                        break;
                    }

                    feedback = feedback_rx.recv() => {
                        if let Some(score) = feedback {
                            last_score = Some(score);
                            debug!(%job_id, %score, "Feedback received from evaluator");
                        } else {
                            warn!(%job_id, "Feedback channel closed — using last known score");
                        }
                    }

                    // Main tuning logic
                    default => {
                        match strategy.next_candidates(&job_id, last_score).await {
                            Ok(candidates) => {
                                // Notify observers
                                for obs in &mut observers {
                                    if let Err(e) = obs.on_new_parameters(&job_id, candidates.clone()).await {
                                        error!(%job_id, error = %e, "Observer failed while handling new params");
                                    }
                                }
                            }
                            Err(e) => {
                                error!(%job_id, error = %e, "Strategy failed, terminating tuning loop");
                                break;
                            }
                        }
                        // Wait a bit before the next iteration to avoid hammering resources.
                        sleep(Duration::from_millis(500)).await;
                    }
                }
            }

            info!(%job_id, "Tuning loop exited");
        });

        Ok(handle)
    }

    /// Signals the orchestrator to gracefully shut down.
    pub fn shutdown(&self) {
        let _ = self.shutdown.send(());
    }
}

// ────────────────────────────────────────────────────────────────────────────
// End-to-end example (compiled as doc-test when `cargo test` runs)
// ────────────────────────────────────────────────────────────────────────────

#[cfg(test)]
mod tests {
    use super::*;
    use tokio::sync::mpsc;

    #[tokio::test]
    async fn grid_search_demo() -> Result<()> {
        tracing_subscriber::fmt()
            .with_test_writer()
            .with_target(false)
            .init();

        // Build a simple grid for demonstration.
        let grid = HashMap::from([
            ("lr".to_string(), vec![0.001, 0.01]),
            ("batch_size".to_string(), vec![32.0, 64.0]),
        ]);

        let cfg = StrategyConfig::GridSearch { grid };

        // Channels for retraining and feedback
        let (retrain_tx, mut retrain_rx) = mpsc::channel::<(TuningJobId, HyperParameters)>(4);
        let (feedback_tx, feedback_rx) = mpsc::channel::<f64>(4);

        let mut orchestrator = TuningOrchestrator::new(cfg)?;
        orchestrator.register_observer(Arc::new(RetrainTrigger::new(retrain_tx)));

        let job_id = TuningJobId("demo_job".into());
        let handle = orchestrator.start(job_id.clone(), feedback_rx)?;

        // Simulate evaluator sending scores back
        task::spawn(async move {
            let scores = vec![0.75, 0.80, 0.78, 0.82];
            for s in scores {
                feedback_tx.send(s).await.ok();
                sleep(Duration::from_millis(200)).await;
            }
        });

        // Listen for retrain triggers
        let mut seen = 0usize;
        while let Some((jid, hp)) = retrain_rx.recv().await {
            assert_eq!(jid, job_id);
            info!("Retrain triggered with params: {:#?}", hp);
            seen += 1;
            if seen >= 3 {
                break;
            }
        }

        // Clean shutdown
        orchestrator.shutdown();
        handle.await.ok();
        Ok(())
    }
}
```