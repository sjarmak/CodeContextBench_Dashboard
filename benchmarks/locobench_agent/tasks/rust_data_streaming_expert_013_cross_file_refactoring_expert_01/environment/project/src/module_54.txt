```rust
//! src/module_54.rs
//!
//! A self-contained, production-grade microâ€“service that consumes social
//! events from Kafka, enriches them with a pluggable â€œstrategyâ€ pipeline, and
//! republishes the result.  The module illustrates ChirpPulseâ€™s core
//! principles: resiliency, hot-swappable analytics, back-pressure aware
//! streaming, and observability.
//!
//! IMPORTANT:  For brevity, the file purposefully omits the surrounding Cargo
//! integration.  The following crates must be enabled in `Cargo.toml`
//
//! [dependencies]
//! async-trait      = "0.1"
//! chrono           = "0.4"
//! futures          = "0.3"
//! rdkafka          = { version = "0.36", features = ["tokio"] }
//! serde            = { version = "1.0", features = ["derive"] }
//! serde_json       = "1.0"
//! thiserror        = "1.0"
//! tokio            = { version = "1.36", features = ["macros", "signal", "rt-multi-thread", "time"] }
//! tracing          = "0.1"
//! tracing-subscriber = { version = "0.3", features = ["env-filter", "fmt"] }

use std::{
    env,
    sync::Arc,
    time::{Duration, SystemTime},
};

use async_trait::async_trait;
use futures::{StreamExt, TryStreamExt};
use rdkafka::{
    config::ClientConfig,
    consumer::{CommitMode, Consumer, StreamConsumer},
    error::KafkaError,
    message::{BorrowedMessage, OwnedHeaders, OwnedMessage},
    producer::{FutureProducer, FutureRecord},
    util::Timeout,
};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::{select, signal, sync::RwLock, task::JoinHandle};
use tracing::{debug, error, info, instrument, warn};

/// ---------------------------------------------------------------------------
/// Domain types
/// ---------------------------------------------------------------------------

/// Raw social event as ingested from upstream sources.
#[derive(Debug, Serialize, Deserialize)]
pub struct SocialEvent {
    /// A unique identifier generated by the source social network.
    pub event_id: String,
    /// The raw text (e.g. tweet, comment, post body).
    pub text: String,
    /// Unix timestamp in milliseconds describing when the event was created.
    pub ts: i64,
    /// The originating network (â€œtwitterâ€, â€œredditâ€, â€œyoutubeâ€ â€¦).
    pub network: String,
}

/// Extension type produced by enrichment strategies.
#[derive(Debug, Serialize, Deserialize)]
pub struct EnrichedEvent {
    /// Original, unmodified input event.
    pub origin: SocialEvent,
    /// Optional per-strategy payloads.
    #[serde(flatten)]
    pub payload: serde_json::Value,
    /// Server-side processing time.
    pub processed_at: i64,
    /// Service build version; enables easy bisection during rollbacks.
    pub version: &'static str,
}

/// ---------------------------------------------------------------------------
/// Configuration + error handling
/// ---------------------------------------------------------------------------

#[derive(Clone)]
pub struct ProcessorConfig {
    pub brokers: String,
    pub input_topic: String,
    pub output_topic: String,
    pub group_id: String,
    /// Maximum un-acked messages to keep in memory.
    pub max_inflight: usize,
}

impl Default for ProcessorConfig {
    /// Load configuration from environment variables, falling back on safe
    /// defaults designed for local development.
    fn default() -> Self {
        Self {
            brokers: env::var("KAFKA_BROKERS").unwrap_or_else(|_| "localhost:9092".into()),
            input_topic: env::var("INPUT_TOPIC").unwrap_or_else(|_| "chirp.raw.social".into()),
            output_topic: env::var("OUTPUT_TOPIC").unwrap_or_else(|_| "chirp.enriched.social".into()),
            group_id: env::var("GROUP_ID").unwrap_or_else(|_| "chirp.enricher.v1".into()),
            max_inflight: env::var("MAX_INFLIGHT")
                .ok()
                .and_then(|v| v.parse::<usize>().ok())
                .unwrap_or(10_000),
        }
    }
}

/// Module-wide error type; encapsulates all possible failures.
#[derive(Error, Debug)]
pub enum ProcessorError {
    #[error("Kafka error: {0}")]
    Kafka(#[from] KafkaError),
    #[error("Serde error: {0}")]
    Serde(#[from] serde_json::Error),
    #[error("Strategy error: {0}")]
    Strategy(String),
    #[error("Unknown error: {0}")]
    Unknown(String),
}

/// ---------------------------------------------------------------------------
/// Strategy pattern â€“ runtime-pluggable analytics
/// ---------------------------------------------------------------------------

#[async_trait]
pub trait AnalysisStrategy: Send + Sync {
    /// Strategy name, used for logging and metrics.
    fn name(&self) -> &'static str;

    /// Apply the strategy to a social event.
    async fn analyze(&self, event: &SocialEvent) -> Result<serde_json::Value, ProcessorError>;
}

/// Simple rule-based toxicity detector (placeholder for an ML model).
pub struct ToxicityStrategy;

#[async_trait]
impl AnalysisStrategy for ToxicityStrategy {
    fn name(&self) -> &'static str {
        "toxicity"
    }

    #[instrument(skip(self, event))]
    async fn analyze(&self, event: &SocialEvent) -> Result<serde_json::Value, ProcessorError> {
        // Extremely naive implementation for demo purposes.
        let toxic_words = ["hate", "stupid", "idiot", "kill"];
        let toxicity_score = toxic_words
            .iter()
            .filter(|word| event.text.to_ascii_lowercase().contains(&word[..]))
            .count() as f32
            / toxic_words.len() as f32;

        Ok(serde_json::json!({
            "toxicity_score": toxicity_score,
        }))
    }
}

/// Lightweight sentiment analyzer (placeholder for more complex NLP).
pub struct SentimentStrategy;

#[async_trait]
impl AnalysisStrategy for SentimentStrategy {
    fn name(&self) -> &'static str {
        "sentiment"
    }

    #[instrument(skip(self, event))]
    async fn analyze(&self, event: &SocialEvent) -> Result<serde_json::Value, ProcessorError> {
        // Toy implementation: positive if contains â€œ<3â€, negative if â€˜:(â€™
        let lower = event.text.to_ascii_lowercase();
        let score = if lower.contains("<3") {
            1.0_f32
        } else if lower.contains(":(") {
            -1.0_f32
        } else {
            0.0
        };
        Ok(serde_json::json!({
            "sentiment_score": score,
        }))
    }
}

/// Thread-safe registry that can hot-swap strategies at runtime.
///
/// In production we'd back this with a gRPC control-plane or watch etcd.
/// Here we simply expose an in-memory list guarded by an `RwLock`.
#[derive(Default)]
pub struct StrategyRegistry {
    strategies: RwLock<Vec<Arc<dyn AnalysisStrategy>>>,
}

impl StrategyRegistry {
    pub fn new() -> Self {
        Self {
            strategies: RwLock::new(vec![
                Arc::new(SentimentStrategy) as Arc<dyn AnalysisStrategy>,
                Arc::new(ToxicityStrategy),
            ]),
        }
    }

    /// Return current snapshot of loaded strategies.
    pub async fn snapshot(&self) -> Vec<Arc<dyn AnalysisStrategy>> {
        self.strategies.read().await.clone()
    }

    /// Dynamically replace all strategies; results in zero-downtime upgrade.
    #[allow(dead_code)]
    pub async fn replace(&self, new_strategies: Vec<Arc<dyn AnalysisStrategy>>) {
        let mut lock = self.strategies.write().await;
        *lock = new_strategies;
        info!(count = lock.len(), "strategy set replaced");
    }
}

/// ---------------------------------------------------------------------------
/// Processor â€“ Kafka glue + concurrency orchestration
/// ---------------------------------------------------------------------------

pub struct EventProcessor {
    cfg: ProcessorConfig,
    consumer: StreamConsumer,
    producer: FutureProducer,
    registry: Arc<StrategyRegistry>,
}

impl EventProcessor {
    pub fn new(cfg: ProcessorConfig, registry: Arc<StrategyRegistry>) -> Result<Self, ProcessorError> {
        // Kafka consumer configuration.
        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", &cfg.brokers)
            .set("group.id", &cfg.group_id)
            .set("enable.auto.commit", "false")
            .set("auto.offset.reset", "earliest")
            .create()?;

        consumer.subscribe(&[&cfg.input_topic])?;

        // Producer with default acks=all.
        let producer: FutureProducer = ClientConfig::new()
            .set("bootstrap.servers", &cfg.brokers)
            .create()?;

        Ok(Self {
            cfg,
            consumer,
            producer,
            registry,
        })
    }

    /// Spawn the processor.  Returns a join-handle so the caller can `await`
    /// graceful shutdown.
    pub async fn spawn(mut self) -> JoinHandle<()> {
        tokio::spawn(async move {
            info!("ðŸ›°ï¸  ChirpPulse processor up and running");
            let shutdown = async {
                signal::ctrl_c().await.expect("failed to install CTRL-C handler");
                info!("shutdown signal received");
            };

            let stream = self.consumer.stream();

            select! {
                _ = shutdown => {
                    info!("processor shutting down");
                }
                res = self.run(stream) => {
                    if let Err(e) = res {
                        error!(error = %e, "processor terminated with error");
                    }
                }
            }
        })
    }

    /// Core processing loop.
    async fn run<S>(&mut self, mut stream: S) -> Result<(), ProcessorError>
    where
        S: futures::Stream<Item = Result<BorrowedMessage<'_>, KafkaError>> + Unpin,
    {
        while let Some(msg) = stream.try_next().await? {
            let payload = match msg.payload_view::<str>() {
                None | Some(Err(_)) => {
                    warn!("discarding message â€“ invalid UTF-8");
                    self.consumer.commit_message(&msg, CommitMode::Async)?;
                    continue;
                }
                Some(Ok(json_str)) => json_str,
            };

            let social: SocialEvent = match serde_json::from_str(payload) {
                Ok(ev) => ev,
                Err(e) => {
                    warn!(error = %e, "discarding message â€“ invalid JSON");
                    self.consumer.commit_message(&msg, CommitMode::Async)?;
                    continue;
                }
            };

            // Cloning a small struct is cheap and avoids lifetime wrestling.
            let enriched = self.enrich_event(social.clone()).await?;

            self.publish(enriched).await?;

            // Manually commit offset after successful publish.
            self.consumer.commit_message(&msg, CommitMode::Async)?;
        }

        Ok(())
    }

    /// Run all registered strategies in parallel.
    #[instrument(skip(self, event), fields(event_id = %event.event_id))]
    async fn enrich_event(&self, event: SocialEvent) -> Result<EnrichedEvent, ProcessorError> {
        let strategies = self.registry.snapshot().await;

        // Fire off analysis in parallel while bounding concurrency to avoid
        // unbounded executor growth (assumes MAX_INFLIGHT is sane).
        let futs = strategies.into_iter().map(|s| async move {
            let payload = s.analyze(&event).await?;
            Ok::<_, ProcessorError>((s.name(), payload))
        });

        let results: Vec<(<&'static str, serde_json::Value>)> =
            futures::future::join_all(futs)
                .await
                .into_iter()
                .collect::<Result<_, _>>()?;

        // Merge per-strategy JSON outputs into a single object.
        let mut merged = serde_json::Map::new();
        for (name, value) in results {
            merged.insert(name.to_string(), value);
        }

        Ok(EnrichedEvent {
            origin: event,
            payload: serde_json::Value::Object(merged),
            processed_at: chrono::Utc::now().timestamp_millis(),
            version: env!("CARGO_PKG_VERSION"),
        })
    }

    #[instrument(skip(self, enriched))]
    async fn publish(&self, enriched: EnrichedEvent) -> Result<(), ProcessorError> {
        let json = serde_json::to_vec(&enriched)?;
        let key = enriched.origin.event_id.clone();

        // `send` returns a future that resolves when Kafka acks the message.
        let delivery = self
            .producer
            .send(
                FutureRecord::to(&self.cfg.output_topic)
                    .key(&key)
                    .payload(&json)
                    .headers(
                        OwnedHeaders::new()
                            .add("schema_version", "1")
                            .add("origin_network", &enriched.origin.network),
                    ),
                Timeout::After(Duration::from_secs(5)),
            )
            .await;

        match delivery {
            Ok(_) => Ok(()),
            Err((e, _)) => {
                // Let the caller decide whether to retry or drop.
                Err(ProcessorError::Kafka(e))
            }
        }
    }
}

/// ---------------------------------------------------------------------------
/// Bootstrap
/// ---------------------------------------------------------------------------

#[tokio::main(flavor = "multi_thread")]
async fn main() -> Result<(), ProcessorError> {
    // Initialize tracing.
    tracing_subscriber::fmt()
        .with_env_filter(tracing_subscriber::EnvFilter::from_default_env())
        .init();

    let cfg = ProcessorConfig::default();
    info!(
        brokers = %cfg.brokers,
        in_topic = %cfg.input_topic,
        out_topic = %cfg.output_topic,
        group = %cfg.group_id,
        "starting ChirpPulse processor",
    );

    let registry = Arc::new(StrategyRegistry::new());
    let processor = EventProcessor::new(cfg, registry)?;

    let handle = processor.spawn().await;

    // Wait for the processor task to finish.
    if let Err(e) = handle.await {
        error!(error = %e, "top-level task aborted");
    };

    Ok(())
}
```