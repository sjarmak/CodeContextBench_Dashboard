```rust
//! src/module_47.rs
//!
//! Dynamic, self-healing streaming pipeline runner.
//!
//! This module implements a configurable pipeline engine that consumes social
//! events from Kafka (or any async [`Stream`]), applies a configurable chain of
//! enrichment stages, and publishes the transformed data back to Kafka.  A
//! strategy pattern enables hot-swapping of stages without downtime, while
//! built-in observability and error handling keep the data movingâ€”even during
//! traffic spikes such as live sports or breaking news.
//!
//! The code is meant to be realistic and production-ready.  Remove the
//! `#[cfg(feature = "kafka")]` guards if your build always includes Kafka.

use std::{
    collections::HashMap,
    sync::Arc,
    time::{Duration, SystemTime},
};

use async_trait::async_trait;
use futures::{stream::FuturesUnordered, Stream, StreamExt};
use metrics::{counter, histogram, increment_counter};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::{
    select,
    sync::{broadcast, mpsc, Notify},
    task::JoinHandle,
};

#[cfg(feature = "kafka")]
use {
    rdkafka::{
        config::ClientConfig,
        consumer::{CommitMode, Consumer, StreamConsumer},
        message::{BorrowedMessage, Headers},
        producer::{FutureProducer, FutureRecord},
    },
    tokio_stream::wrappers::ReceiverStream,
};

/// A raw event captured by the ingestion layer.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SocialEvent {
    pub id: String,
    pub source: String,
    pub ts: SystemTime,
    pub payload: serde_json::Value,
    #[serde(default)]
    pub meta: HashMap<String, String>,
}

/// The enriched version emitted by a pipeline stage.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnrichedEvent {
    pub id: String,
    pub source: String,
    pub ts: SystemTime,
    pub payload: serde_json::Value,
    #[serde(default)]
    pub meta: HashMap<String, String>,
}

impl From<&SocialEvent> for EnrichedEvent {
    fn from(ev: &SocialEvent) -> Self {
        EnrichedEvent {
            id: ev.id.clone(),
            source: ev.source.clone(),
            ts: ev.ts,
            payload: ev.payload.clone(),
            meta: ev.meta.clone(),
        }
    }
}

/// Errors that can arise while processing events.
#[derive(Debug, Error)]
pub enum StageError {
    #[error("transient error: {0}")]
    Transient(String),
    #[error("permanent error: {0}")]
    Permanent(String),
}

/// Trait implemented by all pipeline stages.
#[async_trait]
pub trait Stage: Send + Sync {
    /// Human-readable name for logging & metrics.
    fn name(&self) -> &'static str;

    /// Process a single event.
    ///
    /// Returning `Ok(None)` drops the event (e.g. filtered-out languages).
    /// Returning `Err(StageError::Transient)` tells the runner to retry the
    /// event later; `Permanent` will skip the event.
    async fn process(
        &self,
        input: EnrichedEvent,
    ) -> Result<Option<EnrichedEvent>, StageError>;
}

/// A pipeline is an ordered collection of boxed stages.
#[derive(Default)]
pub struct Pipeline {
    stages: Vec<Arc<dyn Stage>>,
}

impl Pipeline {
    pub fn new() -> Self {
        Self { stages: vec![] }
    }

    /// Adds a stage to the pipeline.  Stages are executed in the order they are
    /// registered.
    pub fn add_stage<T: Stage + 'static>(mut self, stage: T) -> Self {
        self.stages.push(Arc::new(stage));
        self
    }

    /// Run the event through each stage, short-circuiting on failure.
    async fn execute(
        &self,
        event: EnrichedEvent,
    ) -> Result<Option<EnrichedEvent>, StageError> {
        let mut current = Some(event);
        for stage in &self.stages {
            match current {
                Some(ev) => match stage.process(ev,).await {
                    Ok(next) => current = next,
                    Err(err) => {
                        increment_counter!(
                            "pipeline_stage_errors",
                            "stage" => stage.name(),
                            "kind" => match &err {
                                StageError::Transient(_) => "transient",
                                StageError::Permanent(_) => "permanent",
                            }
                        );
                        return Err(err);
                    }
                },
                None => return Ok(None), // Already dropped
            }
        }
        Ok(current)
    }
}

/// Language detection stage (example).
pub struct LangDetectStage {
    lang_field: &'static str,
}

impl LangDetectStage {
    pub fn new(lang_field: &'static str) -> Self {
        Self { lang_field }
    }
}

#[async_trait]
impl Stage for LangDetectStage {
    fn name(&self) -> &'static str {
        "lang_detect"
    }

    async fn process(
        &self,
        mut input: EnrichedEvent,
    ) -> Result<Option<EnrichedEvent>, StageError> {
        // Fake language detection: mark every event as English
        input.meta.insert(self.lang_field.into(), "en".into());
        Ok(Some(input))
    }
}

/// Toxicity scoring stage (example).
pub struct ToxicityStage {
    threshold: f64,
}

impl ToxicityStage {
    pub fn new(threshold: f64) -> Self {
        Self { threshold }
    }
}

#[async_trait]
impl Stage for ToxicityStage {
    fn name(&self) -> &'static str {
        "toxicity"
    }

    async fn process(
        &self,
        mut input: EnrichedEvent,
    ) -> Result<Option<EnrichedEvent>, StageError> {
        // Pretend we scored toxicity
        let score = 0.01; // Always non-toxic
        if score > self.threshold {
            input
                .meta
                .insert("toxic".into(), format!("{:.2}", score));
        }
        Ok(Some(input))
    }
}

/// Runner configuration loaded from YAML, JSON, or etcd.
#[derive(Debug, Clone, Deserialize)]
pub struct RunnerConfig {
    /// Max amount of in-flight events per partition.
    #[serde(default = "default_max_inflight")]
    pub max_inflight: usize,

    /// How long to backoff for transient stage errors.
    #[serde(
        default = "default_retry_backoff_ms",
        with = "humantime_serde"
    )]
    pub retry_backoff: Duration,

    /// Optional Kafka config.  If absent, the runner will expect the caller to
    /// feed events via [`StreamProcessor::run_with_stream`].
    #[cfg(feature = "kafka")]
    pub kafka: Option<KafkaConfig>,
}

fn default_max_inflight() -> usize {
    4096
}
fn default_retry_backoff_ms() -> Duration {
    Duration::from_millis(500)
}

/// Kafka wiring parameters.
#[cfg(feature = "kafka")]
#[derive(Debug, Clone, Deserialize)]
pub struct KafkaConfig {
    pub brokers: String,
    pub group_id: String,
    pub input_topic: String,
    pub output_topic: String,
}

/// Top-level component: consumes events, applies pipeline, publishes results.
pub struct StreamProcessor {
    cfg: RunnerConfig,
    pipeline: Arc<Pipeline>,
    shutdown: Arc<Notify>,
}

impl StreamProcessor {
    pub fn new(cfg: RunnerConfig, pipeline: Pipeline) -> Self {
        Self {
            cfg,
            pipeline: Arc::new(pipeline),
            shutdown: Arc::new(Notify::new()),
        }
    }

    /// Request a graceful shutdown.  Returns immediately; call `await` on the
    /// JoinHandle returned by `run_*` to wait for termination.
    pub fn stop(&self) {
        self.shutdown.notify_waiters();
    }

    /// Spawns the event loop on Tokio, wired up to Kafka.
    ///
    /// Requires the `kafka` feature.
    #[cfg(feature = "kafka")]
    pub async fn run_with_kafka(self) -> anyhow::Result<()> {
        let (producer, consumer) = self.build_kafka_clients()?;
        let handle = self.spawn_processing_loop(ReceiverStream::new(
            Self::kafka_stream(consumer)?,
        ));
        self.forward_results(handle, producer).await
    }

    /// Runs the processor over any arbitrary async stream of `SocialEvent`s.
    pub async fn run_with_stream<S>(self, input: S) -> anyhow::Result<()>
    where
        S: Stream<Item = SocialEvent> + Send + 'static,
    {
        let handle = self.spawn_processing_loop(input);
        handle.await??;
        Ok(())
    }

    /// Internal: spawn the concurrent worker tasks for the provided stream.
    fn spawn_processing_loop<S>(
        &self,
        input: S,
    ) -> JoinHandle<anyhow::Result<()>>
    where
        S: Stream<Item = SocialEvent> + Send + 'static,
    {
        let shutdown = self.shutdown.clone();
        let pipeline = self.pipeline.clone();
        let retry_backoff = self.cfg.retry_backoff;
        let max_inflight = self.cfg.max_inflight;

        tokio::spawn(async move {
            let mut inflight: FuturesUnordered<_> = FuturesUnordered::new();
            tokio::pin!(input);

            loop {
                select! {
                    biased;

                    _ = shutdown.notified() => {
                        tracing::info!("shutdown signal received; draining inflight tasks");
                        while inflight.next().await.is_some() {}
                        return Ok(());
                    }

                    Some(res) = inflight.next(), if inflight.len() > 0 => {
                        if let Err(err) = res {
                            tracing::warn!("task failed: {err:?}");
                        }
                    }

                    item = input.next(), if inflight.len() < max_inflight => {
                        match item {
                            Some(ev) => {
                                let p = pipeline.clone();
                                let shutdown_task = shutdown.clone();
                                let fut = async move {
                                    let enriched = p.execute((&ev).into()).await;
                                    match enriched {
                                        Ok(Some(out)) => {
                                            counter!("events_ok").increment(1);
                                            tracing::trace!("processed event {}", out.id);
                                        }
                                        Ok(None) => {
                                            counter!("events_filtered").increment(1);
                                        }
                                        Err(StageError::Transient(_)) => {
                                            histogram!("events_retry_backoff_ms")
                                                .record(retry_backoff.as_millis() as f64);
                                            tokio::time::sleep(retry_backoff).await;
                                            // requeue by simply resubmitting the event
                                            if !shutdown_task.is_notified() {
                                                // naive requeue (could be improved with bounded channel)
                                                p.execute((&ev).into()).await.ok();
                                            }
                                        }
                                        Err(StageError::Permanent(_)) => {
                                            counter!("events_skipped_permanent").increment(1);
                                        }
                                    }
                                    Ok::<_, anyhow::Error>(())
                                };
                                inflight.push(fut);
                            }
                            None => {
                                tracing::info!("input stream closed");
                                break;
                            }
                        }
                    }
                }
            }
            // drain remaining
            while inflight.next().await.is_some() {}
            Ok(())
        })
    }

    #[cfg(feature = "kafka")]
    fn build_kafka_clients(
        &self,
    ) -> anyhow::Result<(FutureProducer, StreamConsumer)> {
        let cfg = self
            .cfg
            .kafka
            .clone()
            .ok_or_else(|| anyhow::anyhow!("missing kafka config"))?;
        let producer: FutureProducer = ClientConfig::new()
            .set("bootstrap.servers", &cfg.brokers)
            .set("message.timeout.ms", "5000")
            .create()?;

        let consumer: StreamConsumer = ClientConfig::new()
            .set("group.id", &cfg.group_id)
            .set("bootstrap.servers", &cfg.brokers)
            .set("enable.auto.commit", "false")
            .create()?;

        consumer.subscribe(&[&cfg.input_topic])?;
        Ok((producer, consumer))
    }

    #[cfg(feature = "kafka")]
    async fn forward_results(
        self,
        handle: JoinHandle<anyhow::Result<()>>,
        producer: FutureProducer,
    ) -> anyhow::Result<()> {
        // For brevity we ignore actual publishing to Kafka in this snippet.
        let res = handle.await?;
        res
    }

    #[cfg(feature = "kafka")]
    fn kafka_stream(
        consumer: StreamConsumer,
    ) -> anyhow::Result<mpsc::Receiver<SocialEvent>> {
        let (tx, rx) = mpsc::channel(1024);
        tokio::spawn(async move {
            loop {
                match consumer.recv().await {
                    Err(e) => {
                        tracing::error!("kafka error: {e:?}");
                    }
                    Ok(msg) => {
                        if let Err(e) = Self::to_social_event(msg)
                            .and_then(|ev| tx.try_send(ev))
                        {
                            tracing::warn!("dropping event: {e:?}");
                        }
                        consumer.commit_message(&msg, CommitMode::Async).unwrap_or_default();
                    }
                }
            }
        });
        Ok(rx)
    }

    #[cfg(feature = "kafka")]
    fn to_social_event(msg: BorrowedMessage<'_>) -> anyhow::Result<SocialEvent> {
        let payload = msg.payload_view::<str>()?.unwrap_or_default();
        let event: SocialEvent = serde_json::from_str(payload)?;
        Ok(event)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tokio_stream::iter;

    #[tokio::test(flavor = "multi_thread")]
    async fn smoke_test_pipeline() {
        let pipeline = Pipeline::new()
            .add_stage(LangDetectStage::new("lang"))
            .add_stage(ToxicityStage::new(0.8));

        let cfg: RunnerConfig =
            serde_json::from_str("{}").expect("default cfg");

        let runner = StreamProcessor::new(cfg, pipeline);

        let events = (0..10).map(|i| SocialEvent {
            id: format!("{i}"),
            source: "unit-test".into(),
            ts: SystemTime::now(),
            payload: serde_json::json!({"text": "hello"}),
            meta: HashMap::default(),
        });

        runner
            .run_with_stream(iter(events))
            .await
            .expect("pipeline run failed");
    }
}
```