```yaml
# =============================================================================
#  OpsForge Utility Nexus – Local Development & CI Docker Compose Stack
# -----------------------------------------------------------------------------
#  This stack spins up the entire micro-services constellation—including the
#  API Gateway, utility services, supporting infrastructure (PostgreSQL, Redis,
#  monitoring, tracing), and an optional Eureka discovery server—using sane
#  defaults so that new contributors can be productive in minutes.
#
#  Each service is deliberately kept self-contained to illustrate the platform’s
#  hexagonal architecture philosophy: inbound & outbound adapters are wired at
#  runtime via environment variables, without bleeding implementation details
#  into the domain layer.
#
#  NOTE: All first-party images are built from local Dockerfiles so that the
#  compose stack always runs the current checkout—no need to publish snapshots
#  to a registry during inner-loop development.
# =============================================================================
version: "3.9"

# -----------------------------------------------------------------------------
#  Global Networks
# -----------------------------------------------------------------------------
networks:
  backend:      # Internal service-to-service traffic
    driver: bridge
  monitoring:   # Prometheus / Grafana / Zipkin traffic
    driver: bridge

# -----------------------------------------------------------------------------
#  Named Volumes for Persistent State
# -----------------------------------------------------------------------------
volumes:
  db_data:          # PostgreSQL
  redis_data:       # Redis AOF
  grafana_data:     # Dashboards, user prefs, etc.

# -----------------------------------------------------------------------------
#  Service Definitions
# -----------------------------------------------------------------------------
services:

  # ---------------------------------------------------------------------------
  #  Spring Cloud Eureka – Optional service discovery mechanism. All first-party
  #  services will register themselves here, but also fall back to static
  #  configuration if Eureka is unavailable.
  # ---------------------------------------------------------------------------
  discovery:
    image: opsforge/eureka-server:1.0.0
    container_name: discovery
    build:
      context: ./eureka-server
      dockerfile: Dockerfile
    restart: unless-stopped
    ports:
      - "8761:8761"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8761/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend

  # ---------------------------------------------------------------------------
  #  API Gateway – Single entry-point that exposes REST & GraphQL façades;
  #  handles cross-cutting concerns like rate-limiting, circuit-breaking, and
  #  response caching. Serves OpenAPI & GraphQL schemas for developer discovery.
  # ---------------------------------------------------------------------------
  gateway:
    image: opsforge/gateway:1.2.0
    container_name: gateway
    build:
      context: .
      dockerfile: Dockerfile.gateway
    env_file:
      - ./env/gateway.env
    restart: unless-stopped
    depends_on:
      discovery:
        condition: service_started
      redis:
        condition: service_healthy
    ports:
      - "8080:8080"   # Public REST & GraphQL
      - "8081:8081"   # Internal management (Actuator)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend
      - monitoring

  # ---------------------------------------------------------------------------
  #  PostgreSQL – Single instance for local development; in production each
  #  micro-service can own its own database or schema.
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    environment:
      POSTGRES_USER: opsforge
      POSTGRES_PASSWORD: opsforge_pwd
      POSTGRES_DB: opsforge_utility_nexus
    volumes:
      - db_data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d  # Schema bootstrap scripts
    restart: unless-stopped
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - backend

  # ---------------------------------------------------------------------------
  #  Redis – Used for response caching, rate-limiting counters, and temporal
  #  state required by the scheduling utility.
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7.2-alpine
    container_name: redis
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis_data:/data
    restart: unless-stopped
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend

  # ---------------------------------------------------------------------------
  #  Utility Micro-Services
  # ---------------------------------------------------------------------------
  file-conversion-service:
    image: opsforge/file-conversion-service:1.2.0
    container_name: file-conversion-service
    build:
      context: ./file-conversion-service
      dockerfile: Dockerfile
    env_file:
      - ./env/file-conversion.env
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      discovery:
        condition: service_started
    ports:
      - "8090:8090"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8090/actuator/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend

  data-anonymization-service:
    image: opsforge/data-anonymization-service:1.2.0
    container_name: data-anonymization-service
    build:
      context: ./data-anonymization-service
      dockerfile: Dockerfile
    env_file:
      - ./env/data-anonymization.env
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      discovery:
        condition: service_started
    ports:
      - "8091:8091"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8091/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend

  time-scheduler-service:
    image: opsforge/time-scheduler-service:1.2.0
    container_name: time-scheduler-service
    build:
      context: ./time-scheduler-service
      dockerfile: Dockerfile
    env_file:
      - ./env/time-scheduler.env
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      discovery:
        condition: service_started
    ports:
      - "8092:8092"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8092/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend

  checksum-service:
    image: opsforge/checksum-service:1.2.0
    container_name: checksum-service
    build:
      context: ./checksum-service
      dockerfile: Dockerfile
    env_file:
      - ./env/checksum.env
    restart: unless-stopped
    depends_on:
      discovery:
        condition: service_started
    ports:
      - "8093:8093"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8093/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend

  text-transform-service:
    image: opsforge/text-transform-service:1.2.0
    container_name: text-transform-service
    build:
      context: ./text-transform-service
      dockerfile: Dockerfile
    env_file:
      - ./env/text-transform.env
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      discovery:
        condition: service_started
    ports:
      - "8094:8094"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8094/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend

  # ---------------------------------------------------------------------------
  #  Observability Stack
  # ---------------------------------------------------------------------------
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    restart: unless-stopped
    ports:
      - "9090:9090"
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin   # ← Change in production!
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    networks:
      - monitoring

  zipkin:
    image: openzipkin/zipkin:2.24
    container_name: zipkin
    environment:
      STORAGE_TYPE: mem
    restart: unless-stopped
    ports:
      - "9411:9411"
    networks:
      - monitoring
```