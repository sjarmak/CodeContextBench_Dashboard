# =========================================================================
#  LexiLearn MVC Orchestrator – N-Gram Analyzer Configuration
#  File: lexilearn_orchestrator/config/models/ngram_analyzer.conf
#  -------------------------------------------------------------------------
#  This configuration drives the classical N-Gram Analyzer Strategy in the
#  Model layer.  The file is parsed at run-time by the ConfigLoader module
#  (see src/core/config/config_loader.c) and is fully reloadable thanks to
#  Observer Pattern hooks.  All keys are snake_case, section names are
#  lowercase ASCII, and values are UTF-8.
#
#  NOTE:
#    • Boolean values are “true” or “false”.
#    • Lists are comma-separated with no spaces.
#    • Environment variables may be interpolated using ${ENV_VAR}.
#    • Lines beginning with “#” are comments and ignored by the parser.
# =========================================================================


# -------------------------------------------------------------------------
# 1. Meta-data
# -------------------------------------------------------------------------
[metadata]
model_name                    = ngram_analyzer
model_family                  = classical
model_description             = "Variable-length n-gram language model with \
                                   Kneser–Ney smoothing; suitable for short \
                                   answers and essay scoring."
author                        = "LexiLearn Core NLP Team"
created                       = 2023-09-11T14:32:00Z
last_updated                  = 2024-03-28T09:15:47Z
license                       = Apache-2.0
tags                          = nlp, ngram, language-model, production
# Semantic versioning; increment minor on backward-compatible changes.
semantic_version              = 3.1.0


# -------------------------------------------------------------------------
# 2. Pre-processing & Tokenization
# -------------------------------------------------------------------------
[preprocessing]
# Lower-case all text prior to tokenization.
lowercase                     = true
# Unicode normalization form applied before any processing (NFC|NFKC|NFD|NFKD)
unicode_normalization         = NFKC
# Replace accented characters with ASCII equivalents.
strip_accents                 = true
# Minimum token length (characters) after stripping.
min_token_length              = 2
# How to treat numbers in text: keep|remove|replace
handle_numbers                = replace
number_placeholder            = "<NUM>"
# Use lexical lemmatizer; requires lexilearn_langlib shared object.
use_lemmatization             = true
# Path to stop-word list (one token per line, UTF-8).
stopword_list_path            = "/etc/lexilearn/stopwords/en.txt"


# -------------------------------------------------------------------------
# 3. Feature Engineering
# -------------------------------------------------------------------------
[feature_engineering]
# N-gram span: min ≤ n ≤ max. 1-3 captures unigrams, bigrams & trigrams.
ngram_min                     = 1
ngram_max                     = 3
# Maximum number of unique features kept after pruning.
max_features                  = 800_000
# Re-weight counts using sub-linear TF scaling: 1 + log(tf)
sublinear_tf                  = true
# Apply inverse-document-frequency re-weighting.
idf                           = true
# Use binary term-occurrence counts instead of frequencies.
binary_weights                = false
# Namespace in the shared feature store; used for cross-model consistency.
feature_store_namespace       = "ngrams_en_v3"


# -------------------------------------------------------------------------
# 4. Training Parameters
# -------------------------------------------------------------------------
[training]
# Document-frequency thresholds for vocabulary pruning.
min_document_frequency        = 3
max_document_frequency        = 0.9
# Class-weighting scheme for supervised learning (balanced|none).
class_weight                  = balanced
# Smoothing settings -------------------------------------------------------
use_laplace_smoothing         = true
laplace_alpha                 = 1.0
use_kneser_ney                = true
kneser_ney_discount           = 0.75
# Random seed for reproducibility.
random_seed                   = 42


# -------------------------------------------------------------------------
# 5. Hyper-parameter Tuning
# -------------------------------------------------------------------------
[hyperparameter_tuning]
# Strategy can be grid|random|bayesian
tuning_strategy               = bayesian
metric                        = macro_f1
max_trials                    = 40
early_stopping_rounds         = 7

# Search-space definitions (name = type(min,max[,step]))
parameter_space.ngram_max                 = int(2,4)
parameter_space.max_features              = int(50_000,1_000_000)
parameter_space.laplace_alpha             = float(0.05,1.5)
parameter_space.kneser_ney_discount       = float(0.1,1.0)
parameter_space.sublinear_tf              = categorical(true,false)
parameter_space.binary_weights            = categorical(true,false)


# -------------------------------------------------------------------------
# 6. Deployment Requirements
# -------------------------------------------------------------------------
[deployment]
# Max 99th-percentile inference latency (in milliseconds).
target_latency_ms             = 4
# Memory budget per model replica.
max_memory_mb                 = 256
# Canary deployment percentage (0–100).
canary_percentage             = 5
enable_rollback               = true
# Expected QPS for autoscaling hints.
expected_qps                  = 250
# SHA-256 digest of the training dataset snapshot; enforces data lineage.
data_snapshot_sha256          = "3e9af4c8e1dffe87ce5b47e0ec7c5d0f96b32407d865fb9127f8a4a3f4f299ab"


# -------------------------------------------------------------------------
# 7. Monitoring & Drift Detection
# -------------------------------------------------------------------------
[monitoring]
enable_drift_detection        = true
# Size of the sliding window (samples) for drift calculations.
drift_window_size             = 5_000
# Kullback–Leibler divergence threshold for alerting.
drift_threshold_kl            = 0.25
alert_webhook_url             = "https://mlops.lexilearn.edu/hooks/model-alerts"
# Location where inference statistics should be pushed (Prometheus endpoint).
metrics_sink                  = "http://prom.lexilearn.net:9090/ingest"


# -------------------------------------------------------------------------
# 8. Automated Re-training Schedule
# -------------------------------------------------------------------------
[retraining]
enable_automated_retraining   = true
# Cron expression (UTC) – weekly on Sunday at 03:00.
retraining_cron               = "0 3 * * 0"
# Trigger retraining if accuracy drops by ≥ 3%.
minimum_accuracy_drop         = 0.03
# Or if additional labelled samples exceed this threshold.
minimum_data_increment        = 10_000
# Minimum days between consecutive retraining runs.
cooldown_days                 = 14
# On success, promote new model to production automatically.
auto_promote_if_better        = true


# -------------------------------------------------------------------------
# 9. Serialization & Versioning
# -------------------------------------------------------------------------
[serialization]
format                        = onnx
compression                   = zstd
# List of compatible semantic versions that can load this model.
version_compatible            = [2.0.0, 2.1.0, 3.0.0, 3.1.0]


# -------------------------------------------------------------------------
# 10. Environment Overrides
# -------------------------------------------------------------------------
# Values in this section can be overridden at runtime via environment
# variables, e.g. export LEXILEARN_NGRAM_TRAIN_PARALLELISM=8
[env_overrides]
train_parallelism             = 4          # Cores used during fitting
inference_batch_size          = 64
log_level                     = INFO


# -------------------------------------------------------------------------
# 11. Internal Integrity Check (auto-generated – DO NOT EDIT)
# -------------------------------------------------------------------------
[checksum]
# CRC32 of the file contents excluding this section.
config_crc32                  = 0x5AF3D279
# =========================================================================
