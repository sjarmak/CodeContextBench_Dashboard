```cpp
#ifndef CARDIO_INSIGHT_360_PROCESSING_STREAM_PROCESSOR_H_
#define CARDIO_INSIGHT_360_PROCESSING_STREAM_PROCESSOR_H_

/**
 *  File:        stream_processor.h
 *  Project:     CardioInsight360 – Unified Healthcare Analytics Engine
 *  Description: Near-real-time stream-processing façade that bridges the event
 *               streaming layer (Kafka) with the transformation/validation
 *               strategy registry and the downstream ETL pipeline.
 *
 *  The StreamProcessor is designed to:
 *    • Consume HL7/FHIR messages from Kafka with at-least-once semantics
 *    • Route decoded payloads to the appropriate transformation strategy
 *    • Execute signal-specific quality-checks
 *    • Forward curated records to the internal Data-Lake Producer
 *    • Expose internal metrics via the Observer Pattern
 *
 *  Author:      Auto-generated by GenAI – ©2024 CardioInsight360
 *
 *  NOTE: Header-only implementation for simplicity. In the real codebase, the
 *        implementation (.cpp) would live side-by-side to keep compile times
 *        and ABI coupling under control.
 */

#include <atomic>
#include <chrono>
#include <condition_variable>
#include <cstdint>
#include <future>
#include <map>
#include <memory>
#include <mutex>
#include <string>
#include <thread>
#include <type_traits>
#include <unordered_map>
#include <utility>
#include <vector>

// Third-party
#include <rdkafka/rdkafka.h>               // Apache Kafka client (librdkafka)
#include <tbb/pipeline.h>                  // Intel Threading Building Blocks
#include <spdlog/spdlog.h>                 // Logging
#include <spdlog/fmt/fmt.h>

// Project-internal
#include "core/signal_types.h"             // SignalType enum (ECG, BP, SpO2…)
#include "processing/transformation_strategy.h"
#include "processing/quality_check_policy.h"
#include "storage/data_lake_producer.h"
#include "monitoring/metrics_observer.h"

namespace cardio_insight_360::processing {

/*───────────────────────────────────────────────────────────────────────────┐
│  Utility structs                                                          │
└───────────────────────────────────────────────────────────────────────────*/
struct RawMessage
{
    std::string payload;   // UTF-8 encoded HL7/FHIR message
    std::chrono::system_clock::time_point received_at;
};

struct CuratedRecord
{
    std::string key;       // Unique identifier (e.g., patient-visit+timestamp)
    std::string binary;    // Serialized, transformed, validated record
    std::chrono::system_clock::time_point created_at;
};

/*───────────────────────────────────────────────────────────────────────────┐
│  RAII wrapper around librdkafka consumer                                  │
└───────────────────────────────────────────────────────────────────────────*/
class KafkaConsumer final
{
public:
    KafkaConsumer(std::string brokers,
                  std::string topic,
                  std::string group_id,
                  int32_t    queue_capacity = 1000)
        : topic_(std::move(topic))
    {
        // Create configuration objects
        rd_kafka_conf_t* conf = rd_kafka_conf_new();
        if (rd_kafka_conf_set(conf, "bootstrap.servers", brokers.c_str(), errstr_, sizeof(errstr_)) != RD_KAFKA_CONF_OK ||
            rd_kafka_conf_set(conf, "group.id", group_id.c_str(), errstr_, sizeof(errstr_))          != RD_KAFKA_CONF_OK)
        {
            throw std::runtime_error(fmt::format("Kafka conf error: {}", errstr_));
        }

        // High-level consumer
        consumer_ = rd_kafka_new(RD_KAFKA_CONSUMER, conf, errstr_, sizeof(errstr_));
        if (!consumer_) {
            throw std::runtime_error(fmt::format("Failed to create Kafka consumer: {}", errstr_));
        }

        // Assign partition list
        subscription_ = rd_kafka_topic_partition_list_new(1);
        rd_kafka_topic_partition_list_add(subscription_, topic_.c_str(), RD_KAFKA_PARTITION_UA);

        if (auto err = rd_kafka_subscribe(consumer_, subscription_); err != RD_KAFKA_RESP_ERR_NO_ERROR) {
            throw std::runtime_error(fmt::format("Failed to subscribe to topic '{}': {}", topic_, rd_kafka_err2str(err)));
        }

        // Internal queue size bound
        rd_kafka_queue_t* queue = rd_kafka_queue_get_main(consumer_);
        rd_kafka_queue_set_consumed_event_cb(queue, nullptr);
        rd_kafka_queue_set_size(queue, queue_capacity);
        rd_kafka_queue_destroy(queue);
    }

    KafkaConsumer(const KafkaConsumer&)            = delete;
    KafkaConsumer& operator=(const KafkaConsumer&) = delete;
    KafkaConsumer(KafkaConsumer&&)                 = delete;
    KafkaConsumer& operator=(KafkaConsumer&&)      = delete;

    ~KafkaConsumer() noexcept
    {
        try {
            close();
        } catch (...) {
            // Destructors must not throw
        }
    }

    bool poll(RawMessage& out_msg, std::chrono::milliseconds timeout)
    {
        auto* rkmsg = rd_kafka_consumer_poll(consumer_, static_cast<int>(timeout.count()));
        if (!rkmsg) {
            return false; // Timeout
        }
        if (rkmsg->err) {
            spdlog::error("Kafka message error: {}", rd_kafka_message_errstr(rkmsg));
            rd_kafka_message_destroy(rkmsg);
            return false;
        }

        out_msg.payload.assign(static_cast<char*>(rkmsg->payload), rkmsg->len);
        out_msg.received_at = std::chrono::system_clock::now();
        rd_kafka_message_destroy(rkmsg);
        return true;
    }

    void close()
    {
        if (consumer_) {
            rd_kafka_consumer_close(consumer_);
            rd_kafka_topic_partition_list_destroy(subscription_);
            rd_kafka_destroy(consumer_);
            consumer_     = nullptr;
            subscription_ = nullptr;
        }
    }

private:
    rd_kafka_t*                    consumer_{nullptr};
    rd_kafka_topic_partition_list* subscription_{nullptr};
    std::string                    topic_;
    char                           errstr_[512]{};
};

/*───────────────────────────────────────────────────────────────────────────┐
│ StreamProcessor                                                           │
└───────────────────────────────────────────────────────────────────────────*/
class StreamProcessor final :
    public std::enable_shared_from_this<StreamProcessor>
{
public:
    using MetricsObserverPtr = std::shared_ptr<monitoring::IMetricsObserver>;

    StreamProcessor(std::string                      brokers,
                    std::string                      topic,
                    std::string                      group_id,
                    std::shared_ptr<storage::DataLakeProducer> data_lake)
        : consumer_(std::make_unique<KafkaConsumer>(std::move(brokers),
                                                    std::move(topic),
                                                    std::move(group_id))),
          data_lake_(std::move(data_lake))
    {}

    ~StreamProcessor()
    {
        stop();
    }

    // Non-copyable
    StreamProcessor(const StreamProcessor&)            = delete;
    StreamProcessor& operator=(const StreamProcessor&) = delete;

    //-----------------------------------------------------------------------
    // Public API
    //-----------------------------------------------------------------------
    void register_transformation(SignalType type,
                                 std::shared_ptr<TransformationStrategy> strategy)
    {
        if (!strategy) { throw std::invalid_argument("strategy cannot be null"); }
        std::lock_guard lk(registry_mtx_);
        transform_registry_[type] = std::move(strategy);
    }

    void register_quality_check(SignalType type,
                                std::shared_ptr<QualityCheckPolicy> policy)
    {
        if (!policy) { throw std::invalid_argument("policy cannot be null"); }
        std::lock_guard lk(registry_mtx_);
        qc_registry_[type] = std::move(policy);
    }

    void attach_metrics_observer(MetricsObserverPtr observer)
    {
        if (!observer) { throw std::invalid_argument("observer cannot be null"); }
        std::lock_guard lk(observer_mtx_);
        metrics_observers_.emplace_back(std::move(observer));
    }

    // Start/Stop control
    void start()
    {
        bool expected = false;
        if (!running_.compare_exchange_strong(expected, true)) {
            return; // already running
        }

        // Launch consumer thread
        consumer_thread_ = std::thread([self = shared_from_this()] {
            self->run_consumer_loop();
        });
    }

    void stop()
    {
        bool expected = true;
        if (!running_.compare_exchange_strong(expected, false)) {
            return; // not running
        }

        // Wait for thread to finish
        if (consumer_thread_.joinable()) {
            consumer_thread_.join();
        }
    }

    bool is_running() const noexcept { return running_.load(); }

private:
    /*=======================================================================
      Internal helpers
    =======================================================================*/
    void run_consumer_loop()
    {
        try {
            tbb::parallel_pipeline(
                /*max_number_of_live_tokens*/ 4,

                // Stage 1: Kafka —> RawMessage
                tbb::make_filter<void, RawMessage>(
                    tbb::filter_mode::serial_in_order,
                    [this](tbb::flow_control& fc) -> RawMessage {
                        if (!running_) { fc.stop(); /* graceful shutdown */ }
                        RawMessage msg;
                        if (!consumer_->poll(msg, std::chrono::milliseconds{100})) {
                            return {}; // Empty will be skipped in next stage
                        }
                        return msg;
                    })

                &

                // Stage 2: RawMessage —> (SignalType, CuratedRecord)
                tbb::make_filter<RawMessage,
                                 std::pair<SignalType, CuratedRecord>>(
                    tbb::filter_mode::parallel,
                    [this](RawMessage&& raw) {
                        return decode_and_transform(std::move(raw));
                    })

                &

                // Stage 3: QC & Data-Lake persist
                tbb::make_filter<std::pair<SignalType, CuratedRecord>, void>(
                    tbb::filter_mode::parallel,
                    [this](std::pair<SignalType, CuratedRecord>&& item) {
                        validate_and_persist(std::move(item));
                    })
            ); // pipeline
        }
        catch (const std::exception& ex)
        {
            spdlog::critical("StreamProcessor loop terminated due to exception: {}", ex.what());
        }
        catch (...)
        {
            spdlog::critical("StreamProcessor loop terminated due to unknown exception");
        }
    }

    std::pair<SignalType, CuratedRecord>
    decode_and_transform(RawMessage&& raw_msg)
    {
        using clock = std::chrono::system_clock;

        // 1) Basic HL7/FHIR parsing.
        //    NOTE: Real implementation would leverage a dedicated parser.
        auto start = clock::now();
        auto signal_type = infer_signal_type(raw_msg.payload);
        TransformationStrategyPtr strategy = lookup_transform(signal_type);

        auto record = strategy->transform(raw_msg.payload);

        // 2) Build CuratedRecord
        CuratedRecord curated;
        curated.key         = record.key;
        curated.binary      = std::move(record.binary);
        curated.created_at  = clock::now();

        // 3) Metrics
        publish_metric("transform_latency_ms",
                       std::chrono::duration_cast<std::chrono::milliseconds>(
                           curated.created_at - start).count());

        return { signal_type, std::move(curated) };
    }

    void validate_and_persist(std::pair<SignalType, CuratedRecord>&& in)
    {
        auto [type, curated] = std::move(in);

        // 1) Quality check
        auto* qc = lookup_qc(type);
        if (qc && !qc->validate(curated.binary)) {
            spdlog::warn("QC failed for record {}", curated.key);
            publish_metric("qc_fail", 1);
            return;
        }

        // 2) Persist to Data-Lake
        try {
            data_lake_->produce(curated.key, curated.binary);
            publish_metric("records_ingested", 1);
        }
        catch (const std::exception& ex) {
            spdlog::error("Failed to persist record {}: {}", curated.key, ex.what());
            publish_metric("persist_errors", 1);
        }
    }

    /*=======================================================================
      Registry helpers (thread-safe)
    =======================================================================*/
    using TransformationStrategyPtr = std::shared_ptr<TransformationStrategy>;
    using QualityCheckPolicyPtr     = std::shared_ptr<QualityCheckPolicy>;

    TransformationStrategyPtr lookup_transform(SignalType type)
    {
        std::lock_guard lk(registry_mtx_);
        auto it = transform_registry_.find(type);
        if (it == transform_registry_.end()) {
            throw std::runtime_error(fmt::format(
                "No transformation strategy registered for SignalType '{}'",
                signal_type_to_string(type)));
        }
        return it->second;
    }

    QualityCheckPolicy* lookup_qc(SignalType type)
    {
        std::lock_guard lk(registry_mtx_);
        auto it = qc_registry_.find(type);
        return it != qc_registry_.end() ? it->second.get() : nullptr;
    }

    /*=======================================================================
      Observability
    =======================================================================*/
    void publish_metric(const std::string& name, int64_t value)
    {
        std::lock_guard lk(observer_mtx_);
        for (auto& obs : metrics_observers_) {
            obs->on_metric(name, value);
        }
    }

    /*=======================================================================
      Naïve Signal-Type inference (placeholder)
    =======================================================================*/
    static SignalType infer_signal_type(std::string_view payload)
    {
        // In production this would examine HL7 segment codes. Here fallback to ECG
        (void)payload;
        return SignalType::ECG;
    }

    /*=======================================================================
      Members
    =======================================================================*/
    std::unique_ptr<KafkaConsumer> consumer_;                 // Kafka consumer
    std::shared_ptr<storage::DataLakeProducer> data_lake_;    // Output sink

    std::atomic_bool running_{false};
    std::thread      consumer_thread_;

    // Registries
    std::mutex                                                 registry_mtx_;
    std::unordered_map<SignalType, TransformationStrategyPtr>  transform_registry_;
    std::unordered_map<SignalType, QualityCheckPolicyPtr>      qc_registry_;

    // Observer Pattern
    std::mutex                              observer_mtx_;
    std::vector<MetricsObserverPtr>         metrics_observers_;
};

} // namespace cardio_insight_360::processing

#endif // CARDIO_INSIGHT_360_PROCESSING_STREAM_PROCESSOR_H_
```