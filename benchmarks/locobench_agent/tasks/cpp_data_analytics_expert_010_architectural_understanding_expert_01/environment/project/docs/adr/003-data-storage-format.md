```markdown
# ADR-003: Data-Storage Format

*Status*: **Accepted**  
*Date*: 2024-04-27  
*Authors*: Data Platform Working Group (CardioInsight360)

---

## 1. Context

CardioInsight360 ingests heterogeneous, high-volume cardiology data streams:

* HL7/FHIR messages from hospital information systems  
* Continuous ECG telemetry from bedside monitors  
* Episodic uploads from wearable devices (e.g., Holter)  
* DICOM and structured reports from imaging archives  

The platform must subsequently:

1. Persist raw, unmodified payloads to satisfy regulatory traceability.  
2. Persist **curated** (validated, normalized, de-identified) datasets to power near-real-time analytics and long-term cohort studies.  
3. Provide high-throughput, partition-friendly read/write semantics for ETL jobs executed in Intel TBB-based pipelines.  
4. Remain storage-agnostic (direct-attached SSD, NAS, object storage via S3-compatible gateway) without recompilation.  
5. Offer forward-compatible schemas that survive clinical guideline updates (e.g., new ECG lead sets or ICD-10 revisions).

After surveying Apache Parquet, ORC, Avro, Protobuf, FlatBuffers, and proprietary binary formats, we must decide on the canonical **on-disk** representation for both raw and curated zones of the embedded Data Lake faÃ§ade.

---

## 2. Decision

We will adopt **Apache Parquet v2.0** as the primary on-disk columnar format, wrapped by a thin abstraction layer named `ParquetStorageBackend`. The wrapper exposes a **stable C++17 interface** that:

1. Serialises domain objects (`EcgFrame`, `BloodPressureSample`, `DeviceEvent`) into Parquet **row groups** using the [Arrow C++](https://arrow.apache.org/docs/cpp/) API.
2. Adds mandatory metadata footers:
   * `ci360.schemaVersion` â€“ semantic version of the logical schema  
   * `ci360.sourceFormat` â€“ `HL7`, `FHIR`, `DICOM`, `CSV`, â€¦  
   * `ci360.checksum` â€“ SHA-256 of the raw payload (reg-traceability)  
3. Encrypts Parquet **page data** with AES-256-GCM leveraging [Parquet Modular Encryption](https://github.com/apache/parquet-format/blob/master/Encryption.md).
4. Compresses with **Zstandard (zstd) level 3** for balanced CPU/IO utilisation on clinical-grade hardware.
5. Exposes **streaming-ingest** helpers that flush row groups every N seconds or M MiB, whichever comes firstâ€”tuned by runtime configuration.
6. Supports pluggable **partitioning strategies** via Strategy Pattern (`ByDay`, `ByPatient`, `ByInstitution+Modality`, â€¦).

For completely raw payload preservation we use the same Parquet container but store only two columns:

```
| timestamp (INT64: microseconds UTC) | raw_payload (BYTE_ARRAY: binary) |
```

This aligns lineage while enforcing a single storage technology for both raw and curated layers.

---

## 3. Consequences

ğŸ¯  Unified Format  
    â€¢ Simplifies operational tooling (single reader/writer, centralised schema registry).  
    â€¢ Enables predicate push-down and column pruning to accelerate cohort analytics.

ğŸ”’  Compliance  
    â€¢ AES-256-GCM at the page level meets HIPAA and regional data-at-rest mandates.  
    â€¢ Checksums + metadata footers facilitate immutable audits.

âš™ï¸  Performance  
    â€¢ Columnar layout plus zstd yields 6-8Ã— smaller footprint vs. raw JSON while sustaining >250 MiB/s ingest on commodity NVMe.  
    â€¢ Flight/Arrow zero-copy buffers cooperates with TBB for high-parallel ETL.

ğŸ§©  Extensibility  
    â€¢ New physiological modalities (e.g., invasive pressure waveforms) require only schema additions, not new file formats.  
    â€¢ Partitioning strategies are hot-swappable at runtime.

ğŸ¤  Vendor-Neutral  
    â€¢ Parquet is openly specified and widely supported (Spark, Presto, DuckDB), enabling external researchers to run ad-hoc queries without proprietary SDKs.

---

## 4. Rejected Alternatives

* **ORC** â€“ excellent for Hive/Spark, weaker C++ story and fewer healthcare-focused encryption examples.  
* **Avro / Protobuf** â€“ row-oriented, impractical for analytic scan workloads, lacks vectorised reads.  
* **SQLite / PostgreSQL** â€“ row storage; heavy dependencies and migration overhead for embedded edge deployments.  
* **FlatBuffers** â€“ optimal for in-memory transport, but poor compression and tooling for long-term archival.  

---

## 5. Reference Implementation (excerpt)

Below is a **production-grade**, unit-testable snippet from `ParquetStorageBackend.hpp/cpp` illustrating key design points.  
The full implementation lives under `storage/backends/` in the monorepo.

```cpp
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// File: storage/backends/ParquetStorageBackend.hpp
// Description: Parquet-backed StorageBackend implementation for CardioInsight360
// Author: CardioInsight360 Data Platform Team
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#pragma once

#include <chrono>
#include <filesystem>
#include <memory>
#include <string>
#include <unordered_map>

#include <arrow/io/file.h>
#include <arrow/ipc/api.h>
#include <parquet/arrow/writer.h>
#include <parquet/encryption/encryption.h>

#include "core/DomainTypes.hpp"
#include "storage/PartitionStrategy.hpp"

namespace ci360::storage {

struct ParquetOptions
{
    std::string compression         = "ZSTD";
    int         compressionLevel    = 3;
    bool        enableEncryption    = true;
    std::string kmsKeyId            = "local-kms://ci360/parquet/aes-gcm";
    std::chrono::seconds flushEvery = std::chrono::seconds{2};
    size_t      flushBytes          = 4_MiB;  // user-defined literal
};

class ParquetStorageBackend final : public IStorageBackend
{
public:
    ParquetStorageBackend(std::filesystem::path               root,
                          ParquetOptions                      opts,
                          std::unique_ptr<PartitionStrategy>  partitioner);

    void write(const DomainRecord& record) override;
    std::vector<DomainRecord> read(const RecordQuery& query) override;
    void flush() override;
    void close() override;

private:
    struct WriterCtx
    {
        std::shared_ptr<arrow::Schema>            schema;
        std::unique_ptr<parquet::arrow::FileWriter> writer;
        size_t                                    bufferedBytes = 0;
        std::chrono::steady_clock::time_point     lastFlush;
    };

    std::filesystem::path makePath(const DomainRecord& rec) const;
    std::shared_ptr<arrow::Schema> ensureSchema(const DomainRecord& rec);
    void rotateFileIfNeeded(WriterCtx& ctx);

    std::filesystem::path                  m_root;
    ParquetOptions                         m_opts;
    std::unique_ptr<PartitionStrategy>     m_partitioner;
    std::unordered_map<std::string, WriterCtx> m_writers;
    std::mutex                             m_mutex;
};

}   // namespace ci360::storage
```

```cpp
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// File: storage/backends/ParquetStorageBackend.cpp
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#include "ParquetStorageBackend.hpp"

#include <arrow/buffer.h>
#include <arrow/io/file.h>
#include <parquet/arrow/schema.h>
#include <parquet/encryption/encryption.h>
#include <spdlog/spdlog.h>

using namespace ci360::storage;

namespace {

constexpr size_t operator"" _MiB(unsigned long long n) { return n * 1024 * 1024; }

void throwIfNotOk(const arrow::Status& s)
{
    if (!s.ok()) { throw std::runtime_error(s.ToString()); }
}

} // anonymous namespace

ParquetStorageBackend::ParquetStorageBackend(std::filesystem::path               root,
                                             ParquetOptions                      opts,
                                             std::unique_ptr<PartitionStrategy>  partitioner)
    : m_root(std::move(root))
    , m_opts(std::move(opts))
    , m_partitioner(std::move(partitioner))
{
    if (!std::filesystem::exists(m_root))
    {
        std::filesystem::create_directories(m_root);
    }
}

void ParquetStorageBackend::write(const DomainRecord& rec)
{
    const auto path = makePath(rec);
    const auto key  = path.string();

    std::scoped_lock lk(m_mutex);
    auto& ctx = m_writers[key];                     // creates if absent
    if (!ctx.writer)
    {
        ctx.schema = ensureSchema(rec);

        // open Arrow file output stream
        auto outfileResult = arrow::io::FileOutputStream::Open(path, /*truncate=*/true);
        throwIfNotOk(outfileResult.status());
        auto outfile = *outfileResult;

        // Encryption
        std::shared_ptr<parquet::FileEncryptionProperties> encryptionProps = nullptr;
        if (m_opts.enableEncryption)
        {
            encryptionProps = parquet::FileEncryptionProperties::Builder()
                                  .key(m_opts.kmsKeyId)
                                  .algorithm(parquet::ParquetCipher::AES_GCM_V1)
                                  .build();
        }

        // Writer properties
        parquet::WriterProperties::Builder writerPropsBuilder;
        writerPropsBuilder.compression(parquet::Compression::type::ZSTD)
            ->compression_level(m_opts.compressionLevel);

        if (encryptionProps)
        {
            writerPropsBuilder.encryption(encryptionProps);
        }

        throwIfNotOk(parquet::arrow::FileWriter::Open(
            *ctx.schema,
            arrow::default_memory_pool(),
            outfile,
            writerPropsBuilder.build(),
            /*arrow_properties=*/nullptr,
            &ctx.writer));

        ctx.lastFlush = std::chrono::steady_clock::now();
    }

    // Append record batch
    auto batch = rec.toArrowBatch(ctx.schema);
    throwIfNotOk(ctx.writer->WriteRecordBatch(*batch));
    ctx.bufferedBytes += batch->total_bytes();

    rotateFileIfNeeded(ctx);
}

void ParquetStorageBackend::rotateFileIfNeeded(WriterCtx& ctx)
{
    const auto now = std::chrono::steady_clock::now();
    const bool timeExceeded  = (now - ctx.lastFlush) >= m_opts.flushEvery;
    const bool sizeExceeded  = ctx.bufferedBytes >= m_opts.flushBytes;

    if (timeExceeded || sizeExceeded)
    {
        throwIfNotOk(ctx.writer->Close());
        ctx.writer.reset();
        ctx.bufferedBytes = 0;
        spdlog::info("ParquetStorageBackend flushed file (timeExceeded={}, sizeExceeded={})",
                     timeExceeded, sizeExceeded);
    }
}

std::filesystem::path ParquetStorageBackend::makePath(const DomainRecord& rec) const
{
    auto partition = m_partitioner->partition(rec.timestamp());
    std::filesystem::path p = m_root / partition / (rec.typeName() + ".parquet");
    std::filesystem::create_directories(p.parent_path());
    return p;
}

std::shared_ptr<arrow::Schema> ParquetStorageBackend::ensureSchema(const DomainRecord& rec)
{
    if (auto s = ci360::schema::registry().tryGet(rec.typeId()); s) { return s; }
    // Generates and caches Arrow schema lazily
    auto schema = rec.inferArrowSchema();
    ci360::schema::registry().put(rec.typeId(), schema);
    return schema;
}

std::vector<DomainRecord> ParquetStorageBackend::read(const RecordQuery& query)
{
    // Implementation sketch: open reader for each candidate partition,
    // predicate-pushdown via Arrow dataset API, assemble DomainRecord list
    return {}; // trimmed for brevity
}

void ParquetStorageBackend::flush()
{
    std::scoped_lock lk(m_mutex);
    for (auto& [_, ctx] : m_writers)
    {
        if (ctx.writer) { throwIfNotOk(ctx.writer->Close()); }
    }
    m_writers.clear();
}

void ParquetStorageBackend::close()
{
    flush();
}

```

---

## 6. Migration Plan

1. Extend schema registry to serialise Parquet `schemaVersion` metadata.  
2. Implement a **one-time migrator** that converts legacy SQLite pages into Parquet files, validating SHA-256 checksum equality.  
3. Update ETL Pipeline default sink to `ParquetStorageBackend` (feature flag `--sink=parquet`).  
4. Deprecate legacy `SqliteStorageBackend` after two minor releases and remove in the next major.  
5. Provide Grafana dashboards to track ingestion and flush latencies during rollout.

---

## 7. Open Questions

* KMS integration for on-prem hospitals lacking enterprise key vaults (fallback to local envelope keys?).  
* Dataset versioningâ€”Do we embed `delta-log` metadata Ã  la Delta Lake?  
* Partition explosion risk for long-tail research cohorts; may need manifest-based indexing.

---

> **Decision:** Adopt Parquet v2.0 with ZSTD + AES-256-GCM via `ParquetStorageBackend` as the universal file format for the embedded Data Lake.
```
