{
  "selection_criteria": {
    "min_context_length": 50000,
    "min_files_count": 5,
    "context_weight": 0.3,
    "files_weight": 0.3,
    "category_weight": 0.4,
    "category_bonuses": {
      "architectural_understanding": 1.0,
      "cross_file_refactoring": 0.9,
      "bug_investigation": 0.8,
      "security_analysis": 0.7,
      "feature_implementation": 0.5,
      "code_comprehension": 0.4,
      "integration_testing": 0.3,
      "multi_session_development": 0.3
    },
    "top_n": 50
  },
  "statistics": {
    "total_tasks": 8000,
    "filtered_tasks": 7110,
    "selected_tasks": 50
  },
  "tasks": [
    {
      "id": "csharp_data_warehouse_expert_012_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for Micro-Batch Processing Implementation",
      "context_length": 1112012,
      "files_count": 86,
      "task_prompt": "As the lead architect, you need to provide a technical assessment to the engineering team. Your analysis must be based on the provided source files.\n\n1.  **Identify Core Components:** Examine the codebase and identify the primary modules that constitute the existing **batch processing pipeline** and the **stream processing pipeline**. List the module filenames for each pipeline.\n\n2.  **Analyze the Bottleneck:** Based on the code in the batch processing modules, explain in detail why the current batch architecture is ill-suited for the new low-latency, high-frequency micro-batching requirement. Pinpoint specific architectural patterns, dependencies, or implementation details that would lead to poor performance or high operational cost in a micro-batch scenario.\n\n3.  **Propose an Architectural Solution:** Outline a high-level architectural modification to support micro-batching. Your proposal should prioritize code reuse and minimize disruption to the existing, stable pipelines. Specify which existing modules could be reused or adapted, and describe the function of any new components that might be necessary. Justify your design by explaining how it overcomes the bottlenecks you identified.",
      "ground_truth": "Based on a hypothetical analysis of the codebase:\n\n1.  **Component Identification:**\n    -   **Batch Pipeline:** The core components are `module_10` (Timer-triggered Orchestrator), `module_34` (Data Fetcher for large files from an FTP/Blob source), and `module_65` (Heavy, multi-stage data transformation and aggregation).\n    -   **Stream Pipeline:** The core components are `module_7` (Event Hub/Queue Trigger), `module_26` (Data Enrichment/Validation), and `module_80` (Real-time Sink/Writer).\n\n2.  **Bottleneck Analysis:**\n    -   The batch orchestrator, `module_10`, is configured with a timer schedule (`Cron` expression in the config) that cannot be set to less than one minute and is designed for singleton execution to prevent overlapping runs.\n    -   The data fetcher, `module_34`, is optimized for large files. It lists all files in a directory and downloads them, an operation that is inefficient and slow for small, frequent data drops.\n    -   The primary bottleneck is `module_65`. Its constructor or an early-stage method loads several large lookup tables and a pre-trained ML model from storage into memory. This initialization takes 20-30 seconds, which is unacceptable for a 90-second total SLA on a 30-second interval. The architecture assumes a long-running job where this startup cost is amortized over a large dataset.\n\n3.  **Proposed Architectural Solution:**\n    -   Create a new **Micro-Batch Orchestrator**, `module_90` (a new file). This module will be triggered by a message on a dedicated queue, which an upstream data source will populate every 30-60 seconds.\n    -   Reuse the stream processing trigger pattern from `module_7` for the new orchestrator.\n    -   The new orchestrator will call the existing transformation logic in `module_65`. However, `module_65` must be refactored. The expensive initialization logic (loading lookup tables/models) should be moved into a static constructor or a singleton service that is injected via dependency injection. This ensures the resources are loaded only once per function instance, and subsequent calls on a 'warm' instance are fast.\n    -   The data fetching logic from `module_34` cannot be reused directly. The new orchestrator will receive the data payload directly from the trigger's message, bypassing the need for a separate file-fetching step. The core data parsing logic within `module_34` can be extracted into a shared helper class and called by both the old batch pipeline and the new micro-batch orchestrator.\n    -   This approach creates a new, parallel path for micro-batches that reuses the most complex business logic (`module_65`) while creating a new, lightweight orchestration and data ingress mechanism better suited for low-latency requirements.",
      "evaluation_criteria": [
        "**Correctness of Component Identification:** Was the agent able to accurately identify the modules belonging to the batch and stream pipelines?",
        "**Depth of Bottleneck Analysis:** Did the agent correctly identify the specific reasons (e.g., startup cost in `module_65`, inefficient data fetching in `module_34`, timer limitations in `module_10`) why the batch pipeline is unsuitable, referencing specific implementation patterns?",
        "**Architectural Soundness of Proposal:** Is the proposed solution technically feasible, efficient, and well-reasoned? Does it correctly identify which components to reuse, refactor, or create?",
        "**Adherence to Constraints:** Does the proposal respect the constraints of minimizing disruption and maximizing code reuse?",
        "**Clarity of Justification:** Is the rationale behind the proposed architecture clearly articulated, linking the solution directly back to the identified problems?",
        "**Code-Grounded Reasoning:** Are the agent's claims and proposals supported by plausible inferences about the (unseen) code in the specified modules, rather than generic architectural advice?"
      ],
      "language": "csharp",
      "score": 0.9757
    },
    {
      "id": "rust_web_social_expert_073_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for Real-time Feature Integration in an Event-Driven System",
      "context_length": 1064755,
      "files_count": 88,
      "task_prompt": "Your task is to perform a thorough architectural analysis and create a strategic plan for integrating a new 'Real-time Collaborative Whiteboard' feature. You must not write any implementation code. Your response should be a detailed report based on your understanding of the existing codebase.\n\n1.  **Identify Core Architectural Components:** Analyze the provided files (`config.txt`, `package.json`, and `src/module_*.txt` files) to identify and describe the roles of the key components in the existing event-processing pipeline. Specifically, identify the modules responsible for:\n    *   API Gateway / Handling incoming HTTP requests.\n    *   User Authentication and Session Management.\n    *   Publishing commands/events to the message bus.\n    *   Consuming events to update data stores (write models).\n    *   Serving read-optimized data (query models).\n\n2.  **Trace an Existing Data Flow:** Describe the complete lifecycle of a user creating a new post in a study group. Trace this action from the initial API call through the event bus to the final data persistence, naming the specific modules (`module_XX.txt`) involved at each stage.\n\n3.  **Propose an Integration Strategy:** Design a high-level architectural solution for the 'Real-time Collaborative Whiteboard' feature. Your proposal must address:\n    *   The mechanism for handling low-latency, real-time communication (e.g., drawing actions) between clients.\n    *   The new service(s) or module(s) required.\n    *   How whiteboard state will be persisted without compromising the performance of the core application.\n    *   How the new feature will integrate with the existing user authentication and event bus.\n\n4.  **Visualize the Architecture:** Generate two diagrams in MermaidJS `graph TD` format:\n    *   **Diagram 1:** The existing architecture, showing the flow you traced in step 2.\n    *   **Diagram 2:** The proposed architecture, incorporating the new Collaborative Whiteboard components.\n\n5.  **Identify Architectural Risks:** List and explain the top 3 potential architectural risks or bottlenecks associated with your proposed integration. Focus on performance, scalability, and data consistency.",
      "ground_truth": "The hidden architecture is a modular monolith implementing CQRS with a NATS message bus.\n\n-   **Core Components:**\n    -   `package.json` dependencies include `axum`, `tokio`, `sqlx`, `nats`, `serde`, `jsonwebtoken`.\n    -   `src/config.txt` defines `NATS_URL`, `DATABASE_WRITE_URL`, `DATABASE_READ_URL`, and `JWT_SECRET`.\n    -   **API Gateway:** `src/module_1.txt` contains the `axum::Router` setup. It receives HTTP requests and uses `module_15` for auth.\n    -   **Authentication:** `src/module_15.txt` handles JWT validation and session logic.\n    -   **Command Publishing:** The API Gateway (`module_1`) publishes commands (e.g., `CreatePostCommand`) to the NATS bus after a successful request.\n    -   **Command Handlers:** `src/module_68.txt` and `src/module_35.txt` subscribe to command topics. They contain business logic and, upon success, publish domain events (e.g., `PostCreatedEvent`).\n    -   **Event Consumers (Write-Side):** `src/module_48.txt` and `src/module_22.txt` are consumers that subscribe to domain events and update the 'write' database, which is the source of truth.\n    -   **Query Service / Denormalizers:** `src/module_4.txt` is a separate service/module that also consumes domain events to update a denormalized 'read' database. It exposes read-only API endpoints for fetching data efficiently.\n\n-   **Data Flow (Create Post):**\n    1.  HTTP POST `/posts` hits `module_1`.\n    2.  `module_1` authenticates the user via `module_15`.\n    3.  `module_1` constructs and publishes a `CreatePostCommand` to the `commands.posts` NATS topic.\n    4.  `module_68` (Post Command Handler) consumes the command, validates it, saves the core post data to the 'write' DB, and publishes a `PostCreatedEvent` to the `events.posts` topic.\n    5.  `module_4` (Query Denormalizer) consumes the `PostCreatedEvent` and updates its 'read' database with the new post data for fast retrieval.\n\n-   **Proposed Whiteboard Architecture:**\n    -   A new, independent module (`ProposedWhiteboardService`) that manages WebSocket connections.\n    -   This service subscribes to the main NATS bus for events like `StudyGroupSessionStarted` to know when to create a whiteboard session.\n    -   Client-side drawing actions are sent over the WebSocket directly to this service.\n    -   The service broadcasts drawing data to all other clients in the same session's WebSocket room.\n    -   For persistence, the service takes a snapshot of the whiteboard state (e.g., as SVG or JSON) every 10-20 seconds and issues a `SaveWhiteboardSnapshotCommand` to the NATS bus, which is then handled by a standard command handler to save to the primary DB.\n\n-   **Diagrams (MermaidJS):** The ground truth would include valid MermaidJS code for the 'Existing' and 'Proposed' architectures, reflecting the component interactions described above.\n\n-   **Risks:**\n    1.  **Scalability of WebSocket Service:** The new service is stateful (maintaining active connections) and could become a bottleneck. It needs to be designed for horizontal scaling.\n    2.  **Increased Load on Auth Service:** Every WebSocket connection will need to be authenticated, potentially DDOSing the auth module (`module_15`) if not handled carefully (e.g., with tickets or token-based auth).\n    3.  **Data Consistency:** There's a risk of inconsistency between the real-time state seen by users and the persisted snapshot in the database if the server crashes between snapshots.",
      "evaluation_criteria": [
        "**Component Identification Accuracy:** How accurately did the agent identify the roles of the key modules (`module_1`, `module_15`, `module_68`, `module_4`, etc.) based on code analysis?",
        "**Architectural Pattern Recognition:** Did the agent correctly identify the high-level patterns in use, specifically Event-Driven Architecture and CQRS, citing evidence like separate DB URLs or command/event naming?",
        "**Data Flow Analysis:** Was the agent's trace of the 'Create Post' flow logical and did it reference the correct sequence of component interactions (API -> Command -> Handler -> Event -> Consumer)?",
        "**Proposed Solution Viability:** Is the proposed architecture for the whiteboard feature sound? Does it correctly identify WebSockets as the appropriate technology and propose a reasonable strategy for state management and persistence?",
        "**Visualization Quality:** Are the MermaidJS diagrams syntactically correct, clear, and do they accurately represent both the existing and proposed architectures as described in the analysis?",
        "**Risk Assessment Insight:** Are the identified risks relevant to the proposed architecture? Does the explanation demonstrate a deep understanding of the trade-offs in distributed, real-time systems (e.g., stateful vs. stateless services, consistency vs. performance)?"
      ],
      "language": "rust",
      "score": 0.9703
    },
    {
      "id": "c_blockchain_nft_expert_071_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Bottleneck Analysis for NFT Minting Throughput",
      "context_length": 1092663,
      "files_count": 84,
      "task_prompt": "1. **Analyze and document the end-to-end data flow** for a new generative artifact (NFT) being minted and subsequently appearing in the `gallery_gateway`. Your analysis must trace the request across the relevant microservices, starting from the user interaction and ending with the data being queryable. Refer to the provided source code, especially the inter-service communication mechanisms.\n2. **Identify the most likely architectural bottleneck** in this flow that would cause the described latency during a high-volume minting event. Justify your choice with evidence from the system's design (e.g., synchronous vs. asynchronous processing, on-chain vs. off-chain computation, data consistency models).\n3. **Propose a specific, high-level architectural modification** to alleviate this bottleneck. Your proposal should not be a minor code fix but a change in how the services interact. Describe the new data flow and justify your design by explaining the trade-offs (e.g., in terms of complexity, cost, or data consistency).\n4. **List the key files and the primary components/functions** within them that would need to be modified to implement your proposed solution. You do not need to write the code, only identify the modification points.",
      "ground_truth": {
        "data_flow_analysis": "The minting flow is: `wallet_proxy` -> `gallery_gateway` (API request) -> `ledger_core` (on-chain transaction submission) -> `consensus_manager` (block finalization) -> Kafka event (`TransactionConfirmed`) -> `mint_factory` (event consumption, artifact generation via `recipe_composer` and `artifact_factory`) -> Kafka event (`ArtifactCreated`) -> `gallery_gateway` (`query_service` consumes event and updates local state).",
        "bottleneck_identification": "The primary architectural bottleneck is the serial processing of each mint request as an individual, on-chain transaction that must be finalized by the `ledger_core`'s consensus mechanism. This creates a low-throughput choke point, as the system's minting rate is rigidly tied to the blockchain's transaction-per-second (TPS) limit. A secondary bottleneck is the synchronous, computationally expensive art generation within the `mint_factory`'s event handler, which can cause back-pressure on the Kafka topic.",
        "proposed_solution": "Implement a Layer-2-style batching mechanism for minting.\n1. **Introduce a Minting Intent Queue**: The `gallery_gateway` will no longer submit transactions directly to the ledger. Instead, it will publish a lightweight 'MintIntent' message to a dedicated, high-throughput Kafka topic.\n2. **Create a Batch Aggregator**: A new component, either within `ledger_core` or as a separate microservice, will consume from the 'MintIntent' queue. It will collect multiple intents into a 'batch', compute a Merkle root of the batch, and submit a single `BatchCommit` transaction to the `ledger_core`. The full batch data can be stored off-chain (e.g., on IPFS).\n3. **Modify Downstream Consumers**: The `mint_factory` will be updated to listen for a `BatchConfirmed` event from the ledger. Upon receiving it, it will fetch the full batch data and can process the artifact generation for all items in the batch, potentially in parallel using a worker pool. This decouples the slow generation process from the main event loop.",
        "key_files_for_modification": [
          {
            "file": "HoloCanvas/services/gallery_gateway/src/api_handler.c",
            "reason": "Change logic from submitting a transaction to `ledger_core` to publishing a `MintIntent` message to the new Kafka topic."
          },
          {
            "file": "HoloCanvas/services/ledger_core/src/transaction_processor.c",
            "reason": "Needs to be modified to recognize and process the new `BatchCommit` transaction type, validating the Merkle root against the state."
          },
          {
            "file": "HoloCanvas/services/ledger_core/src/batch_aggregator.c",
            "reason": "A new file/component responsible for consuming from the `MintIntent` queue, creating batches, and submitting the `BatchCommit` transaction."
          },
          {
            "file": "HoloCanvas/services/mint_factory/src/event_handler.c",
            "reason": "Change logic to consume `BatchConfirmed` events instead of individual `TransactionConfirmed` events, and to dispatch batch processing work."
          },
          {
            "file": "HoloCanvas/shared/protocol/holocanvas.proto",
            "reason": "Define new message types for `MintIntent` and `BatchCommit`."
          },
          {
            "file": "HoloCanvas/ARCHITECTURE.md",
            "reason": "Document the new batch-minting architecture and data flow."
          }
        ]
      },
      "evaluation_criteria": [
        "**Correct Flow Identification:** Accurately traces the minting data flow across at least `gallery_gateway`, `ledger_core`, and `mint_factory`, and correctly identifies Kafka as the communication bus.",
        "**Accurate Bottleneck Diagnosis:** Correctly identifies the on-chain, serial transaction processing in `ledger_core` as the primary scalability bottleneck.",
        "**Architectural Solution Viability:** Proposes a viable architectural solution, such as batching or a rollup mechanism, that fundamentally addresses the identified bottleneck.",
        "**Analysis of Trade-offs:** Discusses the trade-offs of the proposed solution, such as increased throughput at the cost of higher latency for individual mints and increased system complexity.",
        "**File-level Accuracy:** Correctly identifies the key services (`gallery_gateway`, `ledger_core`, `mint_factory`) and a plausible set of files within them that would require modification.",
        "**Use of Context:** Demonstrates understanding derived from file names and structure (e.g., `pos_strategy.c` implying pluggable consensus, `kafka_client` implying event-driven architecture) beyond just reading a single file's contents."
      ],
      "language": "c",
      "score": 0.964
    },
    {
      "id": "c_api_microservice_expert_080_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Diagnose Performance Degradation in the V2 API Endpoints",
      "context_length": 1102120,
      "files_count": 79,
      "task_prompt": "Your analysis must produce a technical report in markdown format. The report should:\n\n1.  **Identify Key Components:** Pinpoint the specific source modules (`module_XX.txt`) responsible for the following functionalities:\n    *   HTTP request routing (especially the logic that differentiates v1 and v2).\n    *   The core response caching mechanism.\n    *   The primary business logic for handling v2 product-related requests.\n\n2.  **Describe the Request Lifecycle:** Detail the step-by-step flow of a request to a v2 endpoint (e.g., `/api/v2/products/{id}`). Describe how the identified modules interact with each other from the moment the request is received to the moment a response is sent.\n\n3.  **Formulate a Hypothesis:** Based on your analysis of the component interactions, formulate a precise hypothesis explaining why v2 endpoints are inherently slower than v1 endpoints. Your hypothesis must be grounded in the architectural design and the sequence of operations you've identified.",
      "ground_truth": "The core of the problem lies in a flawed interaction between the versioning logic and the caching layer.\n\n-   **Routing Logic (`module_31.txt`):** This module contains the primary request router. It parses the URL and directs traffic. For `/v1/...` requests, it calls functions in modules like `module_15.txt`. For `/v2/...` requests, it calls a specific transformation function in `module_50.txt`.\n\n-   **V2 Transformation Logic (`module_50.txt`):** This module acts as a compatibility/transformation layer. It first calls the same underlying business logic as v1 (e.g., in `module_15.txt`) to get the raw data structure. Then, it performs a series of complex and computationally expensive operations to transform this raw structure into the new v2 API format.\n\n-   **Caching Logic (`module_62.txt`):** This module provides a generic key-value caching service.\n\n-   **The Architectural Flaw:** The request lifecycle for a v2 endpoint is as follows:\n    1.  Request arrives and is parsed by the router in `module_31.txt`.\n    2.  The router identifies it as a v2 request and calls the transformation function in `module_50.txt`.\n    3.  `module_50.txt` calls the core business logic in `module_15.txt` to fetch data.\n    4.  `module_50.txt` performs the **expensive data transformation** to create the v2 response body.\n    5.  **Only after all this work**, the router logic in `module_31.txt` calls the caching function in `module_62.txt` to get/set the response.\n\n    The root cause of the performance degradation is that the expensive v2 data transformation in `module_50.txt` is executed on **every single request**, regardless of whether the result is already in the cache. The cache check happens too late in the lifecycle. The correct architecture would be to check the cache first, and only if there is a cache miss, proceed with fetching and transforming the data.",
      "evaluation_criteria": [
        {
          "criterion": "Correctly identifies the routing module.",
          "metric": "Boolean",
          "expected_value": "Agent must identify `src/module_31.txt` as the router."
        },
        {
          "criterion": "Correctly identifies the caching module.",
          "metric": "Boolean",
          "expected_value": "Agent must identify `src/module_62.txt` as the caching component."
        },
        {
          "criterion": "Correctly identifies the V2 transformation module.",
          "metric": "Boolean",
          "expected_value": "Agent must identify `src/module_50.txt` as the location of V2-specific logic."
        },
        {
          "criterion": "Accurately describes the request lifecycle for a V2 endpoint.",
          "metric": "Ordinal (1-5)",
          "expected_value": "5 - The agent correctly sequences the key steps: routing -> data fetch -> V2 transformation -> cache interaction."
        },
        {
          "criterion": "Correctly identifies and explains the architectural flaw.",
          "metric": "Ordinal (1-5)",
          "expected_value": "5 - The agent explicitly states that the expensive V2 transformation occurs *before* the cache lookup, which is the root cause of the performance issue."
        },
        {
          "criterion": "Clarity and conciseness of the final report.",
          "metric": "Ordinal (1-5)",
          "expected_value": "5 - The report is well-structured, easy to understand, and directly answers all parts of the prompt."
        }
      ],
      "language": "c",
      "score": 0.9496
    },
    {
      "id": "rust_api_microservice_expert_008_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis and Redesign for Cache Invalidation in a Rust Microservice",
      "context_length": 1014465,
      "files_count": 85,
      "task_prompt": "You are tasked with resolving a critical stale data issue in the LedgerLink Nexus service. Your analysis and proposal should be based entirely on the provided source files.\n\n1.  **Analyze the System**: Examine the provided source code to understand the overall architecture. Given the obfuscated filenames, you must infer the purpose of each module from its content.\n2.  **Identify Key Components**: Pinpoint the specific modules responsible for the following functions:\n    *   a) The core response caching logic.\n    *   b) Handling incoming read requests (e.g., `GET` operations).\n    *   c) Handling incoming write requests (e.g., `POST`, `PUT`, `DELETE` operations).\n3.  **Diagnose the Flaw**: Clearly describe the fundamental architectural flaw that causes the stale data issue. Explain *why* updating a resource does not immediately reflect in subsequent read requests.\n4.  **Propose a Solution**: Design an architectural solution to ensure cache consistency. Do not write the full implementation. Instead, describe the design pattern you would employ (e.g., explicit invalidation, write-through caching). Justify your choice.\n5.  **Outline Implementation Plan**: List the specific modules that would require modification to implement your proposed solution. Describe the new interactions that must be established between these modules (e.g., which module calls which, and with what information).",
      "ground_truth": "The agent's response should contain the following key insights, referencing the correct (obfuscated) files:\n\n*   **Key Component Identification**:\n    *   **Caching Service**: The primary caching logic, including `get` and `set` operations, is implemented in `src/module_44.txt`, which appears to be a wrapper around a Redis client. Configuration is pulled from `src/config.txt`.\n    *   **Read Handlers**: `src/module_26.txt` and `src/module_52.txt` contain handlers for `GET` requests. These handlers check for a valid cache entry in `module_44` before proceeding to query the database.\n    *   **Write Handlers**: `src/module_71.txt` contains the handlers for `PUT` and `POST` requests. These handlers process incoming data, validate it, and persist it to the database.\n\n*   **Architectural Flaw Diagnosis**:\n    *   The core flaw is a lack of communication between the write and cache services. The write handlers in `src/module_71.txt` successfully update the state in the primary data store but fail to notify the caching service in `src/module_44.txt` that the data has changed. Consequently, the cache holds a stale version of the resource, which is served by the read handlers in `src/module_26.txt` until its TTL expires.\n\n*   **Proposed Solution & Implementation Plan**:\n    *   **Pattern**: The recommended pattern is explicit 'Cache-Aside' invalidation. Upon a successful write operation, the application logic is responsible for deleting the corresponding entry from the cache.\n    *   **Module Modifications**:\n        1.  **`src/module_44.txt` (Caching Service)**: A new public function, `invalidate(key: &str)`, must be added. This function will execute a `DEL` command on the Redis cache for the given key.\n        2.  **`src/module_71.txt` (Write Handlers)**: The functions handling `PUT` and `POST` requests must be modified. After the database transaction is successfully committed, they must call the new `invalidate` function on the caching service instance. They are responsible for constructing the exact cache key that the corresponding `GET` handler in `src/module_26.txt` would use for that resource.",
      "evaluation_criteria": [
        "**Component Identification Accuracy**: Did the agent correctly identify the primary modules for caching, read handling, and write handling (`module_44`, `module_26`/`52`, `module_71`)?",
        "**Flaw Diagnosis Clarity**: Was the agent able to articulate that the problem is the *absence of an invalidation signal* from the write handlers to the cache, rather than a bug in the caching logic itself?",
        "**Solution Robustness**: Did the agent propose a robust architectural pattern like explicit invalidation? Points are deducted for simplistic or ineffective solutions like 'just lower the cache TTL'.",
        "**Implementation Plan Correctness**: Did the agent correctly identify that both the write handler module and the caching service module require modification?",
        "**Interaction Description**: Did the agent clearly describe the new required interaction: the write handler calling an invalidation function in the cache service after a successful DB write?",
        "**Reasoning Quality**: Does the agent's justification for its proposed solution demonstrate an understanding of trade-offs in distributed systems (e.g., consistency vs. performance)?"
      ],
      "language": "rust",
      "score": 0.9473
    },
    {
      "id": "python_data_streaming_expert_085_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Refactoring for a New Real-Time Ingestion Pathway",
      "context_length": 1157960,
      "files_count": 74,
      "task_prompt": "As a principal software engineer, your task is to analyze the existing PulseStream Nexus architecture and propose a refactoring plan to support the new real-time ingestion requirement. You are not required to write the implementation code, but to provide a clear and detailed architectural blueprint.\n\nYour analysis and proposal must include the following:\n\n1.  **Component Identification:** Analyze the provided source code to identify the primary module(s) responsible for the current batch ingestion, validation, and initial processing orchestration. Justify why this component is a bottleneck for real-time integration.\n\n2.  **Target Architecture Proposal:** Propose a new, decoupled target architecture. You should recommend a suitable architectural pattern (e.g., event-driven, microservices) that would allow both existing batch jobs and the new real-time stream to coexist efficiently. Describe the new services or components and their single responsibilities.\n\n3.  **Logic Mapping:** Identify the key classes and functions from the existing codebase that would be migrated or reused in your proposed new components. Be specific about which logic belongs in which new service.\n\n4.  **Data Flow Explanation:** Provide a high-level overview of the data flow in your new architecture. Using markdown (e.g., a Mermaid sequence diagram or a numbered list), illustrate how a piece of data travels from both a batch source (like S3) and a real-time source (like Kafka) through your proposed system.\n\n5.  **Configuration Impact Analysis:** Explain the conceptual changes required in `src/config.py` to support your new architecture. For example, how would pipeline definitions, data source configurations, or rule-set applications need to be structured differently?",
      "ground_truth": "### Ground Truth Analysis & Solution\n\n**1. Component Identification:**\n\nThe primary monolithic component is **`module_30.py`**. \n- It contains the `MasterBatchOrchestrator` class, which has a `run_pipeline` method that sequentially handles fetching data, running validation checks, and triggering transformations.\n- It directly imports and calls validation logic from **`module_17`** (`ValidationRuleEngine`) and transformation logic from **`module_8`** (`DataTransformer`) within its main execution loop. This tight, synchronous coupling makes it impossible to insert a real-time, record-by-record flow without a major rewrite.\n\n**2. Target Architecture Proposal:**\n\nAn **event-driven architecture using a message queue** is the ideal solution. The monolithic `MasterBatchOrchestrator` will be decomposed into three distinct services:\n\n-   **Ingestion Service:** Responsible solely for interfacing with data sources and publishing raw data to a `raw-data` topic on the message queue.\n-   **Validation Service:** Subscribes to the `raw-data` topic, applies validation rules to each message, and publishes valid records to a `validated-data` topic and invalid records to a `dead-letter-queue`.\n-   **Batch Processing Service:** Subscribes to the `validated-data` topic, performs windowing, aggregation, and batch transformations before writing to the final destination.\n\n**3. Logic Mapping:**\n\n-   **Ingestion Service:** Would reuse the data fetching logic from `module_30.py`, specifically methods like `_fetch_from_s3_source`. It would also contain the *new* logic for a Kafka consumer.\n-   **Validation Service:** Would be built around the `ValidationRuleEngine` from **`module_17.py`**. The loop that iterates through rules inside `module_30.run_pipeline` would be the core logic of this new service.\n-   **Batch Processing Service:** Would reuse the `DataTransformer` class from **`module_8.py`** and the batching/windowing logic from **`module_37.py`** (`TimeWindowAggregator`), which are currently invoked by `module_30`.\n\n**4. Data Flow Explanation (Mermaid Diagram):**\n\n```mermaid\nsequenceDiagram\n    participant S3 Source\n    participant Kafka Source\n    participant Ingestion Service\n    participant Message Queue\n    participant Validation Service\n    participant Processing Service\n\n    S3 Source->>+Ingestion Service: New file notification\n    Ingestion Service->>+Message Queue: Publishes records to 'raw-data'\n    deactivate Ingestion Service\n\n    Kafka Source->>+Ingestion Service: Consumes real-time stream\n    Ingestion Service->>+Message Queue: Publishes records to 'raw-data'\n    deactivate Ingestion Service\n\n    Message Queue-->>+Validation Service: Consumes from 'raw-data'\n    Validation Service-->>Validation Service: Applies rules from module_17\n    Validation Service-->>+Message Queue: Publishes to 'validated-data' or 'dlq'\n    deactivate Validation Service\n\n    Message Queue-->>+Processing Service: Consumes from 'validated-data'\n    Processing Service-->>Processing Service: Applies transforms from module_8 & module_37\n    Processing Service-->>Data Warehouse: Writes final batch\n    deactivate Processing Service\n```\n\n**5. Configuration Impact Analysis:**\n\n`src/config.py` would undergo a fundamental change. \n- The current monolithic `PIPELINE_CONFIG` dictionary, which defines linear stages, would be deprecated.\n- It would be replaced by separate configuration sections for each new service: `INGESTION_SERVICE_CONFIG`, `VALIDATION_SERVICE_CONFIG`, etc.\n- A new `MESSAGE_QUEUE_CONFIG` section would be added to define broker URLs, topic names (`raw-data`, `validated-data`), and consumer group IDs. This externalizes the data flow from the code to the configuration.",
      "evaluation_criteria": [
        "Correctly identifies `module_30.py` as the monolithic bottleneck component.",
        "Accurately describes the tight coupling between ingestion, validation, and transformation as the core architectural flaw.",
        "Proposes a viable, decoupled architecture, such as an event-driven model with a message queue.",
        "Correctly maps specific classes/logic (e.g., from `module_17`, `module_8`) to the appropriate new services in the proposed architecture.",
        "Provides a clear and logical data flow diagram or description for both batch and real-time sources.",
        "Correctly identifies that `src/config.py` must shift from defining linear pipelines to configuring independent services and their communication channels.",
        "The overall proposal is coherent, well-reasoned, and demonstrates an expert understanding of distributed system design patterns."
      ],
      "language": "python",
      "score": 0.9471
    },
    {
      "id": "c_api_graphql_expert_079_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Migration Plan: Custom IPC to gRPC",
      "context_length": 1002187,
      "files_count": 84,
      "task_prompt": "You are a principal engineer tasked with evaluating the feasibility of migrating the SynestheticCanvas suite from its custom IPC mechanism to gRPC. Your analysis must be based on the provided file system.\n\nYour deliverable is a technical report that addresses the following points:\n\n1.  **Component Identification:** Identify and list all the specific files and modules across the entire codebase that constitute the current inter-service communication system. This includes the client-side implementation in the API Gateway, the server-side implementation in the microservices, and the shared library itself.\n\n2.  **Data Flow Analysis:** Based on the code and documentation (especially `docs/architecture/data_flow.puml`), describe the end-to-end flow of a request from the API Gateway to a backend microservice (e.g., `texture-service`) and back. Detail the key functions, data structures, and libraries involved in this communication path.\n\n3.  **Migration Strategy Proposal:** Outline a high-level, phased plan to migrate the `texture-service` and the API Gateway's communication with it to gRPC. Your plan must address:\n    *   The necessary code and build system modifications (e.g., `CMakeLists.txt`) for both the API Gateway and the `texture-service`.\n    *   A strategy for managing the transition to minimize or eliminate service downtime. How can the system operate with a mix of old and new IPC mechanisms simultaneously?\n    *   A description of what the new gRPC-based communication flow would look like.\n\n4.  **Risk Assessment:** Identify and explain the top 3-5 technical risks and challenges associated with this migration. Consider factors beyond just code changes, such as build complexity, performance characteristics, deployment, and operational management.",
      "ground_truth": "### Key Component Identification\n- **Shared IPC Library:** `libs/sc_ipc/src/sc_ipc.c` and its header.\n- **Gateway Client:** `api-gateway/src/services/service_client.c` is the primary consumer of `sc_ipc` for making outbound requests to services.\n- **Service Servers:** The `main()` function in each microservice's `main.c` (e.g., `services/texture-service/src/main.c`, `services/palette-service/src/main.c`) is responsible for initializing the `sc_ipc` server and entering a request-handling loop.\n- **Configuration:** Service addresses and ports for the IPC are managed via `libs/sc_common/sc_config.c` and the respective `*.toml.example` files.\n- **Build System:** `CMakeLists.txt` in the root, gateway, and all services would need modification.\n\n### Migration Strategy Insights\n- **Phased Rollout is Critical:** The most viable strategy is to not replace `sc_ipc` in one go. The `texture-service` should be updated to run both the legacy IPC server and a new gRPC server, listening on different ports.\n- **Configuration-Driven Switching:** The API Gateway should be updated to support both communication clients. The choice of which client to use for a specific service (e.g., `texture-service`) should be driven by a configuration value loaded via `sc_config`. This allows for flipping the switch in production without a redeployment of the gateway.\n- **Contract First:** The first step is to define the service contract in a `.proto` file (e.g., `texture.proto`), which then generates the C++ stubs to be integrated into the C project (often via C-compatible wrapper functions).\n\n### Key Risks\n- **Build System Hell:** Integrating gRPC and Protobuf dependencies into a mature C/CMake project is notoriously difficult and a primary risk. It involves managing transitive dependencies and compiler flags correctly.\n- **Performance Regression:** The existing `sc_ipc` is likely a highly optimized, raw TCP protocol. gRPC, while very fast, adds layers of abstraction (HTTP/2, Protobuf serialization) that could introduce latency or CPU overhead. Performance testing is mandatory.\n- **Operational Complexity:** During the transition, the team must monitor and manage two different communication systems. This increases cognitive load, complicates debugging, and requires more sophisticated monitoring.",
      "evaluation_criteria": [
        "**Component Identification Accuracy:** Did the agent correctly identify the key files (`service_client.c`, service `main.c` files, `sc_ipc.c`) responsible for the current IPC mechanism?",
        "**Architectural Flow Comprehension:** Was the agent able to accurately describe the request flow from the gateway to a microservice, referencing the correct modules (resolvers, service client, handlers)?",
        "**Migration Plan Viability:** Is the proposed migration plan technically sound and realistic? Does it include a phased rollout strategy (e.g., parallel systems) rather than a risky \"big bang\" approach?",
        "**Build & Dependency Awareness:** Did the agent recognize that modifying the `CMakeLists.txt` files and managing the gRPC/Protobuf dependencies is a non-trivial part of the task?",
        "**Risk Assessment Depth:** Did the agent identify sophisticated risks beyond simple coding errors, such as build system complexity, performance implications, and operational overhead during the transition?",
        "**Use of Evidence:** Does the agent's analysis refer to specific files, documentation (`data_flow.puml`), and architectural patterns to support its conclusions?"
      ],
      "language": "c",
      "score": 0.9407
    },
    {
      "id": "rust_data_streaming_expert_013_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Refactoring for High-Velocity Ingestion in ChirpPulse",
      "context_length": 1023423,
      "files_count": 82,
      "task_prompt": "Your task is to analyze the ChirpPulse architecture and propose a modification to handle the anticipated increase in data volume. You must not write any code, but provide a detailed architectural assessment and plan.\n\n1.  **Map the Pipeline:** Identify the primary stages of the data processing pipeline from ingestion to final storage. For each stage, identify the key module(s) responsible for its logic.\n2.  **Identify the Bottleneck Coupling:** Pinpoint the specific mechanism used to hand off data between the data normalization/transformation stage and the sentiment analysis stage. Describe how this mechanism could cause the backpressure problem.\n3.  **Propose an Architectural Solution:** Design a modification to the architecture that decouples the sentiment analysis stage from the main ingestion pipeline, allowing it to scale independently and absorb massive load variations. Your proposal should be a well-established architectural pattern.\n4.  **Justify Your Solution:** Explain why your proposed architecture is superior for this use case. Discuss its impact on scalability, system resilience, and backpressure handling.\n5.  **Identify Affected Components:** List the specific modules that would need to be modified to implement your proposed solution and briefly describe the nature of the changes required for each.",
      "ground_truth": {
        "pipeline_map": {
          "Ingestion": "`module_18.txt`, `module_79.txt` - Responsible for connecting to external APIs and receiving raw data.",
          "Normalization & Quality Checks": "`module_30.txt`, `module_67.txt` - Responsible for parsing raw data, validating schema, cleaning text, and creating a standardized `NormalizedChirp` struct.",
          "Sentiment Analysis": "`module_41.txt` - A CPU-intensive stage that takes `NormalizedChirp` objects, runs them through an analysis model, and produces an `EnrichedChirp` with a sentiment score. This is implemented as a fixed-size worker pool.",
          "Orchestration": "`module_22.txt` - Initializes all components, wires them together, and manages the lifecycle of the various tasks.",
          "Storage Sink": "`module_74.txt` - Batches `EnrichedChirp` objects and writes them to the data lake (abstracted S3 client)."
        },
        "bottleneck_coupling": "Data is passed from the normalization stage (`module_30`) to the sentiment analysis worker pool (`module_41`) via a `tokio::mpsc::channel`. This is an in-memory, bounded queue. If the sentiment analysis workers are slow and cannot consume messages fast enough, the channel buffer will fill up. When full, the `send` operation in the normalization stage will block asynchronously (`await`), creating backpressure that propagates backward and stalls the entire ingestion pipeline.",
        "architectural_solution": "Introduce a durable, external message queue (like Apache Kafka, AWS SQS, or Redis Streams) to act as a buffer between the normalization and sentiment analysis stages. The architecture would change from a linear pipeline to a producer-consumer model.",
        "justification": "1.  **Scalability:** The normalization service (Producer) and the sentiment analysis service (Consumer) can be scaled independently. If sentiment analysis is slow, we can add more consumer instances without affecting the ingestion rate. \n2.  **Resilience:** The message queue acts as a durable buffer. If the sentiment analysis service fails, messages are not lost; they remain in the queue to be processed once the service recovers. \n3.  **Backpressure Management:** The ingestion pipeline is no longer blocked by the slow downstream service. It can publish messages to the queue at its maximum rate. The queue absorbs the load, effectively handling the backpressure and preventing it from impacting ingestion.",
        "affected_components": [
          {
            "module": "`module_30.txt` (Normalization)",
            "change": "Remove the logic that sends data to the `tokio::mpsc::channel`. Replace it with a message queue client (e.g., a Kafka producer) to serialize the `NormalizedChirp` and send it to a specific topic."
          },
          {
            "module": "`module_41.txt` (Sentiment Analysis)",
            "change": "Remove the logic that receives data from the `tokio::mpsc::channel`. Replace it with a message queue client (e.g., a Kafka consumer) to receive messages from the topic, deserialize them, and then perform the analysis."
          },
          {
            "module": "`module_22.txt` (Orchestration)",
            "change": "The orchestration logic needs to be updated. Instead of creating a channel and linking the two stages directly, it would now be responsible for initializing the producer service and the consumer service as separate, potentially independently deployable, tasks."
          },
          {
            "module": "`src/config.txt`",
            "change": "Add new configuration parameters for the message queue, such as broker addresses, topic names, and consumer group IDs."
          }
        ]
      },
      "evaluation_criteria": [
        "**Pipeline Identification (20%):** Correctly identifies the 3-4 primary stages of the pipeline and maps them to the correct modules.",
        "**Bottleneck Analysis (25%):** Accurately identifies the `tokio::mpsc::channel` as the coupling mechanism and correctly explains how it causes backpressure in this context.",
        "**Solution Quality (25%):** Proposes a robust, industry-standard decoupling solution, such as using an external message queue.",
        "**Architectural Justification (15%):** Clearly articulates the benefits of the proposed solution in terms of scalability, resilience, and backpressure handling.",
        "**Impact Assessment (15%):** Correctly identifies all the key modules (`normalization`, `sentiment`, `orchestration`, `config`) that require modification to implement the proposed change."
      ],
      "language": "rust",
      "score": 0.9395
    },
    {
      "id": "python_game_engine_expert_032_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for a Serverless Game Replay System",
      "context_length": 1055500,
      "files_count": 77,
      "task_prompt": "Analyze the LedgerQuest Engine architecture and create a technical design document in Markdown format named `REPLAY_SYSTEM_DESIGN.md`. You should not write any implementation code. Your analysis must be based on the existing codebase and documentation.\n\nThe design document must contain the following sections:\n\n1.  **Overview**: A brief summary of your proposed solution.\n2.  **Data Capture Strategy**: \n    - Identify the precise point(s) within the existing game loop (as defined by the AWS Step Function and its associated services) to capture the necessary data for replays.\n    - Justify your choice. Explain what data should be captured (e.g., player inputs, full state snapshots, state deltas) and discuss the trade-offs (e.g., storage size, replay accuracy, performance impact).\n    - Reference the specific files and modules involved in this capture process.\n3.  **Data Storage and Management**: \n    - Propose a primary AWS service for storing the replay data.\n    - Describe the data format and structure (e.g., how you would organize data for a single game session).\n    - Justify your choice based on cost, scalability, and retrieval patterns.\n4.  **Replay Playback Mechanism**:\n    - Outline the high-level architecture for a new 'Replay Service'.\n    - Describe the API endpoints a client would use to fetch and play a replay.\n    - Explain how the replay data would be processed and fed back into the game engine to reconstruct the session.\n5.  **Architectural Impact and Risks**:\n    - Analyze the performance and latency impact on the live game loop.\n    - Discuss the potential costs associated with your proposed solution.\n    - Identify potential risks or challenges, such as handling game engine updates and maintaining replay compatibility.",
      "ground_truth": "The optimal solution involves capturing player inputs for replay, as it's the most efficient and scalable approach for a deterministic engine like this.\n\n**REPLAY_SYSTEM_DESIGN.md**\n\n### 1. Overview\nThis document proposes a replay system for LedgerQuest by capturing the sequence of player commands for each game tick. This command stream will be stored in Amazon S3. A new Replay Service will provide clients with this command stream, allowing them to reconstruct the game session locally by feeding the commands into the deterministic game engine.\n\n### 2. Data Capture Strategy\n-   **Capture Point**: The ideal capture point is immediately after the `InputProcessor` state in the `game_loop_statemachine.asl.json`. The corresponding service logic is in `ledgerquest/services/game_loop/input_processor.py`.\n-   **Data to Capture**: We will capture the finalized, validated queue of commands for each game tick. This is the minimal data required to reconstruct the game state if the engine is deterministic.\n-   **Justification**: Capturing commands instead of full state snapshots drastically reduces storage requirements (kilobytes vs. megabytes per tick). While it relies on engine determinism, this is a standard assumption for replay systems. Hooking into the Step Function ensures the capture is an integral, reliable part of the game loop.\n\n### 3. Data Storage and Management\n-   **Storage Service**: Amazon S3 is the best choice due to its low cost for long-term storage, high durability, and scalability.\n-   **Data Structure**: For each game session (identified by a `session_id`), a single compressed file (e.g., `session_id.json.gz`) will be created in an S3 bucket. This file will contain an ordered list of objects, where each object represents a tick and contains the command queue for that tick.\n    ```json\n    // Example: session_123.json.gz\n    {\n      \"version\": \"1.0.0\",\n      \"initial_state_ref\": \"s3://.../initial_state.json\",\n      \"ticks\": [\n        { \"tick\": 1, \"commands\": [{\"player_id\": \"p1\", \"action\": \"move\", \"params\": [10, 5]}] },\n        { \"tick\": 2, \"commands\": [] },\n        { \"tick\": 3, \"commands\": [{\"player_id\": \"p1\", \"action\": \"fire\"}] }\n      ]\n    }\n    ```\n\n### 4. Replay Playback Mechanism\n-   **Architecture**: A new serverless service, `ReplayService`, will be created using API Gateway and AWS Lambda.\n-   **API Endpoints**:\n    -   `GET /replays/{session_id}`: A Lambda function will fetch the corresponding replay file from S3, decompress it, and return it to the client.\n-   **Playback Logic**: The client (or a local simulation) will initialize the game engine with the initial state. It will then iterate through the `ticks` array from the replay file, feeding the `commands` for each tick into the engine's input processing system and running the engine simulation for one tick. This will perfectly reconstruct the game session.\n\n### 5. Architectural Impact and Risks\n-   **Performance**: A new step will be added to the Step Function to write the command data to S3. To minimize latency impact on the critical path, this should be implemented as a 'fire-and-forget' task or by invoking a separate Lambda asynchronously.\n-   **Cost**: The primary cost will be S3 storage and data transfer, plus Lambda invocations for capture and retrieval. This design is highly cost-effective compared to state-snapshotting.\n-   **Risks**:\n    -   **Determinism Bugs**: If any part of the engine (e.g., physics, AI) is non-deterministic, replays will desynchronize. This requires rigorous testing.\n    -   **Engine Versioning**: A change to the game logic in a future patch may render old replays incompatible. The replay data format must include an engine version number (`\"version\": \"1.0.0\"`) to allow clients to handle this gracefully (e.g., by refusing to play an incompatible replay).",
      "evaluation_criteria": [
        "Correctly identifies the AWS Step Function (`game_loop_statemachine.asl.json`) as the central orchestrator of the game loop.",
        "Demonstrates understanding of the serverless, stateless execution model by proposing a solution that fits within it.",
        "Correctly identifies `input_processor.py` and/or `state_committer.py` as key files for potential data capture.",
        "Articulates the critical trade-off between capturing inputs versus capturing full game state (storage, cost, determinism).",
        "Proposes a storage solution (e.g., S3) that is appropriate for the data and aligns with serverless best practices.",
        "Outlines a coherent playback mechanism that reuses the deterministic nature of the game engine.",
        "Provides a realistic analysis of the architectural impact, including performance (latency), cost, and key risks like versioning."
      ],
      "language": "python",
      "score": 0.9308
    },
    {
      "id": "csharp_data_warehouse_expert_012_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Centralize and Standardize Asynchronous Operation Retry Logic",
      "context_length": 1113221,
      "files_count": 84,
      "task_prompt": "Your task is to improve the resilience and maintainability of the PulseOps Warehouse by refactoring the existing error handling logic for external API calls.\n\n1.  **Identify:** Scan all files in the `src/` directory, particularly the `module_*.txt` files, to locate all instances of manual retry loops. These are typically `for` or `while` loops containing a `try-catch` block that handles exceptions like `HttpRequestException`, `TimeoutException`, or `WebException` and attempts to retry the failed operation.\n\n2.  **Design & Implement:** In `src/utils.txt`, create a new public static class named `ResilienceHelper`. Implement a generic, asynchronous static method `ExecuteWithRetryAsync<T>`. This method should accept a delegate (`Func<Task<T>>`) representing the asynchronous operation to execute. The method must implement an exponential backoff retry strategy.\n\n3.  **Centralize Configuration:** The retry policy must be configurable. Add the following two public constants to the `src/constants.txt` file:\n    *   `public const int API_MAX_RETRIES = 5;`\n    *   `public const int API_INITIAL_BACKOFF_MS = 200;`\n    The `ResilienceHelper` must use these constants to control its behavior.\n\n4.  **Refactor:** Replace every identified ad-hoc retry loop throughout the `module_*.txt` files with a single call to your new `ResilienceHelper.ExecuteWithRetryAsync` method. You will need to wrap the original API call within a lambda expression to pass it to the helper. Ensure that the logic remains asynchronous and non-blocking.\n\n5.  **Cleanup:** After replacing the logic, remove all the old, now-redundant `for`/`while` loops, `try-catch` blocks, and any associated manual delay or counter variables.",
      "ground_truth": "The expected outcome is a significant reduction in boilerplate code across multiple files, replaced by a robust, centralized pattern.\n\n**Key changes in the final state:**\n\n*   **`src/constants.txt`:**\n    *   Contains two new lines:\n        ```csharp\n        public const int API_MAX_RETRIES = 5;\n        public const int API_INITIAL_BACKOFF_MS = 200;\n        ```\n\n*   **`src/utils.txt`:**\n    *   Contains a new static class, `ResilienceHelper`, with a method signature similar to this:\n        ```csharp\n        public static class ResilienceHelper\n        {\n            public static async Task<T> ExecuteWithRetryAsync<T>(Func<Task<T>> operation)\n            {\n                int attempt = 0;\n                while (true)\n                {\n                    try\n                    {\n                        return await operation();\n                    }\n                    catch (Exception ex) when (IsTransient(ex) && attempt < Constants.API_MAX_RETRIES)\n                    {\n                        attempt++;\n                        var delay = TimeSpan.FromMilliseconds(Constants.API_INITIAL_BACKOFF_MS * Math.Pow(2, attempt - 1));\n                        // A real implementation would also include logging the retry attempt.\n                        await Task.Delay(delay);\n                    }\n                }\n            }\n\n            private static bool IsTransient(Exception ex)\n            {\n                // In a real system, this would be more robust.\n                return ex is HttpRequestException || ex is TimeoutException;\n            }\n        }\n        ```\n\n*   **`src/module_*.txt` files:**\n    *   Multiple files (likely 5-10 or more) will have their code changed.\n    *   Code blocks of 10-15 lines (e.g., a `for` loop with `try-catch` and `Task.Delay`) will be replaced by a single, elegant line:\n        *   **Before:** A complex, multi-line loop.\n        *   **After:** `var processResult = await ResilienceHelper.ExecuteWithRetryAsync(() => someApiService.GetDataAsync(request));`",
      "evaluation_criteria": [
        "**Correctness of Implementation:** The `ResilienceHelper` must correctly implement an exponential backoff strategy using the specified constants from `constants.txt`.",
        "**Completeness of Refactoring:** All identified ad-hoc retry loops across the relevant `module_*.txt` files must be successfully replaced.",
        "**Code Quality and Design:** The new `ResilienceHelper` must be generic (using `<T>`), asynchronous (`async/await`), and accept a delegate (`Func<Task<T>>`) to be maximally reusable.",
        "**Functional Preservation:** The refactoring must not alter the core business logic of the modules. The agent should only replace the error handling wrapper, not the operation itself.",
        "**Code Cleanliness:** The old, redundant retry loops and their associated variables must be completely removed from the codebase.",
        "**Configuration Centralization:** The agent must correctly add the new configuration values to `src/constants.txt` and use them in the `ResilienceHelper`, not hardcode them."
      ],
      "language": "csharp",
      "score": 0.9293
    },
    {
      "id": "cpp_web_blog_expert_040_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for Decoupling the Authentication Service",
      "context_length": 981583,
      "files_count": 82,
      "task_prompt": "Your task is to act as a principal software architect and analyze the IntraLedger BlogSuite codebase to prepare for the authentication service extraction. You must produce a report that addresses the following points:\n\n1.  **Core Component Identification:** Identify the primary C++ source file(s) that constitute the core of the authentication and session management system. Justify your choices by describing the key functions or classes within these files (e.g., password hashing, user registration, session token generation).\n\n2.  **Dependency Mapping:** Identify all other modules within the `src/` directory that are directly coupled to the core authentication/session components. For each dependent module you identify, provide the filename and a brief explanation of the dependency's nature (e.g., 'module_23.cpp depends on the auth system to verify user permissions before processing a request').\n\n3.  **Decoupling Strategy:** Propose a high-level architectural strategy to decouple the authentication logic from the main application. Your strategy should define a clear boundary or contract (e.g., an abstract interface, a set of API endpoints) that would allow the BlogSuite to communicate with a future external authentication service with minimal changes to the dependent modules.",
      "ground_truth": "The agent's response will be evaluated against this ground truth, which represents the hidden architecture of the system.\n\n1.  **Core Component Identification:**\n    *   `src/module_12.cpp`: This module is the core of user management. It contains the `User` class definition, functions for `createUser`, `findUserByUsername`, and critically, `verifyPassword` which involves a password hashing and salting mechanism.\n    *   `src/module_35.cpp`: This module handles session management. It contains functions like `createSessionToken` (likely generating a JWT) and `validateSessionToken`, which are used to maintain user state across requests. It depends on `module_12` to get user details.\n\n2.  **Dependency Mapping:**\n    *   `src/module_23.cpp` (Request Middleware/Guard): This module inspects incoming request headers for a session token and uses `module_35::validateSessionToken` to protect application routes. It's a primary consumer of the session service.\n    *   `src/module_10.cpp` (Post Manager): When creating or editing a post, this module gets the current `userId` from the validated session to assign authorship.\n    *   `src/module_61.cpp` (Comment Manager): Similar to the Post Manager, this module requires a valid `userId` from the session to allow users to post comments.\n    *   `src/module_7.cpp` (API Endpoint Router): This module defines the `/login` endpoint, which directly calls `module_12::verifyPassword` and `module_35::createSessionToken`.\n    *   `src/module_41.cpp` (Search Functionality): The search service may have features for searching only one's own posts, which requires getting the `userId` from the current session.\n    *   `src/config.cpp`: While not a module, it's a critical dependency. It holds configuration used by `module_35`, such as the JWT secret key and token expiration duration.\n\n3.  **Decoupling Strategy:**\n    *   **Define an Interface:** Propose a new abstract base class, `IAuthenticationService`, with pure virtual functions like:\n        ```cpp\n        class IAuthenticationService {\n        public:\n            virtual ~IAuthenticationService() = default;\n            virtual std::optional<UserDetails> authenticate(const std::string& username, const std::string& password) = 0;\n            virtual std::optional<SessionInfo> validateToken(const std::string& token) = 0;\n            virtual void logout(const std::string& token) = 0;\n        };\n        ```\n    *   **Refactor Consumers:** All dependent modules (`23`, `10`, `61`, `7`, `41`) should be refactored to hold a `std::shared_ptr<IAuthenticationService>` and call its methods instead of directly calling functions in `module_12` and `module_35`.\n    *   **Implement Concrete Classes:** Create an initial `LocalAuthService` that implements `IAuthenticationService` by wrapping the existing logic from `module_12` and `module_35`. This allows for an incremental refactor. The future `SSOAuthService` would be a separate implementation that makes network calls to the external SSO service.",
      "evaluation_criteria": [
        "**Correctness of Core Module Identification:** Did the agent correctly identify `module_12.cpp` and `module_35.cpp` as the primary authentication and session components?",
        "**Completeness of Dependency Analysis:** How many of the key dependent modules (`23`, `10`, `61`, `7`, `41`) were correctly identified?",
        "**Accuracy of Dependency Rationale:** Is the agent's explanation for *why* each module is a dependency accurate and specific?",
        "**Soundness of Decoupling Strategy:** Does the proposed strategy effectively abstract the authentication logic? Is the use of an abstract class/interface or a similar pattern correctly identified as the best practice?",
        "**Architectural Acumen:** Does the response demonstrate an understanding of high-level software design principles like Dependency Inversion, Separation of Concerns, and API contracts, or does it offer a naive, surface-level solution?"
      ],
      "language": "cpp",
      "score": 0.9287
    },
    {
      "id": "c_blockchain_nft_expert_071_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Refactor Disparate Event Handling Logic into a Shared Event Dispatcher Module",
      "context_length": 1095933,
      "files_count": 85,
      "task_prompt": "Your task is to implement a major architectural refactoring by creating a shared event dispatcher. You must centralize the event handling logic currently duplicated across multiple services.\n\n**Detailed Requirements:**\n\n1.  **Create a New Shared Module:**\n    *   In the `HoloCanvas/shared/` directory, create a new subdirectory named `event_dispatcher`.\n    *   Inside this new directory, create two new files: `hc_event_dispatcher.h` and `hc_event_dispatcher.c`.\n\n2.  **Define the Dispatcher Interface (`hc_event_dispatcher.h`):**\n    *   Define a function pointer type for event handlers: `typedef hc_error_t (*event_handler_func_t)(const ProtobufCMessage* msg, void* user_context);`.\n    *   Define an opaque struct for the event dispatcher context: `typedef struct EventDispatcher EventDispatcher;`.\n    *   Declare the public API for the dispatcher:\n        *   `EventDispatcher* event_dispatcher_create(const char* kafka_brokers, const char* topic, const char* group_id, void* user_context);`\n        *   `hc_error_t event_dispatcher_register_handler(EventDispatcher* dispatcher, const char* event_type_key, event_handler_func_t handler);`\n        *   `void event_dispatcher_run(EventDispatcher* dispatcher);` (This will contain the main consumption loop).\n        *   `void event_dispatcher_destroy(EventDispatcher* dispatcher);`\n\n3.  **Implement the Dispatcher Logic (`hc_event_dispatcher.c`):**\n    *   Implement the functions declared in the header.\n    *   The implementation should use a hash map or a similar data structure to store the mapping from `event_type_key` strings to `event_handler_func_t` function pointers.\n    *   The `event_dispatcher_run` function must encapsulate the logic for connecting to Kafka (using the existing `hc_kafka` client), consuming messages in a loop, deserializing the message (assuming a generic wrapper proto with an `event_type` field), looking up the handler in the registry, and invoking it with the message payload and user context.\n\n4.  **Refactor Consumer Services:**\n    *   Modify the following services to use the new event dispatcher: `governance_hall`, `mint_factory`, and `muse_observer`.\n    *   **In each service's `main.c`:**\n        *   Remove the existing Kafka connection and consumption loop.\n        *   Instantiate the new `EventDispatcher` using `event_dispatcher_create()`.\n        *   Register all service-specific event handlers using `event_dispatcher_register_handler()`.\n        *   Start the event processing by calling `event_dispatcher_run()`.\n        *   Ensure `event_dispatcher_destroy()` is called on shutdown.\n    *   **In each service's event handling file (`governance_hall/src/event_handler.c`, `mint_factory/src/event_handler.c`, `muse_observer/src/event_listener.c`):**\n        *   Remove the main dispatching function (e.g., the large `switch` statement that routes events).\n        *   Ensure the individual handler functions (e.g., `handle_proposal_created`, `handle_artifact_minted`) are modified to match the `event_handler_func_t` signature and are made visible to `main.c` (i.e., not `static` if they were before, and declared in a local header if necessary).\n\n5.  **Update Build System:**\n    *   Modify `HoloCanvas/shared/CMakeLists.txt` to include the new `event_dispatcher` source files in the `shared_lib` target.\n    *   Verify that the `CMakeLists.txt` files for `governance_hall`, `mint_factory`, and `muse_observer` correctly link against the `shared_lib`.",
      "ground_truth": "The solution is correct if the agent successfully creates the new shared module and refactors the specified services. Key indicators of a correct solution include:\n\n*   **New Files Created:** `HoloCanvas/shared/event_dispatcher/hc_event_dispatcher.h` and `HoloCanvas/shared/event_dispatcher/hc_event_dispatcher.c` exist and are populated.\n\n*   **Key Code Snippet in `hc_event_dispatcher.h`:**\n    ```c\n    // HoloCanvas/shared/event_dispatcher/hc_event_dispatcher.h\n    #include \"shared/common/errors.h\"\n    #include <protobuf-c/protobuf-c.h>\n\n    typedef struct EventDispatcher EventDispatcher;\n\n    typedef hc_error_t (*event_handler_func_t)(const ProtobufCMessage* msg, void* user_context);\n\n    EventDispatcher* event_dispatcher_create(const char* kafka_brokers, const char* topic, const char* group_id, void* user_context);\n    hc_error_t event_dispatcher_register_handler(EventDispatcher* dispatcher, const char* event_type_key, event_handler_func_t handler);\n    void event_dispatcher_run(EventDispatcher* dispatcher);\n    void event_dispatcher_destroy(EventDispatcher* dispatcher);\n    ```\n\n*   **Example of Refactored `main.c` (e.g., `governance_hall/src/main.c`):**\n    ```c\n    // HoloCanvas/services/governance_hall/src/main.c (Conceptual Change)\n    #include \"shared/event_dispatcher/hc_event_dispatcher.h\"\n    #include \"governance.h\" // Assumed to declare handlers like handle_proposal_created, etc.\n\n    int main(int argc, char **argv) {\n        // ... initial setup ...\n        GovernanceState* state = initialize_governance_state();\n        \n        EventDispatcher* dispatcher = event_dispatcher_create(\"kafka:9092\", \"governance_topic\", \"governance_group\", state);\n        if (!dispatcher) { /* handle error */ }\n\n        event_dispatcher_register_handler(dispatcher, \"PROPOSAL_CREATED\", handle_proposal_created_event);\n        event_dispatcher_register_handler(dispatcher, \"VOTE_CAST\", handle_vote_cast_event);\n        // ... register other handlers ...\n\n        printf(\"Starting Governance Hall event dispatcher...\\n\");\n        event_dispatcher_run(dispatcher);\n\n        printf(\"Shutting down...\\n\");\n        event_dispatcher_destroy(dispatcher);\n        destroy_governance_state(state);\n        return 0;\n    }\n    ```\n\n*   **Removal of Logic:** The large `switch` statement for routing events based on type is no longer present in `governance_hall/src/event_handler.c`, `mint_factory/src/event_handler.c`, etc.\n\n*   **`CMakeLists.txt` Modification:** `HoloCanvas/shared/CMakeLists.txt` should be updated:\n    ```cmake\n    # In shared/CMakeLists.txt\n    set(SHARED_SOURCES\n        common/types.c\n        common/errors.c\n        kafka_client/hc_kafka.c\n        crypto_wrapper/hc_crypto.c\n        event_dispatcher/hc_event_dispatcher.c # <-- ADDED LINE\n    )\n    ```",
      "evaluation_criteria": [
        "**Correctness of Abstraction:** The created `hc_event_dispatcher` API must be generic, reusable, and correctly designed with an opaque struct and function pointers.",
        "**Completeness of Refactoring:** All three specified services (`governance_hall`, `mint_factory`, `muse_observer`) must be successfully refactored to use the new dispatcher.",
        "**Code Removal:** The agent must correctly identify and remove the duplicated Kafka consumption and event dispatching logic from the services' `main.c` and `event_handler.c`/`event_listener.c` files.",
        "**Build System Integrity:** The `CMakeLists.txt` files for both the `shared` library and the affected services must be updated correctly, allowing the project to compile without errors.",
        "**Non-Regression of Logic:** The core business logic inside the individual handler functions (e.g., the logic that processes a `PROPOSAL_CREATED` event) must be preserved and correctly integrated with the new handler signature.",
        "**File System Manipulation:** The agent must correctly create new files and directories and modify existing files across the project structure as required.",
        "**Resource Management:** The implementation of `hc_event_dispatcher.c` must demonstrate proper memory and resource management (e.g., freeing the dispatcher context, the handler registry, and closing Kafka handles)."
      ],
      "language": "c",
      "score": 0.9282
    },
    {
      "id": "rust_web_social_expert_073_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Refactor Disparate Event Publishing Logic into a Centralized, Generic Service",
      "context_length": 1067411,
      "files_count": 86,
      "task_prompt": "Your task is to improve the architecture of the EduPulse Live application by centralizing all event publishing logic.\n\n1.  **Analyze the existing implementation:** Examine the source code in `src/module_15.txt`, `src/module_48.txt`, and `src/module_77.txt`. Each of these files contains a distinct function that serializes a module-specific struct to JSON and sends it to an event stream. These functions contain duplicated logic for serialization, error handling, and interacting with a mock event stream client.\n\n2.  **Create a new, centralized module:**\n    *   Create a new file named `src/event_publisher.txt`.\n    *   Inside this new file, define a struct `EventPublisher` that will manage the connection to the event stream.\n    *   Implement a `new()` function for `EventPublisher` that initializes it. For configuration, it should call the mock function `get_event_bus_config()` located in `src/utils.txt`.\n\n3.  **Implement a generic publishing method:**\n    *   On the `EventPublisher` struct, create a public, asynchronous method: `publish<T: serde::Serialize + Sync>`. This method should accept a payload of any type `T` that can be serialized.\n    *   This `publish` method will be responsible for:\n        a. Serializing the payload to a JSON string.\n        b. Calling the mock `send_event_to_stream()` function from `src/utils.txt`.\n        c. Handling potential errors from serialization or sending, returning a `Result`.\n    *   Define a custom `EventPublisherError` enum within `src/event_publisher.txt` to standardize error reporting for this service.\n\n4.  **Refactor existing modules:**\n    *   Modify `src/module_15.txt`, `src/module_48.txt`, and `src/module_77.txt` to use the new `EventPublisher` service.\n    *   Remove the original, now-redundant event publishing functions from these three modules.\n    *   Update the call sites within those modules to instantiate and use the `EventPublisher`'s `publish` method.\n    *   Ensure you add the necessary `use crate::event_publisher::{...}` statements at the top of the modified files.",
      "ground_truth": "The final state of the codebase should reflect the following key changes:\n\n*   **New File `src/event_publisher.txt`:**\n    *   This file must exist.\n    *   It must contain a `pub struct EventPublisher`.\n    *   It must contain a `pub enum EventPublisherError` with variants for serialization and transport errors.\n    *   It must contain an implementation of `EventPublisher` with a `pub fn new(...) -> Self` and a `pub async fn publish<T: serde::Serialize + Sync>(...) -> Result<(), EventPublisherError>`.\n    *   The `publish` method correctly serializes its generic `payload` argument and calls `utils::send_event_to_stream`.\n\n*   **Modified `src/module_15.txt`:**\n    *   The original, module-specific event publishing function has been deleted.\n    *   A `use crate::event_publisher::EventPublisher;` (and possibly `EventPublisherError`) statement is present.\n    *   Code that previously called the old function now instantiates `EventPublisher::new()` and calls the `.publish()` method.\n\n*   **Modified `src/module_48.txt`:**\n    *   Same changes as in `module_15.txt`, but relevant to its own specific event struct and call sites.\n\n*   **Modified `src/module_77.txt`:**\n    *   Same changes as in `module_15.txt`, but relevant to its own specific event struct and call sites.\n\n*   **No other files should be modified.** The changes must be confined to the four files mentioned in the task.",
      "evaluation_criteria": [
        "**Correctness of Centralization:** Was the duplicated event publishing logic successfully consolidated into the new `src/event_publisher.txt` module?",
        "**Proper Use of Generics:** Is the new `publish` method correctly implemented using Rust generics (`<T: serde::Serialize>`) to handle different event data structures without code duplication?",
        "**Code Elimination:** Were the old, redundant publishing functions and their associated helper logic completely removed from `module_15`, `module_48`, and `module_77`?",
        "**Architectural Improvement:** Does the solution correctly use a struct (`EventPublisher`) to manage state and configuration, demonstrating an understanding of dependency management over static functions?",
        "**Dependency Resolution:** Are the necessary `use` statements correctly added to the refactored modules to import the new service?",
        "**Error Handling:** Is a new, unified `EventPublisherError` type created and used correctly in the `publish` method's return signature?",
        "**Surgical Precision:** Did the agent avoid making unnecessary or unrelated changes to the files or the broader codebase?"
      ],
      "language": "rust",
      "score": 0.9242
    },
    {
      "id": "rust_ml_computer_vision_expert_054_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for Microservice Extraction in VisuTility Orchestrator",
      "context_length": 1003239,
      "files_count": 78,
      "task_prompt": "You are a senior software architect. Your goal is to analyze the 'VisuTility Orchestrator' codebase and produce a plan for extracting the 'experiment_tracking' feature into a separate microservice. You must not write any new code, but provide a detailed architectural report.\n\nYour report must contain four sections:\n\n1.  **Core Component Identification:** Identify the primary Rust modules (e.g., `src/module_XX.txt`) that constitute the core logic for experiment tracking. This includes managing experiments, runs, logging metrics, parameters, and artifacts.\n\n2.  **Dependency Analysis:** Create a dependency map for the identified core components. This map should detail:\n    *   **Internal Dependencies:** Which other modules within the monolith do the core experiment tracking modules depend on (e.g., database connectors, shared data structures, configuration loaders)?\n    *   **External Consumers:** Which other modules in the monolith consume the services of the experiment tracking components (e.g., model training or hyperparameter tuning modules that need to log their results)?\n\n3.  **Proposed Service API:** Define a high-level, language-agnostic RESTful API for the new 'Experiment Tracking' microservice. The API should expose all necessary functionalities currently provided internally. Specify the resources, endpoints, HTTP methods, and expected request/response payloads (in brief).\n\n4.  **Refactoring Plan:** List the specific modules within the remaining monolith that will require modification to use the new microservice API instead of the current internal calls. For each module, briefly describe the nature of the change required.",
      "ground_truth": {
        "notes": "This ground truth represents a plausible architecture. The agent's answer should be evaluated on its ability to identify a similarly structured, internally consistent architecture, even if the exact module numbers differ.",
        "core_components": [
          "src/module_30.txt: Likely defines the main `Run` and `Experiment` structs and the primary service trait for tracking.",
          "src/module_9.txt: Contains the implementation for logging and retrieving metrics and parameters.",
          "src/module_38.txt: Handles artifact storage logic, linking file paths or object storage URIs to a specific run."
        ],
        "dependency_analysis": {
          "internal_dependencies": [
            "src/module_6.txt: A generic database connection pool and query builder used by all components that persist data.",
            "src/module_2.txt: Defines shared data types used across the application, like `Timestamp`, `Status`, and custom `Error` types.",
            "src/config.txt: Provides database credentials, artifact storage locations, and other runtime configurations."
          ],
          "external_consumers": [
            "src/module_16.txt: The `model_training` module, which calls the tracking service to create a new run at the start of training and logs epoch-level metrics (loss, accuracy).",
            "src/module_34.txt: The `hyperparameter_tuning` module, which creates a parent run for the tuning job and a nested child run for each trial, logging the parameters and final validation score for each.",
            "src/module_77.txt: The `model_monitoring` service, which periodically logs production performance metrics (e.g., inference latency, prediction drift) to a dedicated experiment."
          ]
        },
        "proposed_api": [
          {
            "endpoint": "POST /api/v1/runs",
            "description": "Create a new run. Takes an `experiment_name` and returns a `run_id`.",
            "payload": "{ \"experiment_name\": \"string\", \"tags\": { \"key\": \"value\" } }"
          },
          {
            "endpoint": "POST /api/v1/runs/{run_id}/metrics",
            "description": "Log a batch of metrics for a given run.",
            "payload": "{ \"metrics\": [{ \"key\": \"loss\", \"value\": 0.123, \"step\": 100 }] }"
          },
          {
            "endpoint": "POST /api/v1/runs/{run_id}/params",
            "description": "Log a batch of parameters for a given run.",
            "payload": "{ \"params\": [{ \"key\": \"learning_rate\", \"value\": \"0.001\" }] }"
          },
          {
            "endpoint": "GET /api/v1/runs/{run_id}",
            "description": "Get all data for a specific run, including its metrics, params, and artifacts."
          },
          {
            "endpoint": "PATCH /api/v1/runs/{run_id}",
            "description": "Update a run's status (e.g., to 'FINISHED' or 'FAILED').",
            "payload": "{ \"status\": \"FINISHED\" }"
          }
        ],
        "refactoring_plan": [
          {
            "module": "src/module_16.txt",
            "change": "Replace direct calls to the internal tracking service (e.g., `tracking_service.log_metric(...)`) with HTTP client calls to the new microservice endpoints (e.g., `POST /api/v1/runs/{run_id}/metrics`)."
          },
          {
            "module": "src/module_34.txt",
            "change": "Modify the tuning loop to create runs via the API. This will likely involve more significant changes to handle the state of each trial asynchronously."
          },
          {
            "module": "src/module_77.txt",
            "change": "Update the monitoring agent to push metrics to the new API endpoint instead of calling the internal Rust functions."
          }
        ]
      },
      "evaluation_criteria": [
        {
          "name": "Correctness of Core Component Identification",
          "description": "Agent correctly identifies a plausible set of modules responsible for experiment tracking, based on code analysis."
        },
        {
          "name": "Accuracy of Dependency Mapping",
          "description": "Agent correctly identifies both key modules that are depended upon (e.g., DB, config) and key modules that are consumers (e.g., training, tuning)."
        },
        {
          "name": "Quality and Completeness of API Design",
          "description": "The proposed API is well-structured, RESTful, and covers the essential functionalities of creating runs and logging data. The design is logical and practical."
        },
        {
          "name": "Viability of Refactoring Plan",
          "description": "The agent correctly identifies the modules that need to be refactored and accurately describes the nature of the required changes (i.e., replacing internal calls with API calls)."
        },
        {
          "name": "Clarity of Rationale",
          "description": "The agent provides clear justifications for its conclusions, referencing evidence from the codebase to support its identification of modules and dependencies."
        }
      ],
      "language": "rust",
      "score": 0.9208
    },
    {
      "id": "typescript_system_monitoring_expert_061_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for Multi-Tenant Configuration Auditing",
      "context_length": 1021293,
      "files_count": 76,
      "task_prompt": "Your task is to analyze the PulseSphere SocialOps codebase and produce an architectural report detailing the end-to-end data flow for a user-defined alerting rule. \n\nYour report must:\n1.  Identify the sequence of primary modules involved in the process, starting from a hypothetical API endpoint that receives the new rule, through its validation and storage, to its use in evaluating incoming metrics, and finally to dispatching a notification.\n2.  For each module in the sequence, specify its filename (e.g., `src/module_XX.ts`).\n3.  For each identified module, provide a concise (1-2 sentence) description of its specific role *in this particular workflow*.\n4.  Identify the core architectural pattern that decouples the components in this workflow.",
      "ground_truth": "The agent is expected to identify the following sequence and architectural pattern. The exact description of each module's role may vary, but it should capture the core function.\n\n**Core Architectural Pattern:** The system uses an **Event-Driven (Publish/Subscribe)** architecture to decouple its components. A central event bus is likely defined or configured in `src/config.ts` and used throughout the application.\n\n**Module-by-Module Data Flow:**\n\n1.  **Configuration Ingestion & Validation:**\n    -   `src/module_49.ts`: **API Handler.** Receives raw configuration data from an external source (e.g., REST API). Performs initial sanitization and publishes a `CONFIG_RECEIVED` event.\n    -   `src/module_20.ts`: **Validation Service.** Subscribes to `CONFIG_RECEIVED`. It validates the configuration data against a schema and business logic. On success, it publishes a `CONFIG_VALIDATED` event.\n\n2.  **Configuration Persistence & Caching:**\n    -   `src/module_63.ts`: **Persistence Manager.** Subscribes to `CONFIG_VALIDATED`. It is responsible for writing the validated alert rule to the primary database, interacting with lower-level storage abstractions.\n    -   `src/module_1.ts`: **Configuration Cache.** Also subscribes to `CONFIG_VALIDATED`. It updates a high-performance, in-memory cache of active alerting rules to be used by the evaluation engine.\n\n3.  **Metric Processing & Alert Evaluation:**\n    -   `src/module_5.ts`: **Metric Ingestor.** Receives raw performance metrics from monitored systems and publishes a `METRIC_RECEIVED` event.\n    -   `src/module_10.ts`: **Alerting Engine.** Subscribes to `METRIC_RECEIVED`. When a metric arrives, this module fetches the relevant rules from the Configuration Cache (`module_1`), evaluates the metric against the rules, and publishes an `ALERT_TRIGGERED` event if a threshold is breached.\n\n4.  **Notification Dispatch:**\n    -   `src/module_45.ts`: **Notification Dispatcher.** Subscribes to `ALERT_TRIGGERED`. This module acts as a router, determining which channel(s) (e.g., email, Slack) the alert should be sent to based on the rule's configuration.\n    -   `src/module_29.ts` / `src/module_66.ts`: **Notification Handlers.** These are concrete implementation modules called by the Dispatcher to send the alert over a specific channel (e.g., `module_29` for a Slack integration, `module_66` for email). The agent only needs to identify one of these as the final step.",
      "evaluation_criteria": [
        "Correctly identifies the core architectural pattern as Event-Driven or Publish/Subscribe.",
        "Correctly identifies at least 6 of the 8 key modules in the ground truth sequence.",
        "The sequence of identified modules must be logically correct, representing the flow of data.",
        "The description for each identified module must accurately reflect its function within this specific workflow.",
        "Demonstrates understanding of the separation of concerns (e.g., correctly distinguishing the Persistence Manager from the Cache).",
        "Correctly identifies the role of events as the communication mechanism between the decoupled modules."
      ],
      "language": "typescript",
      "score": 0.9187
    },
    {
      "id": "cpp_data_analytics_expert_010_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Diagnose and Resolve Alerting Latency Under Concurrent Workloads",
      "context_length": 938016,
      "files_count": 82,
      "task_prompt": "As the lead architect, your task is to analyze the CardioInsight360 architecture to diagnose the root cause of the real-time alerting latency and propose a robust, minimally invasive solution. You must provide a detailed technical plan that a senior developer could implement.\n\nYour analysis and proposal must include:\n1.  **Architectural Trace:** Identify and describe the complete component paths for both the real-time alerting workload (from HL7 ingestion to alert dispatch) and the batch processing workload. Reference specific components, classes, and architectural documents.\n2.  **Root Cause Hypothesis:** Based on your trace and analysis of the code, formulate a precise hypothesis explaining the resource contention. Pinpoint the specific shared resources (e.g., CPU, I/O, thread pools, event bus capacity) and the components responsible for the contention.\n3.  **Solution Proposal:** Propose a specific, targeted architectural modification to mitigate the issue. Your proposal must detail:\n    a. The conceptual change (e.g., workload prioritization, resource partitioning).\n    b. A list of the primary files and classes that will need to be modified.\n    c. A high-level description of the changes required in each file.\n    d. An explanation of how your solution will resolve the contention while minimizing impact on the existing system.",
      "ground_truth": "The core architectural flaw is the lack of explicit resource management and workload prioritization. All services compete in a single, shared resource domain, leading to the starvation of the latency-sensitive stream processing workload by the throughput-oriented batch workload.\n\n**Key Insight:** The system needs a mechanism to partition resources. The most effective and architecturally sound solution is the introduction of dedicated, configurable thread pools for different workload types.\n\n**Implementation Plan Outline:**\n1.  **Configuration (`main_config.json.template`):**\n    - Add a new section, `thread_pools`, to the configuration.\n    - Define two pools: `realtime_critical` (e.g., 4 threads, high priority) and `batch_analytics` (e.g., 8 threads, normal priority).\n2.  **Configuration Manager (`core/config_manager.h/cpp`):**\n    - Add logic to parse the `thread_pools` configuration into a structured data model.\n3.  **Service Manager (`services/service_manager.h/cpp`):**\n    - In the constructor or an `init` method, create the thread pools based on the loaded configuration. Store them in a map (e.g., `std::map<std::string, std::unique_ptr<ThreadPool>>`).\n    - Add a getter method like `getThreadPool(const std::string& name)`.\n    - Modify the service creation logic. When creating `StreamProcessor`, pass it the `realtime_critical` pool. When creating `BatchProcessor`, pass it the `batch_analytics` pool.\n4.  **Processor Components (`processing/stream_processor.h/cpp`, `processing/batch_processor.h/cpp`):**\n    - Modify their constructors to accept a `ThreadPool&` or `std::shared_ptr<ThreadPool>`.\n    - All asynchronous tasks within these components must be submitted to their assigned pool instead of a global or new pool.\n\nThis solution effectively isolates the workloads, ensuring the real-time path always has the resources it needs, directly addressing the latency problem without requiring a major rewrite of the business logic.",
      "evaluation_criteria": [
        "**Correct Component Identification:** Accurately identifies the key classes and components in both the real-time and batch processing data flows (e.g., `HL7MLLPConnector`, `StreamProcessor`, `AlertingService` vs. `SchedulingService`, `BatchProcessor`, `ETLPipeline`).",
        "**Accurate Hypothesis:** Correctly diagnoses resource contention (primarily CPU) as the root cause, citing the lack of workload prioritization in the `ServiceManager` as the specific architectural flaw.",
        "**Architectural Soundness of Solution:** Proposes a robust solution like dedicated thread pools rather than naive fixes (e.g., 'increase server CPU' or 'add random delays'). The solution should be extensible and maintainable.",
        "**Feasibility and Specificity:** The proposal must be concrete, listing the specific files (`service_manager.cpp`, `main_config.json.template`, etc.) and the nature of the changes required for each.",
        "**Synthesis of Information:** Demonstrates the ability to connect information from multiple sources, such as correlating the flow in `data_flow_diagram.md` with the implementation in `service_manager.cpp` and `etl_pipeline.cpp`.",
        "**Minimal Invasiveness:** The proposed solution should respect the existing architecture and patterns, modifying them gracefully rather than suggesting a complete redesign."
      ],
      "language": "cpp",
      "score": 0.9175
    },
    {
      "id": "java_web_ecommerce_expert_000_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis of a Race Condition in the Order Processing Workflow",
      "context_length": 889082,
      "files_count": 83,
      "task_prompt": "You are a Principal Software Engineer tasked with performing a root cause analysis of the duplicate order incident. Your goal is to identify the underlying architectural flaw and propose a robust solution.\n\n1.  **Analyze the System:** Examine the provided files to understand the complete order creation and payment processing flow. Trace the interactions between the primary components involved: `OrderController`, `OrderService`, `PaymentOrchestrationService`, `StripePaymentGateway`, and the relevant data models (`Order`, `PaymentTransaction`).\n\n2.  **Identify the Flaw:** Pinpoint the specific architectural weakness or code pattern that allows for duplicate order creation and payment capture, especially when a client retries a request after a network timeout.\n\n3.  **Propose a Solution:** Formulate a detailed architectural solution to make the order creation process idempotent. Your proposal must be comprehensive and include:\n    *   The conceptual pattern to be used (e.g., Idempotency-Key pattern).\n    *   Specific changes required in the data models (e.g., `Order`, `PaymentTransaction`).\n    *   Modifications needed in the service layer logic (e.g., `OrderService`, `PaymentOrchestrationService`).\n    *   Any necessary changes to the API contract (e.g., DTOs, controller method signatures).\n    *   A brief description of the required database schema migration.\n\nYou are not required to write the full implementation code, but you must clearly articulate *what* needs to change, *where*, and *why*.",
      "ground_truth": "### Architectural Flaw\n\nThe root cause of the issue is that the order creation API endpoint is not idempotent. The system lacks a mechanism to uniquely identify and de-duplicate requests for the same transaction. A client-side retry, triggered by a timeout, initiates a completely new and independent execution of the order creation logic. Since the payment processing and order record creation are not an atomic operation from the client's perspective, this retry leads to duplicate state (two orders in the DB) and duplicate side effects (two charges on the payment gateway).\n\n### Proposed Solution\n\nImplement the **Idempotency-Key** pattern to ensure that repeated requests for the same logical operation do not result in duplicate processing.\n\n1.  **API Contract Change (DTO):**\n    *   **File:** `CommerceSphereEnterpriseSuite/src/main/java/com/commercesphere/enterprise/ordering/dto/OrderDto.java` (or a more specific `CreateOrderRequestDto` if one were created).\n    *   **Change:** Add a new field to the DTO to carry the idempotency key.\n        ```java\n        private String idempotencyKey;\n        ```\n\n2.  **Data Model Change:**\n    *   **File:** `CommerceSphereEnterpriseSuite/src/main/java/com/commercesphere/enterprise/ordering/model/Order.java`\n    *   **Change:** Add a field to the `Order` entity to store the key. It must be unique.\n        ```java\n        @Column(name = \"idempotency_key\", unique = true, nullable = false, updatable = false)\n        private String idempotencyKey;\n        ```\n\n3.  **Database Schema Migration:**\n    *   **File:** Create a new file `CommerceSphereEnterpriseSuite/src/main/resources/db/migration/V3__Add_Idempotency_Key_To_Orders.sql`.\n    *   **Change:** Add the column to the `orders` table with a unique constraint.\n        ```sql\n        ALTER TABLE orders ADD COLUMN idempotency_key VARCHAR(255);\n        UPDATE orders SET idempotency_key = gen_random_uuid() WHERE idempotency_key IS NULL;\n        ALTER TABLE orders ALTER COLUMN idempotency_key SET NOT NULL;\n        ALTER TABLE orders ADD CONSTRAINT uk_orders_idempotency_key UNIQUE (idempotency_key);\n        ```\n\n4.  **Service Layer Logic Modification:**\n    *   **File:** `CommerceSphereEnterpriseSuite/src/main/java/com/commercesphere/enterprise/ordering/service/OrderService.java`\n    *   **Change:** Modify the primary order creation method (e.g., `createOrderFromDto`).\n        *   The method must be annotated with `@Transactional`.\n        *   At the beginning of the method, query the `OrderRepository` to find an order by the provided `idempotencyKey`.\n        *   **If an order is found:** Immediately return that existing order, preventing any further processing.\n        *   **If no order is found:** Proceed with the existing logic of creating the order, calling the `PaymentOrchestrationService`, etc. Crucially, set the `idempotencyKey` on the new `Order` entity before saving it. The database's unique constraint will act as the ultimate guard against race conditions where two threads might pass the initial check simultaneously.",
      "evaluation_criteria": [
        "**Flaw Identification:** Correctly identifies the lack of idempotency as the core architectural flaw, rather than blaming a specific component in isolation.",
        "**Critical Path Analysis:** Demonstrates understanding of the call chain from the `OrderController` through the `OrderService` to the `PaymentOrchestrationService` and `OrderRepository`.",
        "**Solution Pattern:** Proposes a standard, robust pattern like the Idempotency-Key pattern.",
        "**Completeness of Proposal:** The proposed solution correctly identifies the need for changes across multiple layers: API contract (DTO), data model (Entity), persistence (DB Migration), and service logic (Service class).",
        "**Consideration of Concurrency:** Mentions the importance of transactional boundaries (`@Transactional`) and/or database-level unique constraints to prevent race conditions in the check-then-act logic.",
        "**File Specificity:** Accurately names the key files that would need to be modified (e.g., `Order.java`, `OrderService.java`, and the need for a new SQL migration file)."
      ],
      "language": "java",
      "score": 0.9083
    },
    {
      "id": "python_desktop_development_expert_021_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Implement a System-Wide 'Focus Mode' Feature",
      "context_length": 1018897,
      "files_count": 73,
      "task_prompt": "Your task is to implement the 'Focus Mode' feature across the FlockDesk application. This requires understanding and modifying multiple parts of the system, from core services to individual modules.\n\n### Requirements:\n\n1.  **Persistent State:** Introduce a new global setting, `focus_mode_enabled` (boolean), that persists across application restarts. The `SettingsService` is the designated component for managing user preferences.\n\n2.  **Global Toggle:** Add a new menu item under the main 'View' menu in the application's menu bar to toggle Focus Mode on and off. This action should update the persistent state.\n\n3.  **Event-Driven Communication:** When the Focus Mode state changes, a global event must be broadcasted across the application. You must use the existing `EventBus` for this purpose. Do not create direct dependencies between the menu bar and other modules.\n\n4.  **Module-Level Reaction (Chat):** Modify the `Chat` module to react to Focus Mode changes. When Focus Mode is **enabled**, the chat module should visually indicate that notifications are snoozed and should suppress any new incoming message notifications.\n\n5.  **Module-Level Reaction (Dashboard):** Modify the `Dashboard` module to react to Focus Mode changes. When Focus Mode is **enabled**, the main activity feed widget on the dashboard should be visually de-emphasized (e.g., greyed out or have its opacity lowered) to reduce visual noise.\n\nYour implementation must respect the existing architectural patterns (Event-Driven, MVVM, Service Layer).",
      "ground_truth": "The solution is a set of changes across multiple files that correctly uses the established architectural patterns.\n\n1.  **`configs/default_settings.json`**: Added `\"focus_mode_enabled\": false`.\n2.  **`flockdesk/core/ipc/event_types.py`**: Added a new event type like `FOCUS_MODE_CHANGED = 'system/focus_mode_changed'`.\n3.  **`flockdesk/core/shortcuts/commands.py`**: A new class `ToggleFocusModeCommand` is implemented. Its `execute` method gets the `SettingsService` and `EventBus` (likely via a service locator or singleton pattern), toggles the setting, and publishes the `FOCUS_MODE_CHANGED` event with the new boolean state.\n4.  **`flockdesk/core/shell/menu_bar.py`**: A new `QAction` for 'Toggle Focus Mode' is created and connected to an instance of `ToggleFocusModeCommand`.\n5.  **`flockdesk/modules/chat/viewmodel/chat_vm.py`**: The ViewModel's `__init__` subscribes to `FOCUS_MODE_CHANGED`. A handler method updates a new property (e.g., `self.is_snoozed`). The logic for suppressing notifications would also be tied to this property.\n6.  **`flockdesk/modules/dashboard/viewmodel/dashboard_vm.py`**: The ViewModel's `__init__` subscribes to `FOCUS_MODE_CHANGED`. A handler method updates a new property (e.g., `self.activity_feed_opacity`).\n7.  **`flockdesk/modules/dashboard/view/dashboard_widget.py`**: The widget's styling logic for the activity feed is connected to the new ViewModel property, dynamically changing its appearance.",
      "evaluation_criteria": [
        "**Architectural Adherence (Event Bus):** Did the agent correctly use the `EventBus` for broadcasting the state change, or did it attempt to create direct, tightly-coupled calls between UI components and modules?",
        "**Architectural Adherence (Service Layer):** Was the `SettingsService` correctly identified and used as the single source of truth for the persistent 'Focus Mode' state?",
        "**Architectural Adherence (MVVM):** Were the changes in the modules correctly implemented within the ViewModel layer, with the View layer being a passive observer of the ViewModel's state?",
        "**Component Discovery:** Did the agent successfully locate and modify the correct, disparate set of files required for the task (settings config, event types, menu bar, command definitions, and multiple module ViewModels)?",
        "**Code Modularity:** Is the new code well-integrated without breaking existing patterns? Is the `ToggleFocusModeCommand` self-contained and reusable?",
        "**Completeness:** Does the final implementation satisfy all functional requirements: persistence, a working toggle, and the specified UI changes in both the Chat and Dashboard modules?"
      ],
      "language": "python",
      "score": 0.908
    },
    {
      "id": "python_fintech_payment_expert_029_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Refactoring for Instant Payment Settlements",
      "context_length": 907679,
      "files_count": 81,
      "task_prompt": "As a lead architect, you are tasked with designing the integration of the 'Instant Settlement' feature. Your analysis must result in a clear architectural proposal. You are not required to write production code, but you must provide a detailed plan that demonstrates a deep understanding of the existing system.\n\nYour proposal, delivered as a single markdown file named `INSTANT_SETTLEMENT_PROPOSAL.md`, must include:\n\n1.  **Analysis of the Current Architecture:** Briefly explain how the standard payment settlement process works, identifying the key services (`transaction_service`, `wallet_service`, `risk_compliance_service`) and the architectural patterns used (e.g., Saga). Reference specific files that inform your analysis.\n\n2.  **Proposed Solution:** Describe your proposed architecture for the 'Instant Settlement' flow. This must detail:\n    a.  How the `transaction_service` will decide whether to trigger a standard or an instant settlement.\n    b.  The proposed interaction with the external 'InstaPay' API. Will this be synchronous or asynchronous? How does this fit into the existing event-driven system?\n    c.  The necessary modifications or additions to the `transaction_service`, specifically addressing its interaction with the `payment_saga`. Should the existing Saga be modified, or should a new pattern be used for the instant path?\n    d.  The impact on the `wallet_service`'s ledger. How will the ledger be updated upon a successful instant settlement while maintaining data integrity and consistency, especially considering its likely event-sourced nature?\n    e.  The required changes for the `risk_compliance_service` to support potentially different risk rules for high-speed transactions.\n\n3.  **Failure Handling:** Describe the compensation/rollback strategy if the 'InstaPay' API call fails after the funds have been reserved in the user's wallet.\n\n4.  **Sequence Diagram:** Provide a sequence diagram (using Mermaid syntax) illustrating the complete end-to-end flow for a successful 'Instant Settlement' transaction.",
      "ground_truth": "The core of a correct solution is the recognition that the new synchronous requirement cannot be naively shoehorned into the existing asynchronous Saga pattern. The proposal must introduce a parallel execution path.\n\n**Key Insights for a Correct Solution:**\n\n*   **Rejection of Saga Modification:** The solution explicitly states that modifying the `payment_saga` to include a long-running, synchronous, blocking API call is incorrect. It justifies this by explaining that it violates the Saga's principles of loose coupling and resilience.\n*   **Introduction of a Strategy Pattern (or similar):** The `transaction_service` should use a design pattern (like Strategy or a simple factory) to select the appropriate workflow (`StandardSaga` vs. `InstantSyncFlow`) based on transaction properties.\n*   **Decoupled Ledger Update:** The `wallet_service`'s ledger integrity is paramount. The solution must ensure that even in the synchronous flow, the final ledger update is triggered by consuming an immutable, internally-generated event (e.g., `InstantSettlementConfirmedEvent`) after the external API call succeeds. This respects the CQRS/Event Sourcing pattern.\n*   **Dedicated Compensation Logic:** The synchronous flow must have its own robust compensation logic. If the external API call fails, the flow must explicitly trigger compensating actions (e.g., publishing a `RevertFundReservation` event).\n*   **Example Mermaid Sequence Diagram:**\n```mermaid\nsequenceDiagram\n    participant Client\n    participant API Gateway\n    participant TransactionService\n    participant WalletService\n    participant RiskComplianceService\n    participant InstaPay API (External)\n\n    Client->>API Gateway: POST /v1/transactions (type=instant)\n    API Gateway->>TransactionService: create_transaction(data)\n    TransactionService->>TransactionService: new InstantSettlementStrategy\n    TransactionService->>RiskComplianceService: request_assessment(tx_id, type='instant')\n    RiskComplianceService-->>TransactionService: assessment_ok\n    TransactionService->>WalletService: reserve_funds(tx_id, amount)\n    WalletService-->>TransactionService: funds_reserved_ok\n    TransactionService->>InstaPay API (External): POST /settle (tx_data)\n    alt Successful Settlement\n        InstaPay API (External)-->>TransactionService: {status: 'SUCCESS', ref: 'xyz'}\n        TransactionService->>Kafka: Publish(topic='instant_settlements', event='InstantSettlementSucceeded')\n        Note over WalletService: Consumes event, commits ledger transaction\n        TransactionService-->>Client: {status: 'COMPLETED'}\n    else Settlement Fails\n        InstaPay API (External)-->>TransactionService: {status: 'FAILED', reason: '...'}\n        TransactionService->>Kafka: Publish(topic='wallet_commands', event='ReleaseReservedFunds')\n        Note over WalletService: Consumes event, reverts reservation\n        TransactionService-->>Client: {status: 'FAILED'}\n    end\n```",
      "evaluation_criteria": [
        "**Architectural Pattern Comprehension:** Did the agent correctly identify the Saga pattern and explain why a synchronous call within it is an anti-pattern?",
        "**Solution Soundness:** Is the proposed solution (e.g., using a Strategy pattern) architecturally sound and does it effectively segregate the synchronous and asynchronous flows?",
        "**Data Integrity:** Does the proposal protect the integrity of the `wallet_service` ledger by using an event-driven update mechanism instead of a direct, cross-service call?",
        "**Impact Analysis:** Did the agent correctly identify the necessary changes and impacts on all relevant services (`transaction_service`, `wallet_service`, `risk_compliance_service`)?",
        "**Failure & Compensation:** Is the proposed failure handling for the new synchronous path clear, correct, and robust?",
        "**Clarity and Completeness:** Is the final proposal well-structured, clear, and does it include all requested components, including a valid Mermaid sequence diagram?",
        "**File Reference:** Did the agent correctly reference key files (`payment_saga.py`, `saga_pattern.md`, etc.) to support its analysis of the current system?"
      ],
      "language": "python",
      "score": 0.9063
    },
    {
      "id": "rust_blockchain_nft_expert_071_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Blueprint for Fractional NFT (F-NFT) Integration",
      "context_length": 873424,
      "files_count": 83,
      "task_prompt": "You are the lead architect for the CanvasChain Symphony project. Your task is to produce a detailed technical design document for the upcoming Fractional NFT (F-NFT) feature. Your document should be a roadmap for the development team.\n\nYour analysis must include:\n1.  **Impact Analysis**: Identify every service, shared crate, and critical file that will be impacted by the introduction of F-NFTs.\n2.  **Data Model Changes**: Propose specific changes to the core data structures, particularly within the `ccs_common` crate, to represent fractional ownership.\n3.  **Component-Level Design**: For each impacted microservice (`node_service`, `minting_service`, `wallet_service`, `marketplace_service`, etc.), detail the necessary changes. This includes:\n    - New or modified gRPC/RPC endpoints.\n    - Changes to business logic modules (e.g., `staking_logic.rs`, `auction_engine.rs`).\n    - Modifications to state management (`state_machine.rs`, `state_db.rs`).\n4.  **Core User Flows**: Describe the end-to-end sequence of operations and inter-service communication for two key user flows:\n    a.  An owner fractionalizing their existing NFT.\n    b.  A new user purchasing a fraction of an NFT on the marketplace.\n5.  **API & Configuration**: Detail the required changes to the `api_gateway` (including the OpenAPI spec) and any new configuration parameters needed in the `.toml` configuration files.",
      "ground_truth": "The expected output is a detailed markdown document. Key insights the agent must demonstrate include:\n\n*   **Centrality of `ccs_common` and `node_service`**: The most critical changes are to the shared `Nft` struct in `crates/ccs_common/src/nft.rs` and the state transition logic in `services/node_service/src/state_machine.rs`. Any valid plan must start here.\n*   **State Representation**: A new data structure must be proposed for the state database (`ccs_db`) to track fractional owners and their shares, linking them back to the original NFT's ID.\n*   **New Transaction Type**: The plan must specify a new transaction type (e.g., `Fractionalize { nft_id, fraction_count }`) and describe how the `state_machine` would process it atomically.\n*   **Communication Protocol**: The plan must correctly identify gRPC as the primary inter-service communication method and propose new `.proto` definitions in `crates/ccs_proto/` and corresponding server/client implementations.\n*   **Impact on Marketplace Logic**: The agent must recognize that selling fractions is different from selling a whole NFT. The logic in `marketplace_service/src/listing_manager.rs` and `auction_engine.rs` needs to be adapted to handle fungible fractions of a non-fungible asset.\n*   **Flow Example (Purchase)**: A correct purchase flow would look like: `Client -> api_gateway (REST)` -> `api_gateway -> marketplace_service (gRPC)` -> `marketplace_service creates purchase transaction` -> `Transaction submitted to node_service mempool` -> `node_service consensus handler includes in block` -> `node_service state_machine updates ownership in state_db`.",
      "evaluation_criteria": [
        "**Completeness of Impact Analysis**: Did the agent correctly identify the majority of impacted services (node, minting, wallet, marketplace, api_gateway, governance) and shared crates (ccs_common, ccs_proto, ccs_db)?",
        "**Correctness of Proposed Changes**: Are the proposed changes consistent with the existing architecture (e.g., using gRPC, modifying the state machine for core logic)?",
        "**Depth of Component-Level Detail**: Does the analysis go beyond surface-level statements? Does it pinpoint specific files, modules, and functions (e.g., `state_machine.rs`, `auction_engine.rs`) that need modification?",
        "**Logical Coherence of User Flows**: Are the described end-to-end communication flows for fractionalization and purchasing logical and technically sound within the project's architecture?",
        "**Identification of Core Abstractions**: Did the agent correctly identify the modification of `ccs_common/src/nft.rs` and the introduction of a new transaction type in `node_service` as the foundational changes?",
        "**Architectural Reasoning**: Does the agent's plan implicitly or explicitly demonstrate an understanding of why changes must be made in a certain way (e.g., why state changes must be validated by the consensus nodes and not just in a peripheral service)?"
      ],
      "language": "rust",
      "score": 0.9043
    },
    {
      "id": "c_ml_nlp_expert_017_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Analysis and Critique of the MVC Pattern in the LexiLearn C-based ML Orchestrator",
      "context_length": 946983,
      "files_count": 77,
      "task_prompt": "You are a senior software architect tasked with onboarding a new team member. To do this, you need to create a definitive guide to the LexiLearn Orchestrator's architecture. Your primary focus is to explain the system's core MVC design pattern and its implications.\n\nPerform the following actions:\n\n1.  **Map Components to MVC Roles:** Based on the provided source code, identify which high-level directories and key components (e.g., `orchestrator.c`, `pipeline_manager.h`, `model_registry.c`, `dashboard_server.c`, `model_monitor.h`) map to the Model, the View, and the Controller. Justify your mapping.\n\n2.  **Describe Communication Pathways:** Explain the primary communication mechanisms and data flow between the Model, View, and Controller layers. How does the Controller command the Model? How does the Model report state changes? How is the View updated with new information from the other layers?\n\n3.  **Trace a Critical Workflow:** Trace the complete sequence of events and component interactions that occur when the `DriftDetector` (`src/controller/monitoring/drift_detector.h`) identifies a significant data drift, which in turn triggers an automated retraining job. Detail the path of this event, starting from the detector and following it through the relevant controller, pipeline, and scheduler components until a new training job is ready to run.\n\n4.  **Critique the Architecture:** Provide a critical analysis of using the MVC pattern for this C-based ML orchestration system. What are the primary benefits this pattern provides in this context? What are the most significant drawbacks or architectural smells that arise from this choice? Be specific and reference the project's domain (ML orchestration) and implementation language (C).",
      "ground_truth": {
        "component_mapping": {
          "Model": "Contains the entire ML domain logic. Key components: `src/model` directory, including `ml_pipeline/` (training, evaluation), `ml_models/` (model implementations), `feature_store/`, and `preprocessing/`.",
          "View": "Responsible for presenting data and system status. Key components: `src/view` directory, primarily `dashboard_server.c` which serves monitoring data, and `src/common/ll_logger.c` which outputs logs that can be considered a form of view.",
          "Controller": "The core orchestration and decision-making engine. Key components: `src/controller` directory, including `orchestrator.c` (the main controller), `pipeline/` (managing pipeline stages), `monitoring/` (detecting events), and `scheduler/` (executing tasks)."
        },
        "workflow_trace": "1. `drift_detector.c` detects drift and notifies its parent `model_monitor.c`. 2. `model_monitor.c`, acting as an observable subject, uses the observer pattern (`observer.h`) to send a notification. 3. `orchestrator.c`, registered as an observer, receives the drift notification. 4. `orchestrator.c` invokes logic in `retraining_trigger.c` to evaluate the drift against predefined thresholds. 5. If retraining is confirmed, `orchestrator.c` calls `pipeline_manager.c` to create a new training pipeline. 6. `pipeline_manager.c` defines the stages (`ingestion`, `training`, `deployment`) and uses `job_factory.c` to package this pipeline into a runnable job. 7. The job is submitted to `task_scheduler.c`, which queues it for execution.",
        "architectural_critique": {
          "benefits": [
            "**Separation of Concerns:** Isolates complex ML logic (Model) from orchestration (Controller) and reporting (View).",
            "**Modularity:** New models (`hybrid_model.c`) or monitoring techniques can be added to the Model layer with minimal changes to the Controller."
          ],
          "drawbacks": [
            "**Pattern Mismatch:** MVC is for UI event loops, not for long-running, asynchronous backend tasks. This leads to a 'leaky abstraction'.",
            "**Controller Bloat:** The Controller layer becomes overly complex, managing state, scheduling, and error handling for all pipeline operations, risking it becoming a 'God Object'.",
            "**Inefficient Communication:** The interaction between the long-running Model (a training job) and the Controller is not a simple request/response. In C, this requires complex mechanisms like callbacks, observer patterns, or IPC, which can be more cumbersome than patterns designed for data pipelines (e.g., Pipes and Filters)."
          ]
        }
      },
      "evaluation_criteria": [
        "**Component Mapping Accuracy:** Did the agent correctly map the key directories and files to their MVC roles?",
        "**Communication Pathway Identification:** Did the agent correctly identify the use of direct function calls from Controller to Model and the observer pattern for Model-to-Controller notifications?",
        "**Workflow Trace Correctness:** Was the agent able to accurately trace the retraining event across the `monitoring`, `orchestrator`, `pipeline`, and `scheduler` components in the correct sequence?",
        "**Critique Nuance and Depth:** Did the agent provide a balanced critique with valid pros and cons that are specific to the context of a C-based ML orchestrator, rather than generic MVC definitions?",
        "**Evidence-Based Reasoning:** Does the agent's response cite specific files, components, or design patterns (e.g., `observer.h`) to substantiate its analysis?"
      ],
      "language": "c",
      "score": 0.903
    },
    {
      "id": "c_api_microservice_expert_080_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Centralize Disparate Logging Mechanisms into a Unified Logging Module",
      "context_length": 1101018,
      "files_count": 77,
      "task_prompt": "Your task is to introduce a centralized logging system and refactor the entire codebase to use it. You must preserve all existing functionality.\n\n1.  **Create a new logging module:**\n    -   Create two new files: `src/logging.h` and `src/logging.c`.\n\n2.  **Implement the logging API in `src/logging.h` and `src/logging.c`:**\n    -   The system must support distinct log levels. Define an enum in `src/logging.h`:\n        `typedef enum { LOG_LEVEL_DEBUG, LOG_LEVEL_INFO, LOG_LEVEL_WARN, LOG_LEVEL_ERROR } LogLevel;`\n    -   Create an initialization function: `void log_init(LogLevel level, const char* log_file);`. This function should set the minimum log level to output and the destination file. If `log_file` is `NULL`, all logs should go to `stderr`.\n    -   Create a core, variadic logging function: `void log_message(LogLevel level, const char* file, int line, const char* fmt, ...);`. This function should handle the actual logging logic, including checking against the configured log level.\n    -   Create a set of convenience macros in `src/logging.h` that automatically provide file and line information:\n        -   `#define LOG_DEBUG(fmt, ...) log_message(LOG_LEVEL_DEBUG, __FILE__, __LINE__, fmt, ##__VA_ARGS__)`\n        -   `#define LOG_INFO(fmt, ...) log_message(LOG_LEVEL_INFO, __FILE__, __LINE__, fmt, ##__VA_ARGS__)`\n        -   `#define LOG_WARN(fmt, ...) log_message(LOG_LEVEL_WARN, __FILE__, __LINE__, fmt, ##__VA_ARGS__)`\n        -   `#define LOG_ERROR(fmt, ...) log_message(LOG_LEVEL_ERROR, __FILE__, __LINE__, fmt, ##__VA_ARGS__)`\n\n3.  **Integrate the logging system:**\n    -   Locate the application's main entry point, which is the function `mercury_hub_main` within `src/module_31.txt`.\n    -   In `mercury_hub_main`, add a call to `log_init()` at the beginning. Configure it to use `LOG_LEVEL_INFO` and output to `stderr` (pass `NULL` for the filename).\n\n4.  **Perform the cross-file refactoring:**\n    -   Systematically scan all source files in the `src/` directory.\n    -   Identify and replace all ad-hoc logging calls. These primarily consist of `printf(...)` and `fprintf(stderr, ...)` statements that are clearly used for logging (e.g., they start with prefixes like \"INFO:\", \"DEBUG:\", \"[ERROR]\").\n    -   Map the old logging statements to the new macros based on their content:\n        -   Statements indicating errors, failures, or fatal conditions should use `LOG_ERROR`.\n        -   Statements with \"warn\" or \"warning\" should use `LOG_WARN`.\n        -   Statements used for debugging (e.g., prefixed with \"DEBUG:\") should use `LOG_DEBUG`.\n        -   All other informational logging should use `LOG_INFO`.\n    -   Ensure that the format string and all variable arguments from the original `printf`/`fprintf` calls are correctly passed to the new logging macros.\n\n5.  **Update Includes:**\n    -   For every source file that you modify to use the new logging macros, add `#include \"logging.h\"` at the top.",
      "ground_truth": "The ground truth is a description of the final state, not the full code of all 70+ modified files.\n\n**1. New File: `src/logging.h`**\n```c\n#ifndef LOGGING_H\n#define LOGGING_H\n\n#include <stdio.h>\n#include <stdarg.h>\n\n// Public API for the logging module\n\ntypedef enum {\n    LOG_LEVEL_DEBUG,\n    LOG_LEVEL_INFO,\n    LOG_LEVEL_WARN,\n    LOG_LEVEL_ERROR\n} LogLevel;\n\nvoid log_init(LogLevel level, const char* log_file);\nvoid log_message(LogLevel level, const char* file, int line, const char* fmt, ...);\n\n#define LOG_DEBUG(fmt, ...) log_message(LOG_LEVEL_DEBUG, __FILE__, __LINE__, fmt, ##__VA_ARGS__)\n#define LOG_INFO(fmt, ...) log_message(LOG_LEVEL_INFO, __FILE__, __LINE__, fmt, ##__VA_ARGS__)\n#define LOG_WARN(fmt, ...) log_message(LOG_LEVEL_WARN, __FILE__, __LINE__, fmt, ##__VA_ARGS__)\n#define LOG_ERROR(fmt, ...) log_message(LOG_LEVEL_ERROR, __FILE__, __LINE__, fmt, ##__VA_ARGS__)\n\n#endif // LOGGING_H\n```\n\n**2. New File: `src/logging.c`**\n```c\n#include \"logging.h\"\n#include <time.h>\n#include <string.h>\n\nstatic struct {\n    LogLevel level;\n    FILE* output;\n} config = {LOG_LEVEL_INFO, NULL};\n\nvoid log_init(LogLevel level, const char* log_file) {\n    config.level = level;\n    if (log_file) {\n        config.output = fopen(log_file, \"a\");\n        if (!config.output) {\n            config.output = stderr; // Fallback to stderr\n            fprintf(stderr, \"ERROR: Could not open log file %s. Falling back to stderr.\\n\", log_file);\n        }\n    } else {\n        config.output = stderr;\n    }\n}\n\nvoid log_message(LogLevel level, const char* file, int line, const char* fmt, ...) {\n    if (level < config.level) {\n        return;\n    }\n\n    // Get current time\n    time_t timer = time(NULL);\n    struct tm* tm_info = localtime(&timer);\n    char time_buf[26];\n    strftime(time_buf, 26, \"%Y-%m-%d %H:%M:%S\", tm_info);\n\n    const char* level_str[] = {\"DEBUG\", \"INFO\", \"WARN\", \"ERROR\"};\n\n    // Print log prefix\n    fprintf(config.output, \"%s [%s] (%s:%d): \", time_buf, level_str[level], file, line);\n\n    // Print user message\n    va_list args;\n    va_start(args, fmt);\n    vfprintf(config.output, fmt, args);\n    va_end(args);\n\n    fprintf(config.output, \"\\n\");\n    fflush(config.output);\n}\n```\n\n**3. Modification in `src/module_31.txt`:**\n- The file `src/module_31.txt` should now contain `#include \"logging.h\"`.\n- The function `mercury_hub_main` should have `log_init(LOG_LEVEL_INFO, NULL);` as one of its first statements.\n\n**4. Example Refactoring in `src/module_42.txt`:**\n- **Before:** `fprintf(stderr, \"[ERROR] Failed to allocate memory for user session\\n\");`\n- **After:** `LOG_ERROR(\"Failed to allocate memory for user session\");`\n\n**5. Example Refactoring in `src/module_7.txt`:**\n- **Before:** `printf(\"INFO: Processing batch of %d records.\\n\", record_count);`\n- **After:** `LOG_INFO(\"Processing batch of %d records.\", record_count);`\n\n**6. Summary of Changes:**\n- The files `src/logging.c` and `src/logging.h` are created.\n- A significant number of files in `src/` (likely 50+) are modified to replace `printf`/`fprintf` with the new logging macros and to include `logging.h`.",
      "evaluation_criteria": [
        "**Correctness of New Module:** The implementation in `src/logging.c` and `src/logging.h` must be correct, complete, and free of bugs. It must handle variadic arguments and file I/O properly.",
        "**Completeness of Refactoring:** The agent must identify and replace all relevant ad-hoc logging statements across all source files. A partial refactoring is a partial success.",
        "**Accuracy of Replacement:** The agent must correctly map old log messages to the new log levels. It must also accurately transfer format strings and all associated arguments to the new macros without modification.",
        "**Code Integrity and Non-Regression:** The refactored code must be syntactically correct (conceptually compilable). No existing logic should be broken, and the functionality covered by `tests/test_main.txt` must be preserved.",
        "**Correct Initialization:** The `log_init()` function must be called exactly once at the specified entry point (`mercury_hub_main` in `src/module_31.txt`) with the correct parameters.",
        "**Header Management:** The agent must correctly add `#include \"logging.h\"` to all files where the new macros are used, and only those files."
      ],
      "language": "c",
      "score": 0.9025
    },
    {
      "id": "python_data_streaming_expert_085_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Consolidate Dispersed Data Validation Logic into a Centralized Framework",
      "context_length": 1152748,
      "files_count": 73,
      "task_prompt": "Your task is to refactor the data validation logic within the PulseStream Nexus application. You must centralize the disparate validation checks into a new, dedicated validation framework.\n\n1.  **Analyze the Codebase:** Examine `src/module_19.py`, `src/module_73.py`, `src/module_30.py`, and `src/utils.py` to identify all functions and inline code blocks responsible for data validation. Look for patterns like null checks, type assertions (`isinstance`), and regex matching for fields like emails, UUIDs, and timestamps.\n\n2.  **Design and Implement a Validation Framework:**\n    *   Create a new directory `src/validation/`.\n    *   Inside this directory, create a new file `core.py` to house the framework.\n    *   Design a class-based framework. It should include a base `Validator` abstract class and concrete implementations for common rules (e.g., `NotNullValidator`, `TypeValidator`, `RegexValidator`, `TimestampFormatValidator`).\n    *   The framework should allow for the composition of these validators to check a complete data record or dictionary.\n    *   Create a custom exception file `src/validation/exceptions.py` with a `ValidationError` class for standardized error handling.\n\n3.  **Refactor Existing Modules:**\n    *   Modify `src/module_19.py`, `src/module_73.py`, and `src/module_30.py` to use your new validation framework.\n    *   Remove the old, ad-hoc validation functions and inline checks from these modules.\n    *   Replace them with calls to your new, centralized validation logic from `src/validation/core.py`.\n\n4.  **Ensure Integrity:** The refactoring must not alter the application's core logic. Data that was previously considered valid must remain valid, and data that was previously rejected must still be rejected, now by raising the new `ValidationError`.",
      "ground_truth": "The expected outcome is a set of changes across multiple files:\n\n-   **New Files Created:**\n    -   `src/validation/core.py`: Contains a class-based validation framework. At a minimum, it should define a base validator class and several concrete implementations (e.g., for not-null, type, regex).\n    -   `src/validation/exceptions.py`: Contains a custom `ValidationError` class.\n\n-   **Modified Files:**\n    -   `src/module_19.py`: The original validation functions (e.g., `_validate_user_profile`) are removed. The code that calls them is updated to instantiate and use validators from `src/validation/core.py`. An import statement like `from validation.core import SchemaValidator, NotNullValidator` is added.\n    -   `src/module_73.py`: Similar to `module_19`, its transaction validation logic is replaced with calls to the new framework. Redundant code is deleted.\n    -   `src/module_30.py`: Its event validation logic is replaced with calls to the new framework. Redundant code is deleted.\n    -   `src/utils.py`: Any generic validation helper functions that were identified should be removed, and their call sites throughout the project (if any beyond the three target modules) should also be updated.\n\n-   **Code Structure Changes:**\n    -   The overall line count of the project should decrease due to the removal of duplicated code.\n    -   The cyclomatic complexity of the refactored modules should decrease as complex conditional validation blocks are replaced by clearer, declarative calls to the validation framework.",
      "evaluation_criteria": [
        "**Correctness of Abstraction:** The new framework in `src/validation/core.py` is well-designed, reusable, and follows good object-oriented principles.",
        "**Completeness of Refactoring:** All specified ad-hoc validation logic in `module_19`, `module_73`, and `module_30` has been successfully replaced.",
        "**Code Removal:** The old, redundant validation functions and inline checks have been completely removed from the refactored modules.",
        "**Functional Equivalence:** The system's validation behavior is preserved. No valid data is incorrectly rejected, and no invalid data is incorrectly accepted.",
        "**Cross-File Consistency:** The new validation framework is used consistently across all refactored files, including correct import statements.",
        "**Absence of Regressions:** The agent did not introduce syntax errors, break existing imports, or negatively impact other, unrelated parts of the codebase."
      ],
      "language": "python",
      "score": 0.9024
    },
    {
      "id": "c_api_graphql_expert_079_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Refactor Service-Level Error Handling into a Unified Common Library Abstraction",
      "context_length": 1000845,
      "files_count": 84,
      "task_prompt": "Your goal is to centralize and standardize error handling for the entire microservice suite.\n\n**1. Enhance the Common Error Abstraction:**\n   - In `libs/sc_common/include/sc_errors.h`, define a new structured error type, `sc_error_t`. This struct should contain:\n     - An integer `code` (the service-specific error code).\n     - A `char* message` (a detailed, dynamically allocated error message).\n     - An enumeration `sc_service_domain_t` to identify the originating service (e.g., `SC_DOMAIN_PALETTE`, `SC_DOMAIN_TEXTURE`, etc.). You will need to define this enum.\n     - An integer `http_status_code` (the recommended HTTP status to return, e.g., 404, 500).\n   - In `sc_errors.h` and `sc_errors.c`, create and implement factory and destructor functions for this new type:\n     - `sc_error_t* sc_error_create(sc_service_domain_t domain, int code, int http_status, const char* format, ...);` (variadic for easy message formatting)\n     - `void sc_error_destroy(sc_error_t* err);`\n\n**2. Refactor Core Services:**\n   - Select two services for this refactoring: `palette-service` and `texture-service`.\n   - Modify the function signatures within the service and repository layers (`palette_service.c`, `palette_repository.c`, `texture_service.c`, `texture_repository.c`) that currently return an integer error code. They should now return `sc_error_t*` on failure and `NULL` on success. You will need to update the function callers accordingly.\n   - Replace all instances of ad-hoc error code returns (e.g., `return -1;`) with calls to your new `sc_error_create()` factory.\n\n**3. Update the API Gateway:**\n   - Modify the `api-gateway`'s service client (`api-gateway/src/services/service_client.c`) and fallback handlers (`api-gateway/src/rest/fallback_handlers.c`) to correctly interpret the new `sc_error_t*` returned from the services.\n   - The gateway should no longer rely on simple integer codes. It must now use the `http_status_code` and `message` from the `sc_error_t` struct to generate the final HTTP response to the client.\n\n**4. Update Tests:**\n   - Modify the unit tests for the refactored services (`test_palette_service.c`, `test_texture_service.c`) to reflect the new function signatures and error handling logic. Tests that previously checked for integer return values like `-1` must now check for a non-NULL `sc_error_t*` and validate its contents.\n\n**5. Build and Verify:**\n   - Ensure the entire project, including the modified services, common library, and API gateway, compiles without warnings.\n   - Ensure all updated tests pass.",
      "ground_truth": "The solution requires modifications across multiple directories. Key changes would include:\n\n**1. In `libs/sc_common/include/sc_errors.h`:**\n```c\n// New enum\ntypedef enum {\n    SC_DOMAIN_UNKNOWN,\n    SC_DOMAIN_COMMON,\n    SC_DOMAIN_GATEWAY,\n    SC_DOMAIN_PALETTE,\n    SC_DOMAIN_TEXTURE,\n    SC_DOMAIN_AUDIO,\n    SC_DOMAIN_NARRATIVE\n} sc_service_domain_t;\n\n// New struct\ntypedef struct {\n    sc_service_domain_t domain;\n    int code; // Original, internal error code\n    int http_status_code;\n    char* message;\n} sc_error_t;\n\n// New function prototypes\nsc_error_t* sc_error_create(sc_service_domain_t domain, int code, int http_status, const char* format, ...);\nvoid sc_error_destroy(sc_error_t* err);\n```\n\n**2. Example refactoring in `services/palette-service/src/palette_service.c`:**\n\n*Before:*\n```c\nint get_palette_by_id(const char* id, palette_t** palette) {\n    // ... database logic ...\n    if (db_result == NOT_FOUND) {\n        return SC_PALETTE_ERROR_NOT_FOUND; // e.g., -1\n    }\n    // ... more logic ...\n    return 0; // Success\n}\n```\n\n*After:*\n```c\n#include \"sc_errors.h\"\n\nsc_error_t* get_palette_by_id(const char* id, palette_t** palette) {\n    // ... database logic ...\n    if (db_result == NOT_FOUND) {\n        return sc_error_create(SC_DOMAIN_PALETTE, SC_PALETTE_ERROR_NOT_FOUND, 404, \"Palette with ID '%s' not found.\", id);\n    }\n    // ... more logic ...\n    return NULL; // Success\n}\n```\n\n**3. Example refactoring in `api-gateway/src/rest/fallback_handlers.c`:**\n\n*Logic Change:*\nThe handler that previously might have had a `switch` statement on an integer error code now receives an `sc_error_t*`. It can directly use the fields:\n```c\nvoid handle_service_error(http_request_t* req, http_response_t* res, sc_error_t* err) {\n    if (err) {\n        set_http_status(res, err->http_status_code);\n        set_json_response_body(res, \"{\\\"error\\\": \\\"%s\\\"}\", err->message);\n        sc_error_destroy(err); // Clean up the error object\n    } else {\n        // ... handle unexpected case where error is NULL\n    }\n}\n```\n\n**4. List of files expected to be modified:**\n- `libs/sc_common/include/sc_errors.h`\n- `libs/sc_common/src/sc_errors.c`\n- `services/palette-service/include/palette_service.h`\n- `services/palette-service/src/palette_service.c`\n- `services/palette-service/src/palette_repository.c`\n- `services/palette-service/src/palette_handler.c`\n- `services/palette-service/tests/test_palette_service.c`\n- `services/texture-service/include/texture_service.h`\n- `services/texture-service/src/texture_service.c`\n- `services/texture-service/src/texture_repository.c`\n- `services/texture-service/src/texture_handler.c`\n- `services/texture-service/tests/test_texture_service.c`\n- `api-gateway/src/services/service_client.c`\n- `api-gateway/src/rest/fallback_handlers.c` (or equivalent router/error handling logic)\n- `CMakeLists.txt` files for affected modules if new dependencies are introduced (unlikely here, but possible).",
      "evaluation_criteria": [
        "**Correctness & Compilation:** The entire project must compile successfully without new warnings. All modified tests must pass, and no existing functionality should be broken.",
        "**Abstraction Quality:** The new `sc_error_t` abstraction in `sc_common` must be well-defined, and its helper functions must be correctly implemented, especially concerning memory management (e.g., `strdup` for the message, proper `free` in the destructor).",
        "**Refactoring Completeness:** Both `palette-service` and `texture-service` must be fully refactored to use the new error system, from the repository layer up to the handler.",
        "**Gateway Integration:** The API Gateway must correctly consume the new `sc_error_t` objects, using their fields to generate appropriate and consistent HTTP responses.",
        "**Test Adaptation:** Unit tests for the modified services must be updated to reflect the new function signatures and properly assert on the new error struct's contents.",
        "**Code Consistency:** The new error handling pattern must be applied consistently across all modified files.",
        "**Memory Safety:** The agent must correctly manage the lifecycle of `sc_error_t` objects, creating them on error and destroying them once handled to prevent memory leaks."
      ],
      "language": "c",
      "score": 0.9004
    },
    {
      "id": "rust_data_streaming_expert_013_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Abstract Data Ingestion Layer to Support Multiple Sources",
      "context_length": 1033033,
      "files_count": 81,
      "task_prompt": "Your task is to refactor the data ingestion mechanism in the ChirpPulse codebase. You must introduce a new abstraction layer for data sources.\n\n1.  **Create a new `DataSource` Trait**:\n    -   In a new directory and module, `src/data_sources/mod.rs`, define a new public trait named `DataSource`.\n    -   This trait must define an asynchronous method `stream(&self, config: &SourceConfig) -> Pin<Box<dyn Stream<Item = Result<serde_json::Value, anyhow::Error>> + Send>>`. This method will be responsible for connecting to a data source and yielding a stream of raw data objects.\n\n2.  **Isolate the Existing Implementation**:\n    -   The current, hardcoded data source logic is primarily located in `src/module_79.rs` (API client details) and is orchestrated by the stream processor in `src/module_41.rs`.\n    -   Create a new struct `LegacyStreamSource` in a new file, `src/data_sources/legacy_source.rs`.\n    -   Move the relevant connection and data fetching logic from `src/module_79.rs` and `src/module_41.rs` into this new struct.\n    -   Implement the `DataSource` trait for `LegacyStreamSource`.\n\n3.  **Decouple the Core Processor**:\n    -   Modify the primary stream processing orchestrator in `src/module_41.rs`. It currently creates and manages the connection directly.\n    -   Change it to accept a `Box<dyn DataSource>` as a parameter during its initialization.\n    -   The orchestrator should now use the `stream` method from the `DataSource` trait to get its data, instead of using the old, hardcoded functions.\n\n4.  **Update the Application Entrypoint**:\n    -   In the main application setup, likely in `src/module_1.rs` or a similar top-level module, you will now need to instantiate `LegacyStreamSource`, box it, and pass it to the orchestrator from `src/module_41.rs`.\n\n5.  **Update Tests**:\n    -   The tests in `tests/test_main.txt` and `tests/test_utils.txt` likely rely on the old, tightly-coupled structure. You must update them to reflect the new design. This will probably involve creating a mock `DataSource` for testing purposes or updating the test setup to inject the `LegacyStreamSource` instance.",
      "ground_truth": "The final solution must contain the following structural changes:\n\n-   **New Files:**\n    -   `src/data_sources/mod.rs` containing the `pub trait DataSource`.\n    -   `src/data_sources/legacy_source.rs` containing the `struct LegacyStreamSource` and its `impl DataSource` block.\n\n-   **Modified Files:**\n    -   `src/module_41.rs`: The main orchestrator is modified to accept `Box<dyn DataSource>` and calls its `stream` method. The original, direct API call logic is removed.\n    -   `src/module_79.rs`: The functions that were moved to `LegacyStreamSource` are removed. The file is now smaller and contains only related helper functions or types, if any.\n    -   `src/module_1.rs` (or equivalent entrypoint): The code is updated to instantiate `LegacyStreamSource`, box it, and pass it to the orchestrator from `module_41`.\n    -   `tests/test_main.txt`: Tests are updated to inject a `DataSource` (either the real one or a mock).\n    -   `tests/test_utils.txt`: May contain a new `MockDataSource` struct for testing purposes.\n\n-   **Verification:** The project must compile successfully (`cargo check` or `cargo build`). All tests must pass (`cargo test`).",
      "evaluation_criteria": [
        "**Correctness of Abstraction:** The `DataSource` trait is defined correctly in `src/data_sources/mod.rs` with the specified asynchronous `stream` method signature.",
        "**Successful Code Migration:** The logic from `module_79` and `module_41` is correctly moved into the new `LegacyStreamSource` struct, which successfully implements the `DataSource` trait.",
        "**Decoupling of Processor:** `module_41` is successfully refactored to depend on the `DataSource` trait via a trait object (`Box<dyn DataSource>`), not the concrete implementation.",
        "**Application Integrity:** The main application entrypoint correctly instantiates and injects the new `LegacyStreamSource` implementation, preserving the application's overall functionality.",
        "**Test Adaptation:** Tests are properly updated to work with the new abstraction, ideally by using a mock `DataSource` to test the orchestrator's logic independently.",
        "**Code Cleanliness:** Redundant, now-uncalled code from the original modules (`module_79`, `module_41`) has been removed.",
        "**Compilation and Testing:** The final refactored code compiles without errors and all tests pass.",
        "**Idiomatic Rust:** The solution uses Rust's features idiomatically, including traits, async/await, streams, and error handling (`Result`)."
      ],
      "language": "rust",
      "score": 0.8986
    },
    {
      "id": "csharp_web_blog_expert_076_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis and Extension for New Content Syndication Feature",
      "context_length": 871394,
      "files_count": 81,
      "task_prompt": "You are tasked with planning the integration of a new Content Syndication feature. Your analysis and proposal must strictly adhere to the project's existing Hexagonal Architecture.\n\nYour response should be a detailed architectural plan in two parts:\n\n**Part 1: Architectural Analysis**\nBased on the provided file list and project context, analyze the existing codebase and identify the key components of the Hexagonal Architecture. Specifically, identify and list the file(s) that represent:\n1.  Core Domain Models (e.g., the `Post` or `User` entity).\n2.  Application Services / Use Cases (e.g., a service for publishing a post).\n3.  Primary Adapters (driving the application, e.g., a Web API controller).\n4.  Secondary Adapters (driven by the application, e.g., a database repository implementation or a payment gateway client).\n5.  Ports (interfaces defined in the application core that secondary adapters implement).\n\n**Part 2: Implementation Plan**\nPropose a detailed, step-by-step plan to add the Content Syndication feature. The feature should automatically push a post's content to an external endpoint (`https://api.content-aggregator.com/v1/posts`) via an HTTP POST request upon successful publication. Your plan must specify:\n1.  **New Components:** What new files/classes/interfaces should be created? Describe their purpose and where they fit within the Hexagonal Architecture.\n2.  **Existing Modifications:** Which existing file(s) must be modified? Provide a clear, high-level description of the required changes for each file.\n3.  **Configuration:** How and where should the external API key and endpoint URL be configured? Reference the appropriate project file.\n4.  **Dependency Flow:** Briefly explain how your proposed solution maintains the correct dependency flow (i.e., infrastructure depending on the core, not the other way around).",
      "ground_truth": "**Part 1: Architectural Analysis (Example Mapping)**\n*   **Domain Models:** `module_3.txt` (likely a `Post` entity), `module_5.txt` (likely an `Author` entity). These files contain POCOs with business properties and methods, but no dependencies on external frameworks.\n*   **Application Services:** `module_42.txt` (likely `PublishPostService.cs`). This file contains a class that orchestrates the publishing process, depending on interfaces (ports) like a repository and a payment processor.\n*   **Primary Adapters:** `module_25.txt` (likely a `PostsController.cs`). This file defines API endpoints, takes HTTP requests, and calls application services.\n*   **Secondary Adapters:** `module_65.txt` (likely `SqlPostRepository.cs`, implementing a repository port), `module_67.txt` (likely a `StripePaymentAdapter.cs`, implementing a payment port), and `module_20.txt` (likely a `SerilogLoggingAdapter.cs`). These files contain concrete implementations with external dependencies.\n*   **Ports:** `module_15.txt` (likely `IPostRepository.cs`), `module_16.txt` (likely `IPaymentGateway.cs`). These files define the interfaces that the application core uses to communicate with outside systems.\n\n**Part 2: Implementation Plan**\n1.  **New Components:**\n    *   **New Port:** Create a new interface `IContentSyndicator` inside the Application Core's port directory (e.g., logically alongside `module_15.txt`). It will define a single method: `Task SyndicatePostAsync(Post post);`.\n    *   **New Adapter:** Create a new class `HttpContentSyndicatorAdapter` in the infrastructure layer (e.g., logically alongside `module_65.txt`). This class will implement `IContentSyndicator`. Its constructor will take `HttpClient` and `IConfiguration`. The `SyndicatePostAsync` method will build and send the HTTP POST request to the external API.\n\n2.  **Existing Modifications:**\n    *   **Modify `module_42.txt` (`PublishPostService`):** Inject the `IContentSyndicator` interface into its constructor. In the `ExecuteAsync` (or similar) method, after the logic that saves the post to the repository, add a call to `_contentSyndicator.SyndicatePostAsync(post);`.\n    *   **Modify Dependency Injection Configuration:** The DI setup (likely referenced in `config.txt` or a startup class) must be updated to register the new service: `services.AddScoped<IContentSyndicator, HttpContentSyndicatorAdapter>();` and configure the `HttpClient` for it.\n\n3.  **Configuration:**\n    *   Add the following keys to `src/config.txt` (or the file it points to, like `appsettings.json`):\n        ```json\n        \"ContentSyndication\": {\n          \"ApiEndpoint\": \"https://api.content-aggregator.com/v1/posts\",\n          \"ApiKey\": \"YOUR_API_KEY_HERE\"\n        }\n        ```\n\n4.  **Dependency Flow:**\n    *   This solution maintains the correct dependency flow. The `PublishPostService` (Application Core) depends on the `IContentSyndicator` interface (Port, also in the Core). It has no knowledge of the `HttpContentSyndicatorAdapter`. The `HttpContentSyndicatorAdapter` (Infrastructure) depends on the `IContentSyndicator` interface (Core), thus the dependency arrow points inward, correctly adhering to the Hexagonal Architecture's dependency rule.",
      "evaluation_criteria": [
        "**Architectural Pattern Identification:** Correctly identifies the Hexagonal (Ports and Adapters) architecture as the guiding pattern.",
        "**Component Mapping Accuracy:** Accurately maps the obfuscated file names to their corresponding architectural roles (Domain, Service, Port, Adapter). Partial credit for correctly identifying the roles even with incorrect file mappings.",
        "**Adherence to Dependency Rule:** The proposed solution must ensure the application core only depends on abstractions (ports), not on concrete infrastructure adapters.",
        "**Correctness of Plan:** The plan must correctly identify the need for a new port (interface) and a new adapter (implementation class).",
        "**Modification Logic:** Correctly identifies the application service as the place to orchestrate the new step and proposes modifying it to use the new port.",
        "**Configuration Management:** Correctly identifies `src/config.txt` as the source for configuration and proposes a logical structure for the new settings.",
        "**Completeness:** The plan addresses all aspects of the task: new components, modifications, configuration, and the reasoning behind the dependency flow."
      ],
      "language": "csharp",
      "score": 0.897
    },
    {
      "id": "csharp_mobile_game_expert_024_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Proposal for Real-Time 'Hardcore Mode' Sync",
      "context_length": 866981,
      "files_count": 81,
      "task_prompt": "As the lead architect, you are tasked with designing a solution to support a real-time 'Hardcore Mode'. Your proposal must not be a full implementation, but rather a high-level architectural plan. Your plan should be presented as a clear, structured analysis that addresses the following points:\n\n1.  **Analysis of Conflict**: Briefly explain why the 'Hardcore Mode' requirement conflicts with the current architecture, referencing `ADR-003-Offline-Sync-Strategy.md` and relevant source code files.\n\n2.  **Proposed Architectural Pattern**: Identify and justify the primary design pattern(s) (e.g., Strategy, Decorator, etc.) you would use to implement the conditional sync logic. Explain why your chosen pattern is superior to a naive approach (e.g., adding `if (isHardcore)` checks in every command handler).\n\n3.  **Component Placement and Responsibility**: Detail where new classes and interfaces should be created within the existing project structure (`Core`, `Application`, `Infrastructure`). List the key existing classes that will require modification.\n\n4.  **Decision Logic**: Describe how the system will determine which synchronization path (real-time vs. offline-queued) to use for a given player action. Pinpoint the specific class or component that will be responsible for making this decision.",
      "ground_truth": {
        "analysis_of_conflict": "The core conflict is between the immediate, transactional nature of 'Hardcore Mode' and the asynchronous, resilient nature of the offline-first approach detailed in `ADR-003-Offline-Sync-Strategy.md`. The current architecture, visible in `SyncPlayerActionsCommand.cs`, is designed to batch local changes and send them to the server, which is fundamentally different from the required real-time, per-action synchronization.",
        "proposed_architectural_pattern": {
          "pattern": "Strategy Pattern",
          "justification": "The Strategy pattern is ideal because it allows the synchronization algorithm to be selected at runtime without coupling the command handlers (the clients) to the specific implementation details of how syncing is performed. It cleanly separates the 'what' (e.g., create a company) from the 'how' (sync it immediately vs. queue it). This is highly scalable and adheres to the Open/Closed Principle, as new sync modes could be added in the future without modifying the command handlers."
        },
        "component_placement": {
          "new_files": [
            {
              "path": "TycoonVerse/src/TycoonVerse.Application/Interfaces/Strategies/IActionSyncStrategy.cs",
              "description": "Defines the common interface for all synchronization strategies."
            },
            {
              "path": "TycoonVerse/src/TycoonVerse.Application/Strategies/OfflineQueuingStrategy.cs",
              "description": "The concrete implementation that encapsulates the existing logic of queuing actions locally."
            },
            {
              "path": "TycoonVerse/src/TycoonVerse.Infrastructure/Strategies/RealTimeSyncStrategy.cs",
              "description": "The concrete implementation for 'Hardcore Mode'. It belongs in Infrastructure because it will directly use Infrastructure components like `ApiClient.cs`."
            }
          ],
          "modified_files": [
            {
              "path": "TycoonVerse/src/TycoonVerse.Core/Entities/Player.cs",
              "change": "Add a new property, such as `public GameMode Mode { get; set; }`, to the Player entity."
            },
            {
              "path": "TycoonVerse/src/TycoonVerse.Application/Features/**/*.cs",
              "change": "All relevant command handlers (e.g., `CreateCompanyCommand`) will be modified to accept an `IActionSyncStrategy` via dependency injection and use it to process the action, replacing the direct queuing logic."
            },
            {
              "path": "TycoonVerse/src/TycoonVerse.Application/Common/ServiceLocator.cs",
              "change": "Or a new factory class. This component will be modified to include logic for resolving the correct `IActionSyncStrategy` implementation based on the current player's `GameMode`."
            }
          ]
        },
        "decision_logic": "The decision logic will be centralized in a factory or service resolution component, such as the `ServiceLocator` or a new `SyncStrategyFactory`. When a command handler is instantiated, this factory will inspect the currently authenticated player's `GameMode` property. Based on this value, it will inject either an instance of `OfflineQueuingStrategy` or `RealTimeSyncStrategy` into the command handler. The command handler itself remains agnostic to the decision, simply executing the `Sync()` method on the provided strategy interface."
      },
      "evaluation_criteria": [
        "**ADR Comprehension**: Assesses if the agent correctly identifies and explains the conflict with the architecture described in `ADR-003-Offline-Sync-Strategy.md`.",
        "**Architectural Pattern Selection**: Evaluates the agent's ability to choose and justify an appropriate, scalable design pattern (like Strategy) over a naive, brittle solution.",
        "**Layer Adherence**: Measures whether the proposed new components and modifications respect the project's established layered architecture (e.g., interfaces in Application, implementations using external services in Infrastructure).",
        "**Impact Analysis**: Checks if the agent accurately identifies the key existing classes and layers that will be impacted by the change.",
        "**Separation of Concerns**: Assesses if the proposed solution correctly separates the decision-making logic (the factory) from the action execution logic (the command handlers and strategies).",
        "**Clarity and Justification**: Evaluates the overall quality of the explanation, including the reasoning behind architectural choices."
      ],
      "language": "csharp",
      "score": 0.8959
    },
    {
      "id": "rust_api_microservice_expert_008_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Consolidate Disparate Error Handling into a Unified Application Error Type",
      "context_length": 1009480,
      "files_count": 82,
      "task_prompt": "Your task is to refactor the entire `LedgerLink Nexus` codebase to use a unified error handling mechanism.\n\n**Analysis & Discovery:**\n1.  Scan the project, particularly files like `src/module_43.txt`, `src/module_8.txt`, `src/module_52.txt`, and `src/module_71.txt`, to identify the different error handling patterns currently in use. Look for functions returning `Result<T, String>`, `Result<T, anyhow::Error>`, `Result<T, Box<dyn std::error::Error>>`, and custom error structs.\n\n**Implementation:**\n1.  Create a new file named `src/error.rs` to house the centralized error handling logic.\n2.  In `src/error.rs`, define a public enum named `AppError`. This enum should be capable of representing all major failure modes in the application, such as:\n    -   Database errors (wrapping an underlying `sqlx::Error`)\n    -   Caching errors (wrapping an `redis::RedisError`)\n    -   Invalid user input/validation errors.\n    -   Resource not found errors.\n    -   Authentication/Authorization failures.\n    -   Generic internal server errors.\n3.  Implement the `std::error::Error` and `std::fmt::Display` traits for `AppError`. Using a library like `thiserror` is highly recommended to reduce boilerplate.\n4.  Implement `From<T>` conversions for common error types like `sqlx::Error` and `redis::RedisError` to allow for clean, idiomatic error propagation with the `?` operator.\n5.  Since this is an API microservice, `AppError` must be convertible into an HTTP response. Implement the `axum::response::IntoResponse` trait for `AppError`. Each error variant should map to an appropriate HTTP status code (e.g., `ValidationError` -> 400, `Unauthorized` -> 401, `NotFound` -> 404, `DatabaseError` -> 500) and a consistent JSON body, like: `{\"error\": {\"type\": \"validation\", \"message\": \"...\"}}`.\n\n**Refactoring:**\n1.  Modify all relevant functions across all `src/module_*.txt` files to return `Result<T, AppError>` instead of their previous error types.\n2.  Replace the old error instantiation logic (e.g., `Err(\"Invalid ID\".to_string())`, `Err(anyhow::anyhow!(...))`) with the corresponding `AppError` variant (e.g., `Err(AppError::ValidationError(\"Invalid ID\".to_string()))`).\n3.  Add the necessary `use crate::error::AppError;` statements to all modified files.\n4.  Remove any now-unnecessary local error type definitions from the modules.\n\n**Verification:**\n1.  While you cannot run tests, describe how you would modify `tests/test_utils.txt` to assert that API endpoints now return the new, structured JSON error responses with the correct status codes.",
      "ground_truth": "The expected solution involves several key changes across the codebase:\n\n1.  **New File Creation:** A new file `src/error.rs` is created.\n\n2.  **Content of `src/error.rs`:**\n    ```rust\n    // src/error.rs\n    use axum::{\n        http::StatusCode,\n        response::{IntoResponse, Response},\n        Json,\n    };\n    use serde_json::json;\n    use thiserror::Error;\n\n    #[derive(Error, Debug)]\n    pub enum AppError {\n        #[error(\"invalid input: {0}\")]\n        ValidationError(String),\n\n        #[error(\"resource not found: {0}\")]\n        NotFound(String),\n\n        #[error(\"an internal server error occurred\")]\n        InternalServerError(#[from] anyhow::Error),\n\n        #[error(\"database query failed\")]\n        DatabaseQuery(#[from] sqlx::Error),\n\n        #[error(\"failed to access cache\")]\n        Cache(#[from] redis::RedisError),\n\n        #[error(\"unauthorized access\")]\n        Unauthorized,\n    }\n\n    impl IntoResponse for AppError {\n        fn into_response(self) -> Response {\n            let (status, error_type, error_message) = match self {\n                AppError::ValidationError(msg) => (StatusCode::BAD_REQUEST, \"validation\", msg),\n                AppError::NotFound(msg) => (StatusCode::NOT_FOUND, \"not_found\", msg),\n                AppError::Unauthorized => (StatusCode::UNAUTHORIZED, \"unauthorized\", \"Authentication required\".to_string()),\n                AppError::DatabaseQuery(e) => (StatusCode::INTERNAL_SERVER_ERROR, \"database_error\", e.to_string()),\n                AppError::Cache(e) => (StatusCode::INTERNAL_SERVER_ERROR, \"cache_error\", e.to_string()),\n                AppError::InternalServerError(e) => (StatusCode::INTERNAL_SERVER_ERROR, \"internal_error\", e.to_string()),\n            };\n\n            let body = Json(json!({\n                \"error\": {\n                    \"type\": error_type,\n                    \"message\": error_message,\n                }\n            }));\n\n            (status, body).into_response()\n        }\n    }\n    ```\n\n3.  **Modifications in `src/module_*.txt` files:**\n    -   **Before:** `pub async fn get_item(id: Uuid) -> Result<Item, String> { ... Err(\"not found\".to_string()) }`\n    -   **After:** `use crate::error::AppError; ... pub async fn get_item(id: Uuid) -> Result<Item, AppError> { ... Err(AppError::NotFound(\"Item not found\".to_string())) }`\n\n    -   **Before:** `pub fn process_data(data: &Data) -> Result<(), anyhow::Error> { ... let result = some_fallible_op()?; ... }`\n    -   **After:** `use crate::error::AppError; ... pub fn process_data(data: &Data) -> Result<(), AppError> { ... let result = some_fallible_op()?; ... }` (The `?` operator works because of the `From` impls).\n\n4.  **File-level changes:** Dozens of files in `src/` will have their function signatures updated and `use crate::error::AppError;` added. Old error types like `struct Module43Error;` will be deleted.",
      "evaluation_criteria": [
        "**Correctness of Implementation:** The agent must correctly create the `AppError` enum and implement the required traits (`Error`, `Display`, `Debug`, `IntoResponse`, `From`).",
        "**Completeness of Refactoring:** The agent should identify and refactor a vast majority of the disparate error handling sites across all provided modules.",
        "**Cross-File Consistency:** The new error handling pattern (`-> Result<T, AppError>`, `Err(AppError::Variant(...))`) must be applied uniformly across all modified files.",
        "**Code Centralization:** All new, shared error logic must be correctly placed in the new `src/error.rs` file, and old, redundant error types must be removed.",
        "**Idiomatic Rust:** The solution should use standard Rust idioms, such as leveraging `thiserror` and `From` traits to make the code clean and maintainable.",
        "**Non-Destructive Refactoring:** The agent must only change the error handling logic. Core business logic within the functions should remain untouched.",
        "**Test Awareness:** The agent should demonstrate an understanding of how the changes would impact the testing suite by describing the necessary updates to `tests/test_utils.txt`."
      ],
      "language": "rust",
      "score": 0.8959
    },
    {
      "id": "csharp_blockchain_defi_expert_070_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Propose Architectural Refactoring to Decouple Services with an External Message Broker",
      "context_length": 916981,
      "files_count": 77,
      "task_prompt": "You are a senior solutions architect tasked with de-risking the UtilityChain's inter-service communication model. Your analysis should result in a technical design proposal in Markdown format.\n\n1.  **Analyze the Current Architecture:** Examine the existing codebase and documentation (`adr/003-event-driven-module-communication.md`, `IEventBus.cs`, `InMemoryEventBus.cs`, etc.) to fully understand the current event-driven communication pattern.\n2.  **Identify Key Stakeholders:** Identify all the major services/modules within the `UtilityChainCoreSuite` that currently publish or subscribe to events using the `IEventBus`.\n3.  **Create a Technical Design Proposal:** Write a detailed Markdown report (`PROPOSAL.md`) that outlines a migration plan from the `InMemoryEventBus` to a persistent, external message broker (e.g., RabbitMQ, Kafka, or Azure Service Bus).\n\nYour proposal must specifically address the following critical points:\n\n*   **Affected Components:** List the specific services that will require modification and briefly describe the nature of the changes.\n*   **Asynchronicity and Data Consistency:** The current bus is synchronous. A truly asynchronous, external broker introduces challenges with transactional guarantees. How do you ensure that a state-modifying operation (e.g., processing a transaction) and the publication of its corresponding event are atomic? Propose a specific design pattern (e.g., Transactional Outbox Pattern) to maintain data consistency across service boundaries.\n*   **Configuration and Dependency Management:** Detail the necessary changes to configuration files (`appsettings.json`) and the application's startup and dependency injection logic (`Startup.cs`, `Program.cs`) to integrate the new message broker client.\n*   **Resilience and Error Handling:** What new failure modes does an external broker introduce (e.g., network partitions, broker downtime, poison messages)? Propose a robust error-handling strategy, including mechanisms like dead-letter queues (DLQs) and retry policies.\n*   **Architectural Trade-offs:** Conclude with a summary of the pros (e.g., scalability, resilience, decoupling) and cons (e.g., increased operational complexity, latency, new infrastructure dependency) of your proposed solution.",
      "ground_truth": "A successful solution will be a well-structured Markdown document (`PROPOSAL.md`) that contains the following key insights:\n\n*   **Affected Components:** `TransactionProcessor`, `StakingService`, `GovernanceService`, `P2PService`, and `ApiGateway` are the primary stakeholders. The proposal should note that every component using `IEventBus` is affected.\n*   **Data Consistency Solution:** The proposal MUST identify the loss of atomicity as the primary challenge. The gold-standard solution presented should be the **Transactional Outbox Pattern**. A naive solution that simply replaces the bus implementation without addressing this consistency gap is incorrect.\n*   **Configuration Example (`appsettings.json`):**\n    ```json\n    \"MessageBroker\": {\n      \"HostName\": \"rabbitmq.utilitychain.net\",\n      \"UserName\": \"user\",\n      \"Password\": \"password\",\n      \"ExchangeName\": \"utilitychain_events\"\n    }\n    ```\n*   **DI Changes (`Startup.cs`):** The report must show the change from `services.AddSingleton<IEventBus, InMemoryEventBus>();` to something like `services.AddSingleton<IEventBus, RabbitMqEventBus>();` and also show the registration of the outbox poller service: `services.AddHostedService<OutboxProcessorService>();`.\n*   **Resilience Strategy:** The proposal must explicitly recommend using **Dead-Letter Queues (DLQs)** to handle messages that repeatedly fail processing, preventing poison messages from halting the system. It should also recommend **configurable retry policies with exponential backoff** for transient errors like network hiccups when connecting to the broker.\n*   **Trade-off Analysis:** The analysis must be balanced. It should praise the move for enabling scalability, fault isolation, and independent deployments. Critically, it must also acknowledge the costs: increased operational overhead (managing a message broker cluster), introduction of network latency for inter-service calls, and the added complexity of ensuring eventual consistency.",
      "evaluation_criteria": [
        "correctly_identifies_event_bus_usage: Accurately lists the key services that publish and subscribe to the IEventBus.",
        "understands_current_architecture_limitations: Clearly articulates why the synchronous, in-memory event bus is a scalability and resilience risk.",
        "addresses_asynchronous_consistency_challenge: Identifies the atomicity problem and proposes a robust pattern like the Transactional Outbox to solve it. This is a critical criterion.",
        "proposes_concrete_config_and_startup_changes: Provides specific, correct examples of changes required in `appsettings.json` and `Startup.cs`.",
        "analyzes_new_failure_modes_and_resilience: Demonstrates foresight by identifying new risks (broker failure, poison messages) and proposing standard solutions (DLQs, retries).",
        "evaluates_architectural_tradeoffs: Presents a balanced view of the pros and cons of the proposed architectural change.",
        "report_clarity_and_structure: The final Markdown proposal is well-organized, clearly written, and follows the requested structure."
      ],
      "language": "csharp",
      "score": 0.8952
    },
    {
      "id": "java_web_ecommerce_expert_036_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for Real-time Warehouse Management System (WMS) Integration",
      "context_length": 821130,
      "files_count": 84,
      "task_prompt": "Analyze the existing SprintCart Pro architecture and produce a detailed technical plan for integrating a third-party Warehouse Management System (WMS). Your plan should NOT include writing the final implementation code. Instead, it must focus on identifying the necessary architectural changes, new components, and data flows to support two key use cases:\n\n1.  **Outbound Fulfillment Request:** When an order is successfully paid and confirmed, SprintCart Pro must send the relevant fulfillment details to the WMS API.\n2.  **Inbound Stock Update:** SprintCart Pro must expose a secure endpoint that the WMS can call to push real-time stock level updates for specific product SKUs.\n\nYour response should be a detailed markdown document that addresses the following points:\n\n1.  **Module and Dependency Strategy:** Which new Maven module(s), if any, should be created? Which existing modules will they depend on, and which will need to depend on new components?\n2.  **Ports (Interfaces) Definition:** Identify the specific `port` interfaces required in the `sprintcart-pro-domain` module to abstract the WMS interactions. Define the new interfaces and their method signatures.\n3.  **Adapter Implementation Plan:** Describe the key classes for the new adapter that will implement the ports. How will it handle communication (e.g., REST client for outbound, new controller for inbound)?\n4.  **Application Layer Orchestration:** Explain how the `sprintcart-pro-application` services (e.g., `OrderService`, `FulfillmentService`, `CatalogService`) will be modified to orchestrate these new workflows. Describe the sequence of calls.\n5.  **Data Flow Diagram:** Detail the end-to-end data flow for the 'Inbound Stock Update' scenario, starting from the external WMS call and ending with the database update. List the primary components involved in each step (e.g., `WmsAdapter.WmsController` -> `ApplicationService` -> ...).\n6.  **Architectural Justification:** Justify your design by referencing at least two existing architectural documents (`001-hexagonal-architecture.md`, C4 diagrams, etc.) to demonstrate how your plan aligns with the project's established principles.",
      "ground_truth": "### WMS Integration Technical Plan\n\n#### 1. Module and Dependency Strategy\n\nA new Maven module `sprintcart-pro-adapters/wms-adapter` will be created. \n-   **Dependencies:** `wms-adapter` will depend on `sprintcart-pro-application` and `sprintcart-pro-domain` to access the port interfaces and application services it needs to call.\n-   **Configuration:** The main `sprintcart-pro-app` module will need to add `wms-adapter` as a dependency in its `pom.xml` so that the Spring Boot application can discover and wire the new beans (Controller, Adapter implementation).\n\n#### 2. Ports (Interfaces) Definition\n\nTwo new ports will be created in the `sprintcart-pro-domain` module:\n\n-   **Outbound Port:** For sending data to the WMS.\n    -   **File:** `sprintcart-pro-domain/src/main/java/com/sprintcart/domain/ports/out/wms/WmsPort.java`\n    -   **Interface:** \n        ```java\n        package com.sprintcart.domain.ports.out.wms;\n\n        import com.sprintcart.domain.model.order.Order;\n\n        public interface WmsPort {\n            /**\n             * Submits a fulfillment request to the external WMS.\n             * @param order The confirmed order to be fulfilled.\n             */\n            void requestFulfillment(Order order);\n        }\n        ```\n-   **Inbound Port (Use Case):** For updating stock from an external system.\n    -   **File:** `sprintcart-pro-domain/src/main/java/com/sprintcart/domain/ports/in/inventory/UpdateStockUseCase.java`\n    -   **Interface:**\n        ```java\n        package com.sprintcart.domain.ports.in.inventory;\n\n        public interface UpdateStockUseCase {\n            /**\n             * Updates the stock level for a given product SKU.\n             * @param sku The product SKU to update.\n             * @param quantityChange The change in quantity (can be positive or negative).\n             */\n            void updateStockLevel(String sku, int quantityChange);\n        }\n        ```\n\n#### 3. Adapter Implementation Plan\n\nThe `wms-adapter` module will contain:\n\n1.  **`WmsAdapter.java`:** Implements `WmsPort`. It will use a `WebClient` or `RestTemplate` to make an outbound POST request to the WMS API's fulfillment endpoint. It will map the domain `Order` model to a WMS-specific DTO before sending.\n2.  **`WmsController.java`:** A new `@RestController` that exposes a secure endpoint (e.g., `POST /api/internal/wms/stock-update`). It will accept a WMS-specific DTO, validate it, and then call the `UpdateStockUseCase` port.\n3.  **`WmsDto.java`:** A record or class representing the stock update payload from the WMS.\n\n#### 4. Application Layer Orchestration\n\n-   **Outbound (Fulfillment):** The `FulfillmentService` in the `application` module will be injected with `WmsPort`. A new method, `processFulfillment(Order order)`, will be added. This method will be called (e.g., by the `DomainEventProcessor` listening for `OrderPlacedEvent`) and will invoke `wmsPort.requestFulfillment(order)`.\n-   **Inbound (Stock Update):** The `CatalogService` in the `application` module will implement the `UpdateStockUseCase` interface. The `updateStockLevel` method will use the existing `ProductRepositoryPort` to find the product by SKU, update its stock quantity, and save it back to the database.\n\n#### 5. Data Flow Diagram (Inbound Stock Update)\n\n`External WMS` -> `[Network]` -> `1. WmsController` (in `wms-adapter`) -> `2. UpdateStockUseCase` (port in `domain`) -> `3. CatalogService` (implementation in `application`) -> `4. ProductRepositoryPort` (port in `domain`) -> `5. ProductRepositoryAdapter` (implementation in `persistence-adapter`) -> `6. Database`\n\n#### 6. Architectural Justification\n\nThis plan strictly adheres to the principles outlined in **`docs/architecture/adr/001-hexagonal-architecture.md`**. \n\n1.  **Dependency Inversion:** The `domain` core defines the `WmsPort` and `UpdateStockUseCase` interfaces but has no knowledge of the WMS itself. The `wms-adapter`, an infrastructure detail, depends on the domain, not the other way around. This follows the Dependency Rule.\n2.  **Separation of Concerns:** The new `wms-adapter` encapsulates all details of the WMS communication (endpoints, DTOs, client logic), isolating it from the core business logic. This is consistent with the visual separation shown in the **`docs/architecture/c4-model/03_component_diagram.puml`**, where adapters (like Persistence, Payment) are on the periphery of the system.\n",
      "evaluation_criteria": [
        "**Adherence to Hexagonal Architecture:** Did the agent correctly place new logic in the domain, application, and a new adapter layer, respecting the dependency rule?",
        "**Correct Component Identification:** Did the agent correctly identify the need for a new adapter module, new inbound/outbound ports, and a new controller?",
        "**Data Flow Accuracy:** Was the agent able to accurately trace the sequence of component interactions for both inbound and outbound scenarios?",
        "**Architectural Justification:** Did the agent successfully reference and apply the principles from the provided architectural documentation (`.md`, `.puml`)?",
        "**Distinction between Ports:** Did the agent correctly identify that the inbound interaction is a 'Use Case' port while the outbound interaction is a generic 'outbound' port (like a repository)?",
        "**Completeness and Detail:** Is the plan comprehensive, addressing all parts of the prompt with sufficient technical detail (e.g., method signatures, module dependencies)?"
      ],
      "language": "java",
      "score": 0.8942
    },
    {
      "id": "javascript_ml_nlp_expert_053_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Diagnose and Propose a Refactor for the Asynchronous Data Processing Pipeline Bottleneck",
      "context_length": 864944,
      "files_count": 80,
      "task_prompt": "You are a senior software architect tasked with resolving a critical performance issue in the AgoraPulse platform. Your analysis must go beyond superficial code fixes and address the core architectural flaw.\n\n1.  **Map the Data Flow:** Analyze the provided source code to identify and map the sequence of modules that constitute the primary data processing pipeline. Start from data ingestion and trace the flow up to the point where data is dispatched for model inference. List the modules in order of execution.\n\n2.  **Identify the Communication Pattern:** Describe the architectural pattern used for passing data and control between the modules in this pipeline. Is it event-driven, a message queue, direct remote procedure calls (RPCs), or a chain of direct function calls? Provide your reasoning by referencing how modules interact.\n\n3.  **Pinpoint the Bottleneck:** Based on your analysis of the data flow and communication pattern, identify the specific architectural bottleneck that is causing the processing latency. Explain *why* this design is problematic, especially under high load. Your justification must be specific and reference the likely behavior of the code (e.g., blocking I/O, CPU-intensive synchronous operations, inefficient data handling).\n\n4.  **Propose a Refactoring Plan:** Outline a high-level architectural refactoring plan to eliminate the bottleneck and improve the system's scalability and resilience. Your plan should specify:\n    *   The new architectural pattern to be introduced (e.g., Pub/Sub, Message Queue).\n    *   Which specific modules will need to be modified.\n    *   How the interaction between these modules will change.",
      "ground_truth": "The agent is expected to uncover the following architectural details:\n\n*   **Data Flow Map:** The core pipeline consists of a chain of modules: `module_14` (Data Ingestor) -> `module_38` (Text Sanitizer & Normalizer) -> `module_46` (Named Entity Recognition) -> `module_50` (Feature Vectorizer) -> `module_60` (Model Serving Dispatcher).\n\n*   **Communication Pattern:** The system uses a **tightly-coupled, pseudo-asynchronous chain of direct function calls**. Although `async/await` is used, `module_14` directly invokes a method on `module_38`, which in turn directly invokes a method on `module_46`, and so on. There is no message queue or event bus decoupling these stages, making the entire pipeline behave like a single, long-running synchronous operation from the perspective of a single data item.\n\n*   **The Bottleneck:** The primary bottleneck is the interaction between `module_46` and `module_50`. `module_46`'s entity recognition process produces a large, complex object with nested arrays and metadata. To pass this data to `module_50`, the code performs a `JSON.stringify()` on this large object. `module_50`'s first step is to immediately call `JSON.parse()` on the received string. These `JSON.stringify/parse` operations are **synchronous and CPU-intensive**. Under high load, the Node.js event loop becomes blocked by these operations, preventing it from handling other incoming requests and causing a system-wide latency pile-up. The tight coupling ensures that `module_50` cannot start its work until `module_46` has fully completed its synchronous serialization, creating a major chokepoint.\n\n*   **Refactoring Plan:** The correct proposal is to **decouple the pipeline stages with a message queue** (e.g., RabbitMQ, Kafka, or Redis Streams, which may be listed as a dependency in `package.json`).\n    *   **New Pattern:** A Producer/Consumer or Pub/Sub pattern.\n    *   **Modules to Change:** `module_14`, `module_38`, `module_46`, `module_50`, and `module_60` must all be refactored.\n    *   **New Interaction:** `module_14` (the producer) would publish raw data to a `raw_posts` queue/topic. `module_38` would subscribe to `raw_posts`, process the data, and publish its result to a `sanitized_posts` topic. `module_46` would subscribe to that, and so on. This allows each stage to scale independently and removes the synchronous blocking call chain. It also potentially obviates the need for the costly stringify/parse step if the message broker's protocol is more efficient for passing structured data.",
      "evaluation_criteria": [
        "**Pipeline Identification (25%):** Correctly identifies the sequence of key modules (`14`, `38`, `46`, `50`, `60`) involved in the data processing pipeline.",
        "**Architectural Pattern Analysis (20%):** Accurately describes the communication mechanism as a tightly-coupled chain of direct calls, correctly noting the misuse of asynchronicity.",
        "**Bottleneck Pinpointing & Justification (30%):** Correctly identifies the synchronous `JSON.stringify`/`parse` operations between `module_46` and `module_50` as the specific, CPU-bound bottleneck and clearly explains why it impacts performance under load.",
        "**Refactoring Proposal Quality (25%):** Proposes a viable, architecturally sound solution involving a message queue to decouple the components, and correctly identifies the modules that need modification."
      ],
      "language": "javascript",
      "score": 0.892
    },
    {
      "id": "python_game_engine_expert_032_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Abstract the Physics Engine for Pluggability",
      "context_length": 1062870,
      "files_count": 76,
      "task_prompt": "Refactor the physics engine to be pluggable. You must introduce a new abstraction layer and update all dependent code to use this new interface. The functionality of the engine must remain unchanged.\n\n**Detailed Requirements:**\n\n1.  **Create an Interface File:** In the `ledgerquest/engine/physics/` directory, create a new file named `interface.py`.\n\n2.  **Define Abstract Classes:** In the new `interface.py` file, define an abstract base class (ABC) named `AbstractPhysicsSimulator`. This ABC should define the public contract for any physics simulator in the engine. Identify the core methods from the existing `ledgerquest/engine/physics/simulator.py` (like `step`, `add_body`, `remove_body`, etc.) and declare them as abstract methods in the new interface.\n\n3.  **Implement the Interface:** Modify the concrete `Simulator` class in `ledgerquest/engine/physics/simulator.py` so that it inherits from `AbstractPhysicsSimulator` and correctly implements the defined interface.\n\n4.  **Decouple the Game Loop:** The primary consumer of the physics engine is the `PhysicsUpdater` service. Modify `ledgerquest/services/game_loop/physics_updater.py` to depend on the `AbstractPhysicsSimulator` interface, not the concrete `Simulator` class. You will need to adjust how the `PhysicsUpdater` service is initialized and how it accesses the physics simulator instance to use dependency injection.\n\n5.  **Update Component Interactions:** Review `ledgerquest/engine/physics/components.py`. Ensure that any interactions between these components and the simulator are compatible with the new abstraction. The goal is that other engine systems should not need to know about the concrete physics implementation details.\n\n6.  **Maintain Functionality:** The refactoring must not break any existing functionality. All related tests, especially those in `tests/unit/engine/physics/test_simulator.py`, must pass after your changes.",
      "ground_truth": "The solution involves changes across several files, demonstrating a successful decoupling.\n\n-   **`ledgerquest/engine/physics/interface.py` (New File):**\n    ```python\n    from abc import ABC, abstractmethod\n    from typing import List\n    from ledgerquest.engine.ecs.entity import Entity\n\n    class AbstractPhysicsSimulator(ABC):\n        @abstractmethod\n        def add_body(self, entity: Entity):\n            ...\n\n        @abstractmethod\n        def remove_body(self, entity_id: int):\n            ...\n\n        @abstractmethod\n        def step(self, delta_time: float):\n            ...\n\n        @abstractmethod\n        def get_collisions(self) -> List[tuple[Entity, Entity]]:\n            ...\n    ```\n\n-   **`ledgerquest/engine/physics/simulator.py` (Modified):**\n    -   `from .interface import AbstractPhysicsSimulator` is added.\n    -   The class signature is changed to `class Simulator(AbstractPhysicsSimulator):`.\n    -   Methods like `step`, `add_body`, etc., now implement the abstract methods from the interface.\n\n-   **`ledgerquest/services/game_loop/physics_updater.py` (Modified):**\n    -   The import `from ledgerquest.engine.physics.simulator import Simulator` is **removed**.\n    -   A new import is added: `from ledgerquest.engine.physics.interface import AbstractPhysicsSimulator`.\n    -   The `__init__` method is changed to accept the simulator via dependency injection: `def __init__(self, ecs_registry, physics_simulator: AbstractPhysicsSimulator):`\n    -   All internal uses of `self.physics_simulator` now correctly call methods on the abstract type.\n\n-   **`tests/unit/engine/physics/test_simulator.py` (Modified):**\n    -   No functional changes to tests are expected. All tests should pass, verifying that the concrete implementation's logic is sound.\n\n-   **Service Initialization (e.g., `ledgerquest/services/__init__.py`) (Modified):**\n    -   The code that instantiates `PhysicsUpdater` is updated to first create a concrete `Simulator` and then pass it into the `PhysicsUpdater`'s constructor. This demonstrates the principle of dependency injection.",
      "evaluation_criteria": [
        "**Correctness of Abstraction:** Was a new `interface.py` file created with a proper `AbstractPhysicsSimulator` ABC using Python's `abc` module?",
        "**Implementation of Interface:** Does the `Simulator` class in `simulator.py` correctly inherit from and implement the `AbstractPhysicsSimulator` interface?",
        "**Decoupling of Consumer:** Is the `PhysicsUpdater` service in `physics_updater.py` fully decoupled from the concrete `Simulator` class? (i.e., it imports and type-hints against `AbstractPhysicsSimulator`).",
        "**Dependency Injection:** Was the instantiation of `Simulator` moved out of the `PhysicsUpdater` and injected into it at a higher level of the application?",
        "**Functional Equivalence:** Does the refactored code pass all existing unit tests in `tests/unit/engine/physics/test_simulator.py` without modification to the test logic?",
        "**Code Cohesion:** Are the changes localized to the relevant modules (physics, game loop services, and their initializers) without creating unnecessary side-effects in unrelated files?",
        "**Code Quality:** Is the new code clean, readable, and does it follow standard Python conventions?"
      ],
      "language": "python",
      "score": 0.8894
    },
    {
      "id": "csharp_data_warehouse_expert_012_bug_investigation_expert_01",
      "task_category": "bug_investigation",
      "difficulty": "expert",
      "title": "Intermittent Data Corruption in High-Throughput Ingestion Pipeline",
      "context_length": 1113221,
      "files_count": 84,
      "task_prompt": "An urgent issue has been escalated. Our monitoring system has detected a spike in `System.Runtime.Serialization.SerializationException: End of Stream encountered before parsing was completed.` errors originating from the `DataRecordProcessor` class within `src/module_23.txt`. \n\nThe exceptions only occur under heavy, parallel-processed workloads. The corrupted data payloads appear to be truncated or contain jumbled byte sequences from different data records. Your task is to perform a root cause analysis and resolve the issue.\n\n**Objectives:**\n1.  **Identify the root cause** of the data corruption. The exception in `module_23.txt` is a symptom, not the cause.\n2.  **Pinpoint the exact code location(s)** in the codebase responsible for the bug. You will likely need to investigate the upstream data serialization and dispatching logic.\n3.  **Provide a precise, production-ready code modification** to fix the underlying issue permanently.",
      "ground_truth": {
        "explanation": "The root cause is a race condition in the `BufferManager` class defined in `src/utils.txt`. The `ParallelEventDispatcher` in `src/module_65.txt` uses `Parallel.ForEach` to serialize multiple events at once. Each parallel thread requests a reusable `MemoryStream` from the `BufferManager`. However, the `BufferManager` uses a standard `System.Collections.Generic.Queue<T>`, which is not thread-safe. During high load, multiple threads dequeue and enqueue streams concurrently, corrupting the internal state of the queue and causing threads to receive streams that are either still in use or have been improperly reset. This results in jumbled or truncated byte arrays being sent to the `DataRecordProcessor` in `src/module_23.txt`, which then correctly throws a `SerializationException`.",
        "file_with_bug": "src/utils.txt",
        "buggy_code": [
          "// Located in src/utils.txt inside the BufferManager class",
          "private static readonly Queue<MemoryStream> _streamPool = new Queue<MemoryStream>();",
          "",
          "public static MemoryStream GetStream() {",
          "    if (_streamPool.Count > 0) {",
          "        MemoryStream stream = _streamPool.Dequeue();",
          "        stream.SetLength(0); // Reset for reuse",
          "        return stream;",
          "    }",
          "    return new MemoryStream();",
          "}",
          "",
          "public static void ReturnStream(MemoryStream stream) {",
          "    _streamPool.Enqueue(stream);",
          "}"
        ],
        "file_to_fix": "src/utils.txt",
        "fixed_code": [
          "// Located in src/utils.txt inside the BufferManager class",
          "// SOLUTION 1: Using a lock for thread safety",
          "private static readonly Queue<MemoryStream> _streamPool = new Queue<MemoryStream>();",
          "private static readonly object _poolLock = new object();",
          "",
          "public static MemoryStream GetStream() {",
          "    lock (_poolLock) {",
          "        if (_streamPool.Count > 0) {",
          "            MemoryStream stream = _streamPool.Dequeue();",
          "            stream.SetLength(0); // Reset for reuse",
          "            return stream;",
          "        }",
          "    }",
          "    return new MemoryStream();",
          "}",
          "",
          "public static void ReturnStream(MemoryStream stream) {",
          "    lock (_poolLock) {",
          "        _streamPool.Enqueue(stream);",
          "    }",
          "}",
          "",
          "// SOLUTION 2 (Alternative/Better): Using a thread-safe collection",
          "// Replace the Queue<T> and lock with a ConcurrentQueue<T>",
          "private static readonly ConcurrentQueue<MemoryStream> _streamPool = new ConcurrentQueue<MemoryStream>();",
          "",
          "public static MemoryStream GetStream() {",
          "    if (_streamPool.TryDequeue(out MemoryStream stream)) {",
          "        stream.SetLength(0); // Reset for reuse",
          "        return stream;",
          "    }",
          "    return new MemoryStream();",
          "}",
          "",
          "public static void ReturnStream(MemoryStream stream) {",
          "    _streamPool.Enqueue(stream);",
          "}"
        ]
      },
      "evaluation_criteria": [
        "**Root Cause Identification (40%):** Was the agent able to correctly identify the race condition in the `BufferManager`'s non-thread-safe queue as the root cause, instead of incorrectly blaming the deserialization logic in `module_23.txt` or the parallel loop in `module_65.txt`?",
        "**Code Localization (30%):** Did the agent successfully pinpoint the buggy `GetStream`/`ReturnStream` methods in `src/utils.txt` as the precise location of the defect?",
        "**Solution Correctness (20%):** Was the proposed code fix correct? Does it properly use a `lock` or a `ConcurrentQueue` to ensure thread safety and resolve the race condition without introducing deadlocks?",
        "**Explanation Quality (10%):** Did the agent provide a clear and concise explanation of *why* the bug occurs (i.e., multiple threads accessing a shared, non-thread-safe resource) and how the proposed solution remedies the problem?"
      ],
      "language": "csharp",
      "score": 0.8893
    },
    {
      "id": "cpp_system_security_expert_064_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Bottleneck Analysis in a Microservices-based Security Suite",
      "context_length": 852557,
      "files_count": 80,
      "task_prompt": "Your task is to act as a principal engineer and perform an architectural analysis of the FortiLedger360 system to diagnose the reported performance issue.\n\n1.  **Trace the Lifecycle**: Detail the end-to-end flow of an on-demand security scan request, starting from its entry point at the API Gateway through to the service responsible for execution.\n2.  **Identify Architectural Interactions**: Explain how the processing of scans for 'Continuous Scan' tenants could architecturally interfere with and cause latency for 'Pay-As-You-Go' tenants' on-demand requests.\n3.  **Pinpoint the Weakness**: Identify the key components (services, libraries, communication patterns) involved in this interaction and pinpoint the most likely architectural weakness or design flaw that leads to this performance degradation. \n4.  **Provide Evidence**: Substantiate your analysis by referencing specific files (e.g., source code, configuration, or documentation) that support your conclusions.\n\nYou are not required to write or modify any code. Your final output should be a detailed architectural analysis in markdown format.",
      "ground_truth": "### Architectural Analysis of Scan Latency Issue\n\n**1. On-Demand Scan Request Lifecycle**\n\nThe lifecycle of an on-demand scan request is as follows:\n\n-   **API Gateway**: A request hits the public endpoint defined in `api/v1/openapi.yaml`. The `api_gateway` service (`src/services/api_gateway/server.cpp`) receives this request.\n-   **Orchestration**: The gateway doesn't perform the scan itself. It creates a `ScanCommand` (`lib/domain/commands/scan_command.h`) and passes it to the `CommandHandler` (`lib/orchestration/command_handler.cpp`).\n-   **Strategy & Dispatch**: The `CommandHandler` uses a factory or strategy selector to apply the correct logic based on the tenant's subscription type, such as the `PaygScanStrategy`. Following validation, it publishes a `ScanRequested` event to the system's event bus. This is consistent with the event-driven design outlined in `docs/architecture/adr/002-event-driven-architecture.md`.\n-   **Consumption & Execution**: The `scanner_svc` is the consumer of this event. Its `EventConsumer` (`lib/infrastructure/event_bus/event_consumer.cpp`) receives the message and forwards the job to the `ScannerEngine` (`src/services/scanner_svc/scanner_engine.cpp`).\n-   **Execution**: The `ScannerEngine` is responsible for performing the actual security scan.\n\n**2. Architectural Interference and Root Cause**\n\nThe primary architectural weakness is the **lack of Quality of Service (QoS) and workload isolation within the `scanner_svc`**. \n\n-   **Shared Resource Contention**: The `scanner_svc` handles scans for *all* tenant types. Both high-volume, low-urgency 'Continuous Scans' and low-volume, high-urgency on-demand 'PAYG' scans are processed by the same `ScannerEngine`.\n-   **Head-of-Line Blocking**: The implementation in `src/services/scanner_svc/scanner_engine.cpp` reveals that it uses a single, non-prioritized FIFO (First-In, First-Out) work queue to manage scan jobs. When 'Continuous Scan' tenants generate a large backlog of background tasks, new, time-sensitive on-demand requests from 'PAYG' tenants are simply added to the end of this queue. They are forced to wait for all preceding background tasks to complete.\n\n**3. Conclusion**\n\nThe reported latency is not due to a fault in a single component but a systemic architectural issue. The system's design fails to differentiate between different classes of work at the execution layer. This creates a \"noisy neighbor\" problem where the background workload of one class of tenants directly degrades the interactive performance for another. The core flaw is the naive, non-prioritized queuing mechanism in the `ScannerEngine`, making it the architectural bottleneck.",
      "evaluation_criteria": [
        "**Trace Accuracy**: Did the agent correctly trace the request path from the API Gateway, through the Command Handler and Event Bus, to the `scanner_svc`?",
        "**Component Identification**: Did the agent correctly identify the key components involved: `api_gateway`, `command_handler`, `event_bus`, `scanner_svc`, and `scanner_engine`?",
        "**Pattern Recognition**: Did the agent recognize the use of the Strategy pattern (`payg_scan_strategy`) and the event-driven communication model as described in the ADRs?",
        "**Root Cause Analysis**: Did the agent correctly identify the lack of a priority queue or QoS mechanism in the `ScannerEngine`'s work scheduler as the primary architectural bottleneck?",
        "**Evidence-Based Reasoning**: Did the agent support its claims by correctly referencing specific files (e.g., `scanner_engine.cpp`, `command_handler.cpp`, ADRs) to justify its conclusion?",
        "**Clarity of Explanation**: Was the final analysis clear, well-structured, and easy for a human stakeholder to understand?"
      ],
      "language": "cpp",
      "score": 0.8888
    },
    {
      "id": "cpp_web_dashboard_expert_039_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Extension for Plugin API and Event Integration",
      "context_length": 863245,
      "files_count": 79,
      "task_prompt": "You are a senior architect tasked with designing the extension to the MosaicBoard Studio plugin system. Your goal is to produce a high-level technical design document outlining how to achieve this, without writing the full implementation. \n\nYour analysis must be based on the existing codebase and answer the following questions:\n\n1.  **Current State Analysis:** Briefly describe how the `PluginManager` currently loads and interacts with plugins. Which are the key classes and interfaces involved in this process?\n\n2.  **API Endpoint Registration:** Propose a mechanism for a plugin to register its own HTTP API endpoints (e.g., `POST /api/v1/plugins/my-plugin/custom-action`). Describe the necessary changes to the `PluginManager`, the `Server`, and the plugin interface. How would the `Server` know how to route a request to the correct plugin's handler function?\n\n3.  **Event Bus Integration:** Propose a mechanism for a plugin to subscribe to and handle events from the core `EventBus`. How would a plugin get access to the `EventBus` instance? What changes are required in the plugin lifecycle management within `PluginManager`?\n\n4.  **Interface Design:** Based on your proposals, should the existing `ITile.h` interface be modified, or should a new, more comprehensive plugin interface (e.g., `IPlugin.h`) be created? Justify your choice and outline the key methods this interface should contain.\n\nYour response should be a clear, written explanation referencing specific files, classes, and design patterns observable in the codebase.",
      "ground_truth": "The core of a correct solution involves creating a more comprehensive plugin interface and using the `PluginManager` as a mediator to prevent tight coupling between plugins and the server core.\n\n1.  **Analysis:** The current system uses `PluginManager` to dynamically load shared libraries. It looks for a C-style factory function (e.g., `create_plugin()`) which returns an object that can create instances of `ITile`. The `PluginManager`'s responsibility is limited to managing the lifecycle of these tile-providing objects.\n\n2.  **Proposed Design:**\n    *   **New Interface:** Create a new `IPlugin.h` interface. The main class in a plugin's shared library will implement this. The `ITile` interface will remain for the visual components themselves.\n        ```cpp\n        // In a new IPlugin.h\n        class IPlugin {\n        public:\n            virtual ~IPlugin() = default;\n            // Called by PluginManager after loading\n            virtual void initialize(const PluginContext& context) = 0;\n            // Returns API endpoints for the Server to register\n            virtual std::vector<RouteDefinition> getApiRoutes() = 0;\n            // Returns factories for dashboard tiles\n            virtual std::vector<std::shared_ptr<ITileFactory>> getTileFactories() = 0;\n            virtual void shutdown() = 0;\n        };\n        ```\n        The `PluginContext` struct would contain a reference to the `EventBus`: `struct PluginContext { EventBus& eventBus; };`\n\n    *   **PluginManager Changes:** `PluginManager::loadPlugins()` must be updated. It will now expect the `create_plugin()` entry point in each `.so`/`.dll` to return a `std::unique_ptr<IPlugin>`. After creating the instance, it will:\n        a. Create a `PluginContext` object containing a reference to the application's `EventBus`.\n        b. Call `plugin->initialize(context)`. This allows the plugin to subscribe to events.\n        c. Call `plugin->getApiRoutes()` and pass the returned `RouteDefinition` vector to the `Server`'s routing engine for registration.\n        d. The `PluginManager` will now store `std::unique_ptr<IPlugin>` instead of the old tile factories.\n\n    *   **Server Interaction:** The `Server` class does not need to know about plugins directly. It only needs a method like `registerRoutes(const std::vector<RouteDefinition>& routes)`. The `Application` or `main` function will orchestrate this by first letting `PluginManager` load plugins and collect routes, and then passing those routes to the `Server` instance before starting it.\n\nThis design correctly separates concerns, enhances plugin capabilities without exposing core server internals, and provides a clear, type-safe contract for future plugin development.",
      "evaluation_criteria": [
        "**Architectural Comprehension:** Did the agent correctly identify the roles and interactions of `PluginManager`, `Server`, `EventBus`, and the existing plugin entry points?",
        "**Design Quality (Decoupling):** Does the proposed solution avoid tightly coupling plugins to the `Server`'s implementation? Is the `PluginManager` correctly used as a mediator?",
        "**Interface Design:** Did the agent propose creating a new, more suitable interface (`IPlugin`) rather than inappropriately modifying `ITile`? Is the proposed interface logical?",
        "**Lifecycle Management:** Does the plan correctly identify the need to modify the plugin loading and initialization sequence in `PluginManager` to handle event subscriptions and route registration?",
        "**Problem Decomposition:** Was the agent able to break down the problem into the distinct parts: API registration, event handling, and interface design?",
        "**Code-to-Concept Mapping:** Did the agent successfully reference specific C++ classes and files from the provided list to support its design proposal?"
      ],
      "language": "cpp",
      "score": 0.8882
    },
    {
      "id": "java_mobile_social_expert_058_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Analyze and Document the Data Synchronization and Conflict Resolution Strategy",
      "context_length": 857138,
      "files_count": 79,
      "task_prompt": "Your task is to analyze the architecture responsible for background data synchronization and conflict resolution for user journal entries. Based on your analysis of the provided files, specifically focusing on `SyncWorker.java`, `SyncConflictResolver.java`, `JournalRepository.java`, `JournalRepositoryImpl.java`, and the Dagger modules, answer the following questions:\n\n1.  **High-Level Flow:** Describe the end-to-end process that is initiated to synchronize local journal entries with the remote server. Start from what triggers the process and end with the data being successfully synced or marked for manual resolution. A sequence diagram in Mermaid format or a detailed, numbered list is required.\n\n2.  **Component Responsibilities:** Detail the specific role of the following key components in the synchronization process:\n    *   `SyncWorker`: What is its primary responsibility? How is it triggered?\n    *   `JournalRepository`: How does it facilitate the separation between local and remote data operations during a sync?\n    *   `SyncConflictResolver`: What is its purpose? What specific conflict resolution strategies does it seem to implement (e.g., last-write-wins, server-authoritative, client-authoritative)?\n    *   `DatabaseModule` & `NetworkModule`: How do these DI modules provide the necessary dependencies for the synchronization to function correctly in a decoupled manner?\n\n3.  **Architectural Pattern Identification:** Identify and name the primary architectural and design patterns employed in this synchronization system. Explain why these patterns are a good fit for this problem domain (offline-first mobile application).\n\n4.  **Architectural Justification & Trade-offs:** Explain the key architectural benefits of this design. Why was this complex approach likely chosen over a simpler 'fetch-on-load' strategy? What are the potential trade-offs or complexities introduced by this design (e.g., battery usage, data consistency challenges)?",
      "ground_truth": "Here are the key insights the agent should uncover:\n\n1.  **High-Level Flow:**\n    *   **Trigger:** The `SyncWorker` is likely enqueued by `WorkManager` based on constraints like network availability or on a periodic basis (e.g., every few hours).\n    *   **Fetch Local Changes:** `SyncWorker` starts and calls a method on `JournalRepository`, like `getUnsyncedEntries()`, to get all local journal entries marked as dirty or new.\n    *   **API Call:** For each unsynced entry, the `JournalRepository` makes an API call (e.g., POST for new entries, PUT for updated ones) to the server.\n    *   **Conflict Detection:** If the API returns a success code (e.g., 200 OK), the local entry is marked as synced. If it returns a conflict error (e.g., 409 Conflict), this indicates the server has a newer version of the entry.\n    *   **Conflict Resolution:** The repository catches the conflict and passes the local and a newly fetched server version of the entry to the `SyncConflictResolver`.\n    *   **Resolution Strategy:** `SyncConflictResolver` implements a strategy. For example, it might compare timestamps. If the server's version is newer ('last-write-wins'), it might overwrite the local data. If it cannot auto-resolve, it might mark the local entry with a 'conflict' state for the user to resolve manually in the UI.\n    *   **Completion:** The `SyncWorker` returns `Result.success()` if the process completes, or `Result.retry()`/`Result.failure()` if systemic issues occur.\n\n2.  **Component Responsibilities:**\n    *   `SyncWorker`: An Android `WorkManager` implementation responsible for orchestrating the entire background sync process. It's the entry point and manages the lifecycle of the background task.\n    *   `JournalRepository`: Implements the Repository Pattern. It abstracts the data sources, providing a single source of truth for the `SyncWorker`. It contains the business logic to fetch from the local DB, push to the remote API, and handle API responses.\n    *   `SyncConflictResolver`: Implements the Strategy Pattern for conflict resolution. It's a specialized component whose only job is to compare two versions of a `JournalEntry` and decide on a resolution. The current strategy appears to be a hybrid: it uses a 'server-authoritative' or 'last-write-wins' approach based on a `lastModified` timestamp, but with a fallback to flag the entry for manual user intervention if automatic resolution is too risky (e.g., significant text changes).\n    *   `DatabaseModule` & `NetworkModule`: These Dagger modules provide singleton instances of the `AppDatabase` (and its DAOs) and the `ApiService` (Retrofit instance) respectively. This allows components like `JournalRepositoryImpl` to receive these dependencies via constructor injection, decoupling them from the specifics of how the database or network client are created and configured.\n\n3.  **Architectural Patterns:**\n    *   **Worker Pattern:** Using `WorkManager` for reliable, deferrable background execution.\n    *   **Repository Pattern:** Decoupling data sources (local/remote) from the business logic.\n    *   **Dependency Injection:** Used throughout to manage dependencies and promote loose coupling and testability.\n    *   **Strategy Pattern:** `SyncConflictResolver` encapsulates different resolution algorithms, allowing the strategy to be changed without altering the repository that uses it.\n\n4.  **Architectural Justification & Trade-offs:**\n    *   **Benefits:** The primary benefit is a robust **offline-first user experience**. Users are not blocked by poor connectivity. The architecture ensures data consistency across devices and prevents data loss. It's highly scalable and maintainable due to the separation of concerns (sync orchestration, data access, and conflict logic are all in separate components).\n    *   **Reasoning vs. Simpler Approach:** A 'fetch-on-load' strategy is simple but fails completely when offline. It also doesn't handle cases where the same user modifies data from two different devices (e.g., a phone and a tablet), which would lead to data overwrites and loss. This more complex design is necessary for a reliable, multi-platform experience.\n    *   **Trade-offs:**\n        *   **Complexity:** The logic is significantly more complex than a simple fetch, making it harder to debug and maintain.\n        *   **Battery Consumption:** Poorly configured background jobs can lead to significant battery drain.\n        *   **Data Immediacy:** Data is not real-time. There is a lag between a change being made on one device and it appearing on another, dependent on the sync interval.",
      "evaluation_criteria": [
        "Correctly identifies the `WorkManager` as the trigger for the synchronization process.",
        "Accurately describes the sequence of operations, including fetching local data, making API calls, and handling responses.",
        "Correctly identifies the roles of `SyncWorker`, `JournalRepository`, and `SyncConflictResolver`.",
        "Demonstrates understanding of Dependency Injection by explaining the role of the Dagger modules.",
        "Correctly identifies the key architectural patterns (Worker, Repository, Strategy).",
        "Provides a clear and logical justification for the offline-first architecture over simpler alternatives.",
        "Identifies relevant trade-offs such as increased complexity and potential battery usage.",
        "The overall explanation is coherent, technically accurate, and demonstrates a deep understanding of modern Android architecture."
      ],
      "language": "java",
      "score": 0.8866
    },
    {
      "id": "go_ml_nlp_expert_053_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Bottleneck Analysis and Refactoring Proposal for EchoPulse's Data Ingestion Pipeline",
      "context_length": 841973,
      "files_count": 80,
      "task_prompt": "You are a principal software architect tasked with evaluating the EchoPulse platform's readiness for a 10x increase in data ingestion volume. Your primary focus is on the data path from initial signal reception to its storage in the feature store.\n\nYour specific tasks are:\n1.  **Map the Data Flow:** Analyze the provided source code to trace the path of an incoming social signal. Identify the key modules (from the `src/module_*.go` files) responsible for a) receiving the data, b) performing NLP-based feature extraction, and c) writing the extracted features to the feature store.\n2.  **Identify the Architectural Bottleneck:** Based on the data flow and the interactions between the identified modules, pinpoint the primary architectural pattern or implementation detail that will fail to scale under a 10x load. Provide a technical explanation for why this is a bottleneck.\n3.  **Propose a Refactoring Strategy:** Design a high-level refactoring plan to address the identified bottleneck. You should not write code, but describe the changes to the system's architecture. Recommend specific architectural patterns or technologies (e.g., message queues, worker pools, caching strategies) that would be appropriate.\n4.  **Justify Your Proposal:** Explain how your proposed architecture resolves the scalability issue. Contrast the current data flow with your proposed one, highlighting the benefits in terms of throughput, latency, and system resilience. Refer back to the specific modules you identified in step 1.",
      "ground_truth": "The agent is expected to uncover the following hidden architecture and propose a corresponding solution:\n\n*   **Identified Data Flow and Bottleneck:**\n    1.  **Ingestion:** `module_54.go` contains the primary HTTP server entry point. It defines a handler for `/api/v1/signal` that receives incoming data.\n    2.  **Synchronous Processing:** Inside this HTTP handler, `module_54.go` makes a direct, blocking function call to `processSignal` in `module_69.go`. This `processSignal` function is extremely CPU-intensive, performing complex NLP feature extractions (e.g., dependency parsing, named entity recognition).\n    3.  **Synchronous Storage:** After the long processing step, `module_69.go` then makes another direct, blocking call to a function in `module_25.go`, which is responsible for connecting to a PostgreSQL database and writing the features to the `features` table.\n    4.  **The Bottleneck:** The core architectural flaw is that the entire ingest-process-store pipeline is executed synchronously within a single HTTP request-response cycle. This ties up a connection and a server thread for the entire duration of the slow processing and database write. Under a 10x load, this will quickly exhaust the server's connection pool and compute resources, leading to extreme latency and request failures.\n\n*   **Expected Refactoring Proposal:**\n    1.  **Decouple with a Message Queue:** Modify the HTTP handler in `module_54.go`. Its sole responsibility should be to perform minimal validation on the incoming data, serialize it into a message, and publish it to a durable message queue (e.g., Kafka, RabbitMQ, or NATS). It should then immediately return a `202 Accepted` status to the client. This makes the ingestion endpoint extremely fast and lightweight.\n    2.  **Introduce Asynchronous Workers:** Create a new pool of 'Processor' workers (or refactor `module_69.go` to run as a separate, scalable service). These workers will act as consumers for the message queue. Each worker will pull one message at a time, perform the CPU-intensive feature extraction from `module_69.go`, and then handle the database write via `module_25.go`.\n    3.  **Benefits Justification:** This new architecture decouples the ingestion layer from the processing layer. The ingestion endpoint (`module_54.go`) can now handle massive bursts of traffic. The processing workload can be scaled independently by adjusting the number of 'Processor' workers. The message queue provides resilience (failed processing can be retried) and acts as a shock absorber, smoothing out load spikes.",
      "evaluation_criteria": [
        {
          "name": "Data Flow Mapping Accuracy",
          "description": "Correctly identifies the chain of responsibility from ingestion to storage, specifically naming `module_54` (ingest), `module_69` (process), and `module_25` (store) as the key components in the synchronous chain.",
          "weight": 3
        },
        {
          "name": "Bottleneck Identification",
          "description": "Correctly identifies the 'synchronous execution of CPU-bound processing and I/O within a single HTTP request' as the primary architectural bottleneck.",
          "weight": 3
        },
        {
          "name": "Architectural Solution Quality",
          "description": "Proposes a viable, industry-standard solution, such as decoupling with a message queue and using a pool of asynchronous workers.",
          "weight": 2
        },
        {
          "name": "Justification and Rationale",
          "description": "Clearly explains *why* the synchronous model fails at scale and *how* the proposed asynchronous, decoupled model solves for throughput, latency, and resilience, referencing the specific modules involved.",
          "weight": 2
        }
      ],
      "language": "go",
      "score": 0.8861
    },
    {
      "id": "typescript_desktop_productivity_expert_055_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for Real-Time Collaboration Feature",
      "context_length": 783545,
      "files_count": 84,
      "task_prompt": "You are a principal engineer tasked with creating a technical design document for adding real-time collaboration to PaletteFlow Studio. Before writing the full design, you must first perform a thorough analysis of the current architecture. Your analysis should be presented as a markdown report answering the following key questions. Your answers must be justified by referencing specific files, design patterns, and architectural components in the existing codebase.\n\n### Architectural Analysis Report\n\n1.  **State Management & Source of Truth:**\n    -   Where is the primary source of truth for a user's workspace data (nodes, links, content) currently stored?\n    -   Describe the current state management strategy in both the main and renderer processes. How do they synchronize?\n    -   Explain why this model is or is not suitable for a multi-user real-time environment.\n\n2.  **Data Flow for Mutations:**\n    -   Trace the complete data flow when a user performs a simple action, such as moving a node on the canvas in the renderer process, to the point where that change is persisted.\n    -   Identify the key components (classes, services, functions) involved in this flow, including the IPC communication layer.\n    -   Analyze the limitations of this data flow model in a scenario with multiple concurrent users.\n\n3.  **Impact on the Plugin System:**\n    -   The plugin API allows third-party code to read and modify workspace data (e.g., via the Node API described in `docs/plugin-api/reference/node-api.md`).\n    -   How would the introduction of real-time collaboration affect the stability and predictability of the plugin ecosystem?\n    -   What specific changes or additions to the plugin API (`IPluginService.ts`, API docs) would be necessary to support a collaborative environment safely?\n\n4.  **Conflict Resolution Strategy:**\n    -   Examine the core domain entities (e.g., `Workspace.ts`, `Node.ts`). Do they contain any properties that would support conflict resolution (e.g., version numbers, vector clocks, last-updated timestamps)?\n    -   Based on the application's layered architecture (`core`, `adapters`, `main`, `renderer`), where would you recommend implementing the conflict resolution logic (e.g., CRDTs, OT)? Justify your choice.",
      "ground_truth": "**1. State Management & Source of Truth:**\n- The single source of truth is the JSON file on the local file system, managed by `FileSystemWorkspaceRepository.ts` in the main process.\n- The main process holds the authoritative state. The renderer process maintains a copy of the state for rendering (e.g., in a state management library like Redux/Zustand, as hinted by `renderer/state/store.ts`). Synchronization happens via explicit IPC calls from the renderer to the main process for mutations, and the main process can push updates back to the renderer, but it's not designed for peer-to-peer updates.\n- This model is completely unsuitable for real-time collaboration. It has no central server, no mechanism for broadcasting changes to multiple clients, and relies on a slow, single-writer file system lock.\n\n**2. Data Flow for Mutations:**\n- The flow is: `NodeComponent.tsx` (drag event) -> `useViewModel.ts` (or similar hook) -> `renderer/ipc/bridge.ts` (sends IPC message like 'update-node-position') -> `main/IpcMainManager.ts` (receives message) -> `main/ipc/handlers/workspaceHandlers.ts` (handler logic) -> `UpdateNodePosition.ts` (use case) -> `FileSystemWorkspaceRepository.ts` (writes to disk).\n- The key limitation is that it's a point-to-point, asynchronous request-response model. It cannot broadcast changes to other connected clients. It's also high-latency due to file I/O and would suffer from race conditions if multiple users tried to write to the same file via a central server without a locking mechanism.\n\n**3. Impact on the Plugin System:**\n- The current plugin API (defined by `IPluginService.ts` and documented in `docs/plugin-api/`) likely assumes it has exclusive and immediate access to workspace data. In a collaborative session, a plugin's change could be instantly overwritten by a remote change, or it could trigger a cascade of network events. This would lead to unpredictable behavior and data corruption.\n- The API would need significant changes: introducing transactional updates, adding events for remote changes (e.g., `onRemoteNodeUpdated`), and potentially restricting direct mutations in favor of command-based operations that can be serialized and ordered.\n\n**4. Conflict Resolution Strategy:**\n- The core entities (`Node.ts`, `Workspace.ts`, `BaseContent.ts`) lack any properties for conflict resolution. They have IDs, but no version vectors, Lamport timestamps, or OT-specific metadata.\n- The logic should be implemented in the `core/application/use-cases` layer. This is because the use cases are the orchestrators of business logic, sitting between the external interfaces (like IPC) and the domain entities. Placing it here ensures that any mutation, regardless of its origin (local user, remote user, plugin), passes through the same conflict resolution and validation logic before being persisted. The domain entities themselves would need to be modified to carry the necessary metadata.",
      "evaluation_criteria": [
        "**Correctly Identifies Architecture:** Accurately describes the Electron main/renderer architecture and the clean architecture layering (`core`, `adapters`).",
        "**Accurately Traces Data Flow:** Correctly traces the full path of a data mutation from the renderer UI, through the IPC bridge, to the main process handler, and finally to the file system repository.",
        "**Identifies State Management Flaws:** Correctly identifies the file system as the source of truth and explains why this single-user model is fundamentally incompatible with real-time collaboration.",
        "**Analyzes IPC Limitations:** Correctly identifies the IPC mechanism as a request-response pattern and explains its inadequacy for broadcasting state to multiple clients.",
        "**Assesses Plugin API Risks:** Demonstrates understanding of the plugin system's architecture and correctly identifies the risks of synchronous data access in a distributed context.",
        "**Proposes Correct Locus for Business Logic:** Correctly identifies the `core/application/use-cases` layer as the appropriate location for new conflict resolution logic, justifying the choice based on architectural principles.",
        "**Synthesizes Information:** The overall quality of the analysis, connecting findings from different files (docs, code, domain models) into a coherent and accurate architectural assessment."
      ],
      "language": "typescript",
      "score": 0.8845
    },
    {
      "id": "rust_web_social_expert_073_bug_investigation_expert_01",
      "task_category": "bug_investigation",
      "difficulty": "expert",
      "title": "Cross-User Data Leakage Under Concurrent Load in Session Initialization",
      "context_length": 1067411,
      "files_count": 86,
      "task_prompt": "A critical, intermittent data leakage bug has been reported in the EduPulse Live application. Users are occasionally seeing data belonging to other users on their dashboards immediately after logging in. Your task is to perform a thorough investigation of the codebase to identify the root cause of this data leakage.\n\nYour analysis must:\n1.  Identify the specific code flaw(s) responsible for the bug.\n2.  Pinpoint the exact files and line numbers containing the problematic code.\n3.  Provide a detailed, step-by-step explanation of the conditions (the 'race') that trigger the bug.\n4.  Explain why this bug is intermittent and only manifests under concurrent load.",
      "ground_truth": {
        "root_cause_summary": "The root cause is a classic race condition involving a mismanaged global state used for request context. A legacy component reads user context from a global `Arc<Mutex<Option<UserContext>>>`, which is incorrectly written to by a modern authentication middleware. While the middleware also correctly uses a `tokio::task_local` for most of the application, its write to the global state for backward compatibility creates a window for data corruption. Under concurrent load, one request's context can be overwritten in this global variable by another request before the first request's handler has finished reading from it, leading to data leakage.",
        "culprit_files": [
          {
            "file_path": "src/module_48.txt",
            "description": "This file contains the primary authentication and context-setting middleware. It correctly populates a `task_local` but also incorrectly writes the user context to a globally shared `Arc<Mutex<...>>` for a legacy component.",
            "line_number": 215
          },
          {
            "file_path": "src/module_15.txt",
            "description": "This file contains a legacy data-fetching function for the user dashboard. This function exclusively reads user context from the flawed global `Arc<Mutex<...>>` instead of the modern `task_local`, making it vulnerable to the race condition.",
            "line_number": 98
          },
          {
            "file_path": "src/utils.txt",
            "description": "This utility file contains the definition of the global static variable (`LEGACY_USER_CONTEXT: Arc<Mutex<Option<UserContext>>>`).",
            "line_number": 132
          }
        ],
        "race_condition_explanation": "1. Request A for User 'Alice' arrives on a worker thread.\n2. The middleware in `module_48.txt` authenticates Alice. It locks the global mutex, writes `Some(Alice's Context)`, and unlocks it. It then populates the `task_local` correctly.\n3. An OS or async runtime context switch occurs. Request B for User 'Bob' is scheduled on the *same* worker thread.\n4. The middleware in `module_48.txt` authenticates Bob. It locks the global mutex, writes `Some(Bob's Context)`, overwriting Alice's entry, and unlocks it.\n5. Control returns to Request A's handler, which proceeds to call the legacy dashboard data function in `module_15.txt`.\n6. The function in `module_15.txt` locks the global mutex. It reads the context, but now it reads `Some(Bob's Context)`.\n7. The function then fetches and returns dashboard data for Bob, which is then served in the HTTP response to Alice."
      },
      "evaluation_criteria": [
        "**Correctness of Root Cause Analysis:** Did the agent correctly identify the race condition caused by using a global `Arc<Mutex<...>>` for request-specific data?",
        "**Accuracy of File Identification:** Did the agent correctly identify `module_48.txt` and `module_15.txt` as the primary files involved in the bug?",
        "**Precision of Code Localization:** How accurately did the agent pinpoint the specific lines of code responsible for writing to and reading from the flawed global state?",
        "**Clarity of Explanation:** Was the agent's explanation of the race condition clear, logical, and did it correctly describe the sequence of events leading to the data leak?",
        "**Identification of Legacy Pattern:** Did the agent recognize the anti-pattern of mixing `task_local` context passing with a legacy global state approach?",
        "**Efficiency of Investigation:** Did the agent follow a logical debugging process (e.g., searching for shared state first) or did it use a brute-force, inefficient approach?"
      ],
      "language": "rust",
      "score": 0.8842
    },
    {
      "id": "javascript_web_social_expert_073_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Analyze Performance Bottleneck in Asynchronous User Progress Update Flow",
      "context_length": 739599,
      "files_count": 87,
      "task_prompt": "Your task is to perform a root cause analysis of the user progress update latency. You must not write or modify any code. Instead, produce a detailed architectural analysis report in markdown format. \n\nThe report must:\n1.  **Trace the complete, end-to-end data flow.** Start from the user's action in the frontend (e.g., completing a lecture in `LecturePlayer.js`), through the API Gateway, to all relevant backend services, and finally back to the frontend components that display the progress (`ProgressChart.js`, `ActivityStream.js`). Clearly distinguish between synchronous API calls and asynchronous event-driven messages.\n2.  **Identify the key architectural components involved.** List the specific services, databases, message brokers, and critical code modules (e.g., controllers, services, repositories, event producers/consumers) that participate in this flow.\n3.  **Pinpoint the top 3 most likely architectural areas causing the delay.** For each potential bottleneck, provide a detailed justification referencing specific files (e.g., `docker-compose.yml`, `nginx.conf`, service code, or ADRs) and architectural patterns (e.g., eventual consistency, producer/consumer configuration) that support your hypothesis.",
      "ground_truth": "### Architectural Analysis: User Progress Update Latency\n\n#### 1. End-to-End Data Flow\n\nThe process involves two distinct paths: a synchronous write path and an asynchronous event path, followed by a separate read path.\n\n*   **Synchronous Write Path:**\n    1.  A user interacts with `frontend/src/components/course/LecturePlayer.js`, triggering a progress update.\n    2.  The frontend calls a function in `frontend/src/services/courseService.js`, which sends a `PUT` request to an endpoint like `/api/courses/:courseId/progress`.\n    3.  The request is routed by `services/api-gateway/nginx.conf` to the `course-service`.\n    4.  `services/course-service/src/api/courseController.js` handles the request, calling `services/course-service/src/services/courseService.js`.\n    5.  The service logic updates the progress in its PostgreSQL database via `services/course-service/src/repositories/courseRepository.js`.\n\n*   **Asynchronous Event Path (based on `ADR/002`):**\n    1.  After successfully updating its database, the `course-service` uses `services/course-service/src/events/producer.js` to publish a `USER_PROGRESS_UPDATED` event to a Kafka topic.\n    2.  Other services, such as the `notification-service`, might consume this event to send real-time alerts. The `auth-service` might also consume it to update an aggregated user model.\n\n*   **Frontend Read Path:**\n    1.  The `frontend/src/pages/DashboardPage.js` and its components (`ProgressChart.js`, `ActivityStream.js`) are responsible for displaying the data.\n    2.  They fetch this data by making `GET` requests to the `course-service` via the API Gateway. This is likely done on a polling interval or upon initial page load.\n\n**The perceived latency is the time delta between the completion of the synchronous write path and the next execution of the frontend read path that retrieves the new state.** The intermittent nature is caused by variable delays in the asynchronous path, which can affect data aggregation or related features, and potential load-related issues in the core services or infrastructure.\n\n#### 2. Key Architectural Components\n\n*   **Services:** `frontend`, `api-gateway`, `course-service`, `notification-service` (as an event consumer example).\n*   **Infrastructure:** NGINX (`api-gateway`), Kafka (`message-broker` defined in `docker-compose.yml`), PostgreSQL (database for `course-service`).\n*   **Key Files:**\n    *   `docs/ADR/002-event-sourcing-with-kafka.md` (Confirms architecture)\n    *   `docker-compose.yml` (Defines Kafka service configuration)\n    *   `services/course-service/src/services/courseService.js` (Core write logic)\n    *   `services/course-service/src/events/producer.js` (Async event publishing)\n    *   `frontend/src/pages/DashboardPage.js` (Initiates the read path)\n\n#### 3. Top 3 Potential Bottlenecks\n\n1.  **Kafka Broker Contention/Producer Lag:** The most likely culprit for intermittent delays under load. The `docker-compose.yml` file shows a single Kafka broker configuration. Under high traffic, this single broker can become a bottleneck, increasing the time it takes for the `course-service` producer to successfully publish the `USER_PROGRESS_UPDATED` event. If the producer's configuration is synchronous or has a long timeout, this could even delay the API response to the user, but more likely it just delays the event's propagation through the system, affecting any downstream consumers and creating the eventual consistency lag.\n\n2.  **Eventual Consistency & Frontend Polling Strategy:** This is a design-related bottleneck. The architecture is built on eventual consistency, meaning data updates are not instantaneously reflected everywhere. The frontend (`DashboardPage.js`) is likely not using WebSockets for real-time updates but is instead polling the `course-service` API at a fixed interval (e.g., every 30 seconds). This polling interval creates a baseline perceived latency. The problem becomes \"intermittent\" because a user's action might occur just after a poll, forcing them to wait for the full interval for the next update.\n\n3.  **Database Transaction Load in `course-service`:** During peak hours, the `course-service`'s PostgreSQL database could be under heavy write load. The transaction to update user progress might take longer to commit. This would delay the entire chain of events, including the subsequent Kafka message publication. While the primary database update is synchronous, its performance degradation under load would directly translate to a longer wait before the asynchronous part of the flow even begins, thus contributing significantly to the overall delay.",
      "evaluation_criteria": [
        "**Flow Tracing Accuracy:** Correctly identifies and separates the synchronous write, asynchronous event, and frontend read paths.",
        "**Component Identification:** Accurately lists the critical services, infrastructure, and specific files involved in the process.",
        "**Evidence-Based Reasoning:** Cites specific files (`docker-compose.yml`, ADRs, service code) to justify conclusions.",
        "**Problem Diagnosis:** Correctly identifies the asynchronous, event-driven nature of the architecture (`eventual consistency`) as the root cause of the *type* of delay.",
        "**Bottleneck Plausibility:** Proposes at least two plausible, architecturally-sound bottlenecks (e.g., Kafka contention, frontend polling, DB load).",
        "**Understanding of Trade-offs:** Demonstrates an understanding that this latency is a trade-off inherent in the chosen loosely-coupled, event-driven architecture."
      ],
      "language": "javascript",
      "score": 0.8834
    },
    {
      "id": "rust_ml_computer_vision_expert_054_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Decouple Experiment Tracking Logic via Trait Abstraction",
      "context_length": 1011970,
      "files_count": 78,
      "task_prompt": "Your task is to refactor the experiment tracking mechanism in the VisuTility Orchestrator. You must decouple the core logic from the concrete implementation of the `RawMetricClient`.\n\n**Detailed Instructions:**\n\n1.  **Identify Usage:** Locate all usages of `RawMetricClient` and its methods (`log_scalar`, `log_hyperparameter`, `upload_model_artifact`). You will find that it is primarily instantiated in `src/module_7.txt` and passed as a dependency to functions within `src/module_34.txt` (hyperparameter tuning) and `src/module_62.txt` (model training).\n\n2.  **Create New Module:** Create a new file named `src/experiment_tracker.txt`.\n\n3.  **Define Abstraction:** In the new `src/experiment_tracker.txt` file, define a public Rust trait named `ExperimentTracker`. This trait should abstract the core tracking operations:\n    ```rust\n    use std::path::Path;\n\n    pub trait ExperimentTracker {\n        fn log_scalar(&self, metric_name: &str, value: f64, step: u64);\n        fn log_hyperparameter(&self, param_name: &str, value: &str);\n        fn upload_model_artifact(&self, local_path: &Path, remote_name: &str) -> Result<(), std::io::Error>;\n    }\n    ```\n\n4.  **Create Concrete Implementation:** In the same `src/experiment_tracker.txt` file, create a struct named `DefaultTracker`. This struct should hold an instance of the `RawMetricClient`. Implement the `ExperimentTracker` trait for `DefaultTracker`, where the trait methods simply call the corresponding methods on the internal `RawMetricClient`.\n\n5.  **Refactor Core Modules:** Modify the functions in `src/module_34.txt` and `src/module_62.txt` that currently accept `&RawMetricClient`. Change their signatures to accept a generic type that implements the `ExperimentTracker` trait (e.g., `tracker: &T where T: ExperimentTracker`). Update the function bodies to call the methods on the new trait (`tracker.log_scalar(...)` etc.) instead of the concrete client.\n\n6.  **Update Instantiation:** In `src/module_7.txt`, where `RawMetricClient` was previously instantiated and passed to other modules, you must now instantiate `DefaultTracker` and pass it to the newly refactored functions.\n\n7.  **Integrate New Module:** Declare the new module as public by adding `pub mod experiment_tracker;` at the top of `src/utils.txt` to make it accessible to the rest of the application.\n\n8.  **Encapsulate the Old Client:** Modify the `RawMetricClient` struct and its `impl` block in `src/utils.txt` to be private to the crate (i.e., remove the `pub` keyword). It should only be accessible from within the new `src/experiment_tracker.txt` module.\n\n9.  **Update Tests:** Modify `tests/test_main.txt`. Create a `MockTracker` struct that also implements the `ExperimentTracker` trait. This mock should not perform any I/O but can use internal state (e.g., `RefCell<Vec<String>>`) to record which methods were called. Update the tests to use this `MockTracker` instead of the real `RawMetricClient`, allowing for unit tests that don't depend on the client's implementation details.",
      "ground_truth": "The ideal solution involves changes across 5 files.\n\n1.  **`src/experiment_tracker.txt` (New File):** Contains the `ExperimentTracker` trait definition and the `DefaultTracker` struct which implements the trait by wrapping the `RawMetricClient`.\n2.  **`src/utils.txt`:** The `RawMetricClient` struct and its `impl` are no longer `pub`. The file now contains `pub mod experiment_tracker;` at the top.\n3.  **`src/module_34.txt` & `src/module_62.txt`:** All functions that previously took `client: &RawMetricClient` now have a signature like `fn some_function<T: ExperimentTracker>(..., tracker: &T, ...)`. All internal calls are updated to `tracker.log_scalar(...)` etc. There should be no remaining references to `RawMetricClient`.\n4.  **`src/module_7.txt`:** The line `let client = RawMetricClient::new(...)` is replaced with `let tracker = DefaultTracker::new(...)` (or similar, depending on the constructor for `DefaultTracker`). This `tracker` instance is then passed to the functions in `module_34` and `module_62`.\n5.  **`tests/test_main.txt`:** Contains a new `MockTracker` struct that implements `ExperimentTracker`. Test functions are updated to instantiate `MockTracker` and pass it to the functions under test. Assertions are made against the state of the `MockTracker` after the function call.",
      "evaluation_criteria": [
        "**Correctness of Abstraction:** The `ExperimentTracker` trait in `src/experiment_tracker.txt` must be defined correctly, and the `DefaultTracker` struct must correctly implement it.",
        "**Successful Decoupling:** `module_34.txt` and `module_62.txt` must be completely free of any direct references to `RawMetricClient`.",
        "**Correct Use of Generics/Traits:** Function signatures in `module_34.txt` and `module_62.txt` must be correctly modified to accept an object implementing `ExperimentTracker`.",
        "**Updated Dependency Injection:** `module_7.txt` must correctly instantiate `DefaultTracker` and pass it to the refactored functions.",
        "**Module System Integration:** The new `experiment_tracker` module must be correctly declared in `src/utils.txt`.",
        "**Encapsulation:** The visibility of `RawMetricClient` in `src/utils.txt` must be correctly reduced to non-`pub`.",
        "**Test Adaptation:** `tests/test_main.txt` must be updated to use a mock implementation of the `ExperimentTracker` trait, and the tests should remain valid.",
        "**Code Integrity:** The final code across all modified files must be syntactically correct Rust code and maintain the original application logic."
      ],
      "language": "rust",
      "score": 0.883
    },
    {
      "id": "csharp_data_etl_expert_047_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis and Optimization of Inter-Service Data Transfer",
      "context_length": 743163,
      "files_count": 86,
      "task_prompt": "As a senior architect, you are tasked with resolving a critical performance issue. Your goal is to analyze the data transfer mechanism between the `PaletteStream.Transformer` and `PaletteStream.Quality` services and propose a more performant architectural pattern for handling large data payloads, without fundamentally changing the event-driven nature of the system.\n\nComplete the following steps and provide a detailed report:\n\n1.  **Analyze the Current Data Flow:** Based on the provided files (documentation and source code), describe the exact mechanism by which data is passed from the `Transformer` service to the `Quality` service after a transformation step completes. Identify the key classes, methods, event models (e.g., `DataTransformedEvent`), and Kafka topics involved in this process.\n\n2.  **Identify the Architectural Bottleneck:** Explain precisely why the current approach of sending the full data payload via a Kafka event is an architectural anti-pattern for large datasets. Reference specific limitations of message brokers and the impact on system resources (network bandwidth, memory, serialization overhead).\n\n3.  **Propose an Optimized Architecture:** Propose a new architectural design to solve this issue. The recommended approach should be based on the \"Claim Check\" enterprise integration pattern. Detail how this pattern would be implemented in the PaletteStream context:\n    *   What service would be responsible for generating the 'claim check'?\n    *   Where would the large data payload be stored temporarily?\n    *   What information would the new, lightweight event message contain?\n    *   How would the `Quality` service use this message to retrieve the data?\n\n4.  **Detail the Required System Changes:** List the specific services, classes, and configuration files that would need to be modified to implement your proposed solution. Be specific about the changes (e.g., \"Modify `DataTransformedEvent` in `Shared.Events` to include a data URI field instead of a byte array payload\").\n\n5.  **Analyze Trade-offs:** Discuss the pros and cons of your proposed solution. Consider aspects like system complexity, introduction of new dependencies, data lifecycle management (e.g., cleaning up the temporary data), and fault tolerance.",
      "ground_truth": {
        "current_flow_analysis": "The `TransformationWorker` class in the `PaletteStream.Transformer` service, after processing data, serializes the entire resulting DataFrame. It then constructs a `DataTransformedEvent` (defined in `src/Shared/PaletteStream.Shared.Events/DataEvents.cs`) and embeds this large serialized payload into the event. This event is published to the `palette-data-transformed` Kafka topic using the `KafkaProducer`. The `QualityCheckRunner` class in the `PaletteStream.Quality` service is a consumer of this topic. It receives the `DataTransformedEvent`, deserializes the large payload back into a DataFrame, and then proceeds with the quality checks.",
        "bottleneck_explanation": "Sending large payloads (1GB+) via Kafka is a bottleneck due to: \n1. **Message Size Limits:** Kafka has a configurable but practical limit on message size (`message.max.bytes`). Exceeding this causes failures.\n2. **Broker/Network Strain:** Pushing large messages consumes significant network bandwidth and puts heavy memory/IO pressure on the Kafka brokers, slowing down the entire event bus for all services.\n3. **Serialization/Deserialization Overhead:** The CPU and memory cost of serializing and deserializing gigabyte-scale objects in both the producer (`Transformer`) and consumer (`Quality`) services is substantial and adds significant latency.\n4. **Consumer Lag:** A consumer taking a long time to process one large message can cause consumer lag, delaying the processing of all subsequent messages in its partition.",
        "proposed_solution_claim_check": {
          "description": "Implement the Claim Check pattern.",
          "implementation_steps": [
            "The `Transformer` service, after processing, will upload the large DataFrame payload to a shared, high-throughput blob storage system (e.g., Azure Blob Storage, leveraging a similar pattern to the `DataLakeClient` in the `Loader` service).",
            "The `Transformer` service then generates the 'claim check'a URI or unique identifier pointing to the stored object in blob storage.",
            "A modified `DataTransformedEvent` is published to Kafka. This event is now lightweight and contains metadata and the 'claim check' URI instead of the full data payload.",
            "The `Quality` service consumes the lightweight event, extracts the 'claim check' URI, and uses a blob storage client to stream the data directly from the external store for processing."
          ]
        },
        "system_changes": [
          {
            "file": "src/Shared/PaletteStream.Shared.Events/DataEvents.cs",
            "change": "Modify the `DataTransformedEvent` class to remove the data payload property (e.g., `byte[] ProcessedData`) and add a `string DataLocationUri` property."
          },
          {
            "file": "src/Services/PaletteStream.Transformer/Core/TransformationWorker.cs",
            "change": "Inject a blob storage client. Modify the logic to upload the processed DataFrame to blob storage and publish the event with the returned URI."
          },
          {
            "file": "src/Services/PaletteStream.Quality/Core/QualityCheckRunner.cs",
            "change": "Inject a blob storage client. Modify the event handling logic to read the URI from the event and download the data from blob storage before running checks."
          },
          {
            "file": "src/Services/PaletteStream.Transformer/appsettings.json",
            "change": "Add a new configuration section for the blob storage connection string and container name."
          },
          {
            "file": "src/Services/PaletteStream.Quality/appsettings.json",
            "change": "Add a new configuration section for the blob storage connection string and container name."
          },
          {
            "file": "src/Services/PaletteStream.Transformer/PaletteStream.Transformer.csproj",
            "change": "Add a package reference for the required blob storage SDK (e.g., Azure.Storage.Blobs)."
          },
          {
            "file": "src/Services/PaletteStream.Quality/PaletteStream.Quality.csproj",
            "change": "Add a package reference for the required blob storage SDK."
          }
        ],
        "trade_off_analysis": {
          "pros": [
            "**Performance:** Drastically reduces latency and improves throughput for large datasets.",
            "**Scalability:** Reduces load on the Kafka cluster, allowing the event bus to scale and remain performant for its primary purpose: event notification.",
            "**Stability:** Avoids message size limit errors and reduces the risk of service memory exhaustion."
          ],
          "cons": [
            "**Increased Complexity:** Introduces a new component (blob storage) into the data flow between these two services.",
            "**New Dependency:** Both services now depend on the availability and performance of the blob storage system.",
            "**Data Lifecycle Management:** A mechanism is needed to clean up the temporary data from blob storage after it has been processed (or if processing fails) to prevent orphaned data and control costs. This could involve a separate cleanup job or TTL policies on the storage.",
            "**Transactional Complexity:** The operation is no longer a single 'publish' step. It's now a two-phase process (upload, then publish). Failure after the upload but before the publish can lead to orphaned data."
          ]
        }
      },
      "evaluation_criteria": [
        "**Current State Analysis (20%):** Accurately identifies the key classes (`TransformationWorker`, `QualityCheckRunner`), the event (`DataTransformedEvent`), and the communication channel (Kafka) from the codebase.",
        "**Bottleneck Identification (20%):** Correctly explains *why* the current architecture is flawed for large data, citing specific technical constraints of message brokers.",
        "**Solution Appropriateness (25%):** Proposes the Claim Check pattern or an equally valid alternative and clearly describes its implementation in the context of the given services.",
        "**Impact Analysis Completeness (25%):** Correctly identifies all major components that require modification, including services, shared libraries, project files, and configuration.",
        "**Trade-off Analysis Depth (10%):** Provides a balanced discussion of both the advantages and disadvantages of the proposed solution, including operational concerns like data cleanup."
      ],
      "language": "csharp",
      "score": 0.8809
    },
    {
      "id": "c_fintech_payment_expert_065_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Design for Cross-Service Tuition Auto-Payment",
      "context_length": 886345,
      "files_count": 75,
      "task_prompt": "As a senior software architect for the EduPay Ledger Academy project, you are tasked with creating a detailed architectural proposal for the new 'Tuition Auto-Payment' feature. Your proposal should be a technical markdown document that outlines how to implement this feature without violating the existing architectural patterns.\n\nYour proposal must address the following points:\n\n1.  **Interaction Diagram/Sequence:** Describe the step-by-step flow of communication between the `financial_aid_service`, the `bursar_service`, and any other relevant components. Detail the sequence of commands, events, and queries.\n2.  **Saga Pattern Integration:** Explain how the Saga pattern will be used to manage the distributed transaction. Will you extend an existing saga (like the one in `disburse_stipend_saga.c`) or create a new one? Justify your choice and describe the saga's steps, including any necessary compensating actions.\n3.  **New Domain Events:** Define the new domain events required for this feature (e.g., for successful application, failure, etc.). Specify which service would publish these events and which services would consume them.\n4.  **CQRS/Event Sourcing Impact:** Detail how this new workflow will be recorded in the event store. Explain which aggregates in the `bursar_service` (e.g., `Account`, `Ledger`) would be affected. Also, describe how the `projections_service` would need to be updated to reflect these auto-payments in its read models (e.g., for student dashboards).\n5.  **Compliance and Consent:** Referencing the principles in `docs/architecture/05_security_and_compliance.md` and `docs/lessons/L03_Regulatory_Compliance_in_Code.md`, explain how your design handles the student's opt-in consent. Where would this consent flag be stored and how would it be checked in the workflow?\n6.  **Affected Components:** List the key files and components (e.g., specific use cases, handlers, domain models, infrastructure) that would require modification to implement your design.",
      "ground_truth": "### Architectural Proposal: Tuition Auto-Payment\n\n#### 1. Interaction Sequence\n\n1.  **Saga Start**: The process begins when a `DisburseStipendCommand` is handled by the `financial_aid_service`.\n2.  **Stipend Disbursement**: The `DisburseStipendSaga` executes its primary step, disbursing the funds. Upon success, the `financial_aid_service` publishes a `StipendDisbursed` event via RabbitMQ.\n3.  **Saga Continuation & Pre-condition Check**: The Saga Coordinator, listening for `StipendDisbursed`, proceeds to the next step. It issues a query to the `bursar_service`'s read side (or a dedicated projection) to fetch the student's auto-pay consent status and outstanding balance.\n4.  **Conditional Command**: If the student has opted-in and has a balance, the Saga Coordinator sends a `ProcessTuitionAutoPay` command to the `bursar_service`.\n5.  **Bursar Processing**: The `bursar_service`'s command handler processes the payment, debiting the student's internal funds account and crediting the tuition ledger. Upon success, it persists and publishes a `TuitionPaymentAppliedFromStipend` event.\n6.  **Projection Update**: The `projections_service`'s `DashboardProjector` consumes the `TuitionPaymentAppliedFromStipend` event and updates the student's account balance read model.\n7.  **Saga Completion**: The Saga Coordinator receives confirmation (e.g., by consuming the success event) and marks the saga step as complete.\n\n#### 2. Saga Pattern Integration\n\nThe existing `DisburseStipendSaga` (defined in `disburse_stipend_saga.c/h`) will be extended. This is preferable to a new saga because auto-payment is a direct consequence of disbursement, not an independent business process.\n\n-   **New Saga Step**: A step named `ApplyPaymentToTuition` will be added after the `DisburseStipend` step.\n-   **Compensating Action**: A corresponding compensating action, `ReverseTuitionApplication`, must be implemented. This action would issue a command to the `bursar_service` to reverse the transaction if a later step in the saga were to fail.\n\n#### 3. New Domain Events\n\n-   `TuitionPaymentAppliedFromStipend`: Published by `bursar_service` on successful auto-payment. Contains `student_id`, `amount`, `original_stipend_transaction_id`.\n-   `TuitionAutoPaySkipped`: (Optional but good practice) Published by the Saga Coordinator if the student is not opted-in or has no balance.\n-   `TuitionAutoPayFailed`: Published by `bursar_service` if the internal transfer fails for any reason (e.g., insufficient funds after a race condition).\n\n#### 4. CQRS/Event Sourcing Impact\n\n-   **Aggregates**: In `bursar_service`, the `Account` aggregate will be modified. The `ProcessTuitionAutoPay` command handler will invoke methods on the `Account` instance, which will generate two events: `FundsDebited` (for the stipend holding account) and `FundsCredited` (for the tuition ledger account). These are then stored in the `postgres_event_store`.\n-   **Projections**: The `projections_service/event_handlers/dashboard_projector.c` must be modified. It needs a new handler function to subscribe to the `TuitionPaymentAppliedFromStipend` event. When received, it will update the denormalized SQL table that stores student account balances, ensuring the UI reflects the payment instantly.\n\n#### 5. Compliance and Consent\n\n-   **Consent Storage**: The student's opt-in consent flag for auto-payment should be stored as an attribute on the student's `Account` aggregate within the `bursar_service`'s bounded context. This ensures the data lives within the service that acts upon it.\n-   **Consent Check**: As per `05_security_and_compliance.md`, explicit consent is required. The Saga Coordinator will perform this check (Step 1.3) by querying the `bursar_service`'s read model *before* issuing the `ProcessTuitionAutoPay` command. This prevents unauthorized movement of funds.\n\n#### 6. Affected Components\n\n-   `docs/architecture/04_saga_pattern.md`: Update with the new saga flow.\n-   `src/services/financial_aid_service/application/use_cases/disburse_stipend_saga.c/h`: Add new step definition and logic to the saga coordinator.\n-   `src/services/bursar_service/domain/account.c/h`: Add logic to store and manage the `autoPayConsent` flag.\n-   `src/services/bursar_service/application/commands/`: New `process_tuition_auto_pay_command.h`.\n-   `src/services/bursar_service/application/use_cases/`: New `process_tuition_auto_pay.c/h` use case handler.\n-   `src/projections_service/event_handlers/dashboard_projector.c`: Add new handler for `TuitionPaymentAppliedFromStipend`.\n-   `src/shared_kernel/domain/events/`: Define new event structs in a new header file.",
      "evaluation_criteria": [
        "**Pattern Adherence:** Correctly identifies and proposes extending the existing Saga pattern, respecting the CQRS/ES flow.",
        "**Component Identification:** Accurately identifies the roles of `financial_aid_service`, `bursar_service`, `projections_service`, and the Saga Coordinator.",
        "**Data Flow Correctness:** Proposes a logical and correct sequence of events, commands, and queries, including the crucial pre-condition check.",
        "**Event Design:** Defines specific and meaningful new domain events appropriate for the feature.",
        "**Compliance Integration:** Explicitly addresses the student consent requirement and correctly places the check in the workflow, referencing the compliance documentation.",
        "**Codebase Awareness:** Demonstrates understanding of the codebase by correctly identifying key files/modules that would need modification (e.g., `disburse_stipend_saga.c`, `dashboard_projector.c`).",
        "**Justification Quality:** Provides clear reasoning for architectural decisions, such as choosing to extend the existing saga versus creating a new one."
      ],
      "language": "c",
      "score": 0.8806
    },
    {
      "id": "c_blockchain_nft_expert_071_bug_investigation_expert_01",
      "task_category": "bug_investigation",
      "difficulty": "expert",
      "title": "Intermittent Consensus Failure on Governance Proposal Transactions",
      "context_length": 1095335,
      "files_count": 82,
      "task_prompt": "Investigate the root cause of the intermittent 'INVALID_BLOCK_SIZE' consensus failures. Your primary objective is to identify the exact file and line(s) of code causing this bug. Your analysis should explain:\n1. Why are blocks being created with an incorrect size?\n2. Why does this issue only manifest when using the `da_strategy` and not the `pos_strategy`?\n3. Why is the bug triggered by `TX_GOVERNANCE_UPDATE_RECIPE` transactions specifically?\n\nYour final output should be a clear explanation of the root cause. You do not need to provide a patch, but describing the necessary fix is a plus.",
      "ground_truth": {
        "vulnerable_file": "HoloCanvas//services//ledger_core//src//block_builder.c",
        "bug_description": "In `block_builder.c`, within the function that adds transactions to a new block, there is a conditional block to handle `TX_GOVERNANCE_UPDATE_RECIPE` transactions. The line that updates the block's size for this transaction's payload is incorrect. It uses `current_block_size += sizeof(transaction->payload);`. This should be `current_block_size += transaction->payload_size;`, which correctly references the size of the dynamically allocated payload from the transaction structure (defined in `transaction.h`). The `sizeof()` operator on the pointer results in adding only 4 or 8 bytes, causing the block builder to believe the block is much smaller than it is, leading it to add more transactions and exceed the protocol's maximum block size limit.",
        "secondary_issue_file": "HoloCanvas//services//ledger_core//src//strategies//pos_strategy.c",
        "secondary_issue_description": "The consensus validation function within `pos_strategy.c` is incomplete. It lacks the rigorous block size check that is correctly implemented in `da_strategy.c`. This discrepancy in validation logic is why the bug in `block_builder.c` is only exposed when the `da_strategy` is active, making the bug appear intermittent and strategy-dependent.",
        "key_files_for_analysis": [
          "HoloCanvas//services//ledger_core//src//block_builder.c",
          "HoloCanvas//services//ledger_core//src//strategies//da_strategy.c",
          "HoloCanvas//services//ledger_core//src//strategies//pos_strategy.c",
          "HoloCanvas//services//ledger_core//include//transaction.h"
        ]
      },
      "evaluation_criteria": [
        "Correctly identifies `block_builder.c` as the file containing the root cause of the invalid block creation.",
        "Correctly pinpoints the specific line of code using `sizeof(pointer)` as the bug.",
        "Accurately explains *why* `sizeof(pointer)` is incorrect in this context (i.e., it measures pointer size, not buffer size).",
        "Correctly identifies the lack of validation in `pos_strategy.c` as the reason for the discrepancy between consensus engines.",
        "Demonstrates an efficient investigation path, starting from the error logs and logically tracing back to the source, without getting sidetracked by irrelevant services like `mint_factory` or `wallet_proxy`.",
        "Provides a clear, coherent, and complete explanation of the entire bug lifecycle, from transaction creation to consensus failure."
      ],
      "language": "c",
      "score": 0.8779
    },
    {
      "id": "cpp_web_blog_expert_040_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Consolidate Dispersed Search Functionality into a Centralized Service",
      "context_length": 975132,
      "files_count": 79,
      "task_prompt": "Your task is to refactor the entire search functionality into a new, self-contained `SearchService` component. This will involve identifying the dispersed code, creating a new class to encapsulate it, and updating the existing modules to use this new service.\n\n**Detailed Requirements:**\n\n1.  **Create New Files:** Create a new directory `src/search/` and add two new files inside it: `SearchService.h` and `SearchService.cpp`.\n\n2.  **Define the `SearchService` Interface:** In `src/search/SearchService.h`, define a class named `SearchService`. This class should expose a clean public interface for all search-related operations. It must include at least these two methods:\n    *   `void indexDocument(int documentId, const std::string& documentContent);`\n    *   `std::vector<int> query(const std::string& searchTerm);`\n\n3.  **Identify and Move Logic:**\n    *   Locate the document indexing logic, which is currently inside `module_18.cpp`. Move this logic into the `indexDocument` method of your new `SearchService` class.\n    *   Locate the core search query processing logic and the underlying inverted index data structure, which are currently in `module_41.cpp` and `module_61.cpp`. Consolidate and move this logic into the `query` method and private members of the `SearchService` class.\n\n4.  **Refactor Call Sites:**\n    *   Modify `module_18.cpp` (where blog posts are created/updated) to no longer perform indexing directly. Instead, it should instantiate or get an instance of `SearchService` and call its `indexDocument` method.\n    *   Modify `module_41.cpp` (which handles the search API endpoint) to use the `SearchService`'s `query` method to get search results.\n\n5.  **Cleanup:** Remove the original, now-redundant search functions, helper functions, and data structure definitions from `module_18.cpp`, `module_41.cpp`, and `module_61.cpp` to eliminate code duplication and finalize the refactoring.",
      "ground_truth": "The expected outcome is a set of changes that centralize the search logic.\n\n**Key Changes:**\n-   **New Files:** `src/search/SearchService.h` and `src/search/SearchService.cpp` exist.\n-   **`SearchService.h`:** Contains the `SearchService` class definition with the specified `indexDocument` and `query` public methods. It also likely contains private member declarations for the inverted index and helper functions.\n-   **`SearchService.cpp`:** Contains the full implementation of the `SearchService` methods, including logic for tokenizing text, updating the inverted index, and searching the index.\n-   **`module_18.cpp`:** The original indexing function(s) are removed. The file now includes `search/SearchService.h` and contains a call to `SearchService::getInstance().indexDocument(...)` or a similar mechanism.\n-   **`module_41.cpp`:** The original query function(s) are removed. The file now includes `search/SearchService.h` and contains a call to `SearchService::getInstance().query(...)` to fetch search results.\n-   **`module_61.cpp`:** The utility functions and data structures related to the search index are removed.\n-   The overall functionality of the blog's search feature remains unchanged from an end-user perspective.",
      "evaluation_criteria": [
        "**Correctness & Compilation:** Does the refactored code compile without errors and warnings?",
        "**Functionality Preservation:** Does the search feature work exactly as it did before the refactoring?",
        "**Abstraction Quality:** Is the `SearchService` interface well-designed and does it properly encapsulate all search-related concerns?",
        "**Code Consolidation:** Was all relevant search logic from the specified modules successfully moved to the new service, with no remnants left behind?",
        "**Call Site Modernization:** Were the call sites in `module_18.cpp` and `module_41.cpp` correctly updated to use the new `SearchService`?",
        "**Code Cleanliness:** Was the old, now-redundant code fully removed from the original modules?",
        "**File & Directory Structure:** Were the new files created in the correct `src/search/` directory as specified?",
        "**Minimality of Changes:** Did the agent refactor only the necessary code without introducing unrelated changes to other files?"
      ],
      "language": "cpp",
      "score": 0.8769
    },
    {
      "id": "typescript_system_monitoring_expert_061_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Centralize Dispersed Alerting Logic into a Unified Notification Service",
      "context_length": 1026021,
      "files_count": 75,
      "task_prompt": "Your task is to refactor the disparate alerting mechanisms scattered across the PulseSphere SocialOps codebase into a new, unified Notification Service.\n\n**Detailed Requirements:**\n\n1.  **Identify Alerting Logic:**\n    - Scan all `src/module_*.ts` files and `src/utils.ts` to locate all instances of alert generation.\n    - Look for calls to the `sendLegacyEmailAlert` function in `src/utils.ts`.\n    - Find any hardcoded `fetch` or `axios` calls to webhook URLs (e.g., containing 'hooks.slack.com' or 'chat.teams.microsoft.com').\n    - Identify critical error logging intended as an alert, typically marked with a comment like `// ALERT: ...` or `// CRITICAL-ALERT: ...`.\n\n2.  **Design and Implement the Notification Service:**\n    - Create a new directory: `src/services`.\n    - Create a new file: `src/services/notificationService.ts`.\n    - In this new file, define the following:\n        - An `AlertMessage` type: `{ severity: 'critical' | 'warning' | 'info'; title: string; details: Record<string, any>; sourceModule: string; }`\n        - A `NotificationChannel` interface with a single method: `send(message: AlertMessage): Promise<void>;`\n        - Concrete classes that implement `NotificationChannel`: `EmailChannel`, `WebhookChannel`, and `LogChannel`.\n        - A primary `NotificationService` class. This class should be initialized with a list of active channels and expose a single public method: `dispatch(alert: AlertMessage): Promise<void>`. The `dispatch` method will iterate over its configured channels and call their `send` methods.\n        - Export a singleton instance of the `NotificationService` for use throughout the application.\n\n3.  **Centralize Configuration:**\n    - Create a new directory: `src/config`.\n    - Create a new file: `src/config/notifications.ts`.\n    - Move all hardcoded configuration details (e.g., recipient email addresses, webhook URLs) from the modules and `utils.ts` into this new config file.\n    - The `NotificationService` should import its configuration from this file to initialize its channels.\n\n4.  **Refactor Existing Modules:**\n    - Go through each module where you identified alerting logic.\n    - Replace the old, direct implementation with a call to the new `notificationService.dispatch()` method.\n    - Ensure you correctly construct the `AlertMessage` object using the context available at the call site.\n\n5.  **Code Cleanup:**\n    - Once all modules are refactored, remove the now-redundant `sendLegacyEmailAlert` function and any related helper types from `src/utils.ts`.\n    - Ensure no unused imports remain in the refactored files.",
      "ground_truth": "The expected outcome is a codebase where all alerting logic is handled by a single service.\n\n**Key Changes:**\n\n1.  **New Files Created:**\n    - `src/services/notificationService.ts`: Contains the `NotificationService` class, `NotificationChannel` interface, `AlertMessage` type, and concrete channel implementations (`EmailChannel`, `WebhookChannel`, `LogChannel`). Exports a singleton `notificationService`.\n    - `src/config/notifications.ts`: Exports a `notificationConfig` object containing settings like `emailRecipients`, `webhookUrl`, etc.\n\n2.  **Modified `utils.ts`:**\n    - The function `sendLegacyEmailAlert(to: string, subject: string, body: string): Promise<void>` and any related types have been completely removed.\n\n3.  **Example Module Refactoring (e.g., in `src/module_63.ts`):**\n\n    *   **Before:**\n        ```typescript\n        import { sendLegacyEmailAlert } from '../utils';\n        // ...\n        async function checkBackupIntegrity(backupId: string) {\n          const isCorrupt = await someCheck(backupId);\n          if (isCorrupt) {\n            console.error(`CRITICAL-ALERT: Backup ${backupId} is corrupt!`);\n            await sendLegacyEmailAlert('sysops@pulsesphere.com', 'Corrupt Backup Detected', `Backup with ID ${backupId} failed integrity check.`);\n          }\n        }\n        ```\n\n    *   **After:**\n        ```typescript\n        import { notificationService } from '../services/notificationService';\n        // ...\n        async function checkBackupIntegrity(backupId: string) {\n          const isCorrupt = await someCheck(backupId);\n          if (isCorrupt) {\n            await notificationService.dispatch({\n              severity: 'critical',\n              title: 'Corrupt Backup Detected',\n              sourceModule: 'module_63',\n              details: {\n                backupId: backupId,\n                checkTime: new Date().toISOString(),\n                reason: 'Failed integrity check.'\n              }\n            });\n          }\n        }\n        ```\n\n4.  **All other modules containing alert logic** (e.g., `module_30`, `module_50`, `module_70`) will exhibit similar refactoring patterns, replacing their unique alert calls with a standardized call to `notificationService.dispatch()`.",
      "evaluation_criteria": [
        "**Correctness & Compilation:** The final code must compile successfully without any TypeScript errors.",
        "**Completeness of Refactoring:** All instances of legacy alerting (email, webhooks, tagged logs) must be identified and replaced with the new service call.",
        "**Quality of Abstraction:** The new `NotificationService` and `NotificationChannel` interface must be well-designed, extensible, and follow standard object-oriented principles.",
        "**Configuration Centralization:** All hardcoded configuration values (emails, URLs) must be successfully moved to `src/config/notifications.ts` and used by the service.",
        "**Code Removal (Cleanup):** The old `sendLegacyEmailAlert` function in `src/utils.ts` must be successfully removed. No dead code should remain.",
        "**Functional Equivalence:** The refactoring must not alter the core business logic of the modules. The conditions that trigger alerts should remain identical.",
        "**Modularity and Imports:** The agent must correctly manage imports/exports for the new services and update them in all refactored modules."
      ],
      "language": "typescript",
      "score": 0.8765
    },
    {
      "id": "csharp_ml_training_expert_087_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Refactoring: Decompose Monitoring Module into a Microservice",
      "context_length": 812061,
      "files_count": 79,
      "task_prompt": "You are a senior software architect. Your task is to create a detailed architectural refactoring plan to extract the `CanvasCraft.Monitoring` project into a new, standalone microservice. Your plan must be based on a thorough analysis of the existing codebase and architectural documents.\n\nYour final output should be a technical plan that addresses the following points:\n\n1.  **Analysis of Current State:** Briefly describe how the monitoring system currently works within the modular monolith. Identify the key components involved and their interaction patterns (e.g., how monitoring is triggered and how its results are consumed).\n\n2.  **Proposed Microservice Architecture:**\n    a.  **Communication Strategy:** Define how the new `MonitoringService` will communicate with the main `CanvasCraft.Api` application. Justify your choice of communication pattern (e.g., synchronous vs. asynchronous) by referencing existing patterns in the codebase.\n    b.  **Data Flow:** Detail the flow of data and events between the main application and the new microservice. Specifically, explain what triggers the monitoring logic and how the results (e.g., a detected drift) are communicated back to the system to trigger actions like automated retraining.\n\n3.  **Implementation & Refactoring Plan:**\n    a.  **Code Changes:** List the primary projects and specific files/classes in the existing solution that will require modification. Describe the nature of the changes needed (e.g., removal of code, modification of a service client).\n    b.  **New Components:** Describe the high-level structure of the new `MonitoringService`, including its API contract if it requires one.\n    c.  **Deployment:** Explain how the `docker-compose.yml` file should be updated to include the new microservice for local development and testing.",
      "ground_truth": "### 1. Analysis of Current State\nThe current system is a modular monolith. The `ServingController` handles prediction requests and publishes a `ModelPredictionQueriedEvent` to a RabbitMQ message bus. Within the same process, the `CanvasCraft.Monitoring` module runs as a background service, subscribed to this message bus. It uses an internal Observer pattern (`ModelServingSubject` notifying `DataDriftObserver` and `PerformanceFadeObserver`). When these observers detect an issue, they publish a new event (e.g., `RetrainingRequiredEvent`) back to RabbitMQ. The `AutomatedRetrainingTrigger` in the `CanvasCraft.Pipeline` module subscribes to this event to initiate a new training pipeline.\n\n### 2. Proposed Microservice Architecture\n\na. **Communication Strategy:** The communication will be fully asynchronous using the existing RabbitMQ message bus. This aligns with `ADR-002` and ensures loose coupling and independent scalability. The main application will be the producer of prediction events, and the new `MonitoringService` will be the consumer. The `MonitoringService` will, in turn, become a producer of `RetrainingRequiredEvent`s.\n\nb. **Data Flow:**\n   1. `CanvasCraft.Api` receives a prediction request and publishes `ModelPredictionQueriedEvent` to a RabbitMQ topic.\n   2. The new standalone `MonitoringService` is the sole subscriber to this topic.\n   3. The `MonitoringService` processes the event, updates its internal state, and runs its drift/fade detection logic.\n   4. If a significant event is detected, the `MonitoringService` publishes a `RetrainingRequiredEvent` to a different RabbitMQ topic.\n   5. The `AutomatedRetrainingTrigger` within the main application's `CanvasCraft.Pipeline` module subscribes to this `RetrainingRequiredEvent` topic and initiates the retraining pipeline.\n\n### 3. Implementation & Refactoring Plan\n\na. **Code Changes:**\n   - **`CanvasCraft.Api/Startup.cs`**: Remove dependency injection registration for all services from the `CanvasCraft.Monitoring` project.\n   - **`CanvasCraft.sln` & Project Files**: Remove the `CanvasCraft.Monitoring` project reference from the solution and any project that references it.\n   - **`CanvasCraft.Pipeline/Triggers/AutomatedRetrainingTrigger.cs`**: Ensure its message bus subscription logic is robust enough to handle events from an external service. No significant change is likely needed if it's already using `IMessageBus`.\n   - **`CanvasCraft.Api/Controllers/MonitoringController.cs`**: This controller, if used for querying monitoring status, must be refactored. Its GET endpoints should be moved to the new microservice. The main API could optionally act as a proxy to the new service's API for a unified front-end experience.\n\nb. **New Components:**\n   - A new C# solution and project for `MonitoringService` will be created, containing all the code from the original `CanvasCraft.Monitoring` project.\n   - It will have its own `Program.cs`/`Startup.cs` to register its services and configure the RabbitMQ listener.\n   - It will expose a minimal REST API for health checks and to allow manual querying of drift/performance status.\n\nc. **Deployment (`docker-compose.yml`):**\n   A new service definition must be added:\n   ```yaml\n   services:\n     # ... existing services (api, db, rabbitmq)\n     monitoring-service:\n       image: canvascraft-monitoring-service\n       build:\n         context: .\n         dockerfile: src/MonitoringService/Dockerfile # New Dockerfile for the service\n       depends_on:\n         - rabbitmq\n         - db\n       environment:\n         - RabbitMq__HostName=rabbitmq\n         - ConnectionStrings__DefaultConnection=...\n       networks:\n         - canvascraft-net\n   ```",
      "evaluation_criteria": [
        "Correctly identifies the existing architecture as a modular monolith with event-driven communication via RabbitMQ.",
        "Accurately describes the current data flow from API event publication to monitoring consumption and subsequent trigger activation.",
        "Proposes a viable, asynchronous communication pattern for the new microservice that leverages existing infrastructure (RabbitMQ).",
        "Clearly defines the responsibilities of both the main application and the new microservice in the proposed architecture.",
        "Correctly identifies the full-circle event path, including how monitoring results are communicated back to trigger retraining.",
        "Lists the key files/areas for modification, including `Startup.cs` (DI removal), project references, and `docker-compose.yml`.",
        "Demonstrates an understanding of microservice principles by creating a plan that promotes loose coupling and independent deployment."
      ],
      "language": "csharp",
      "score": 0.875
    },
    {
      "id": "java_api_rest_expert_006_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Architectural Analysis for a New Cross-Service Orchestration Feature",
      "context_length": 778633,
      "files_count": 80,
      "task_prompt": "You are a senior architect tasked with designing a solution for a new 'Data Processing Pipeline' feature. This feature must allow users to submit a job that executes a sequence of operations across multiple services. For example, a pipeline might first anonymize a dataset using the `data-anonymizer-service` and then convert the resulting data into a different format using the `file-converter-service`.\n\nYour task is to produce a high-level architectural proposal. You are not required to write the full implementation, but you must demonstrate a deep understanding of the existing system to ensure your proposal is consistent and viable.\n\nYour analysis must include:\n1.  A brief summary of the current architecture's key components and communication patterns.\n2.  A proposal for implementing the new 'Data Processing Pipeline' feature. Your proposal should address where the orchestration logic should live (e.g., in a new service or an existing one) and justify your choice by analyzing architectural trade-offs (e.g., Orchestration vs. Choreography).\n3.  A detailed description of the data flow and component interactions for a sample pipeline job (e.g., Anonymize then Convert). This should cover the initial request, state management, inter-service communication, and how the final result is made available to the user.\n4.  A list of the key new modules/files that would need to be created and a list of existing files that would require significant modification to implement your proposed design.",
      "ground_truth": "The optimal solution is to introduce a new microservice dedicated to orchestration, adhering to the existing architectural principles.\n\n**1. Current Architecture Summary:** The system is a Java/Spring Boot-based microservices architecture. It uses Spring Cloud for an API Gateway (`api-gateway`), Service Discovery (`service-discovery`), and Centralized Configuration (`config-server`). Services are designed with a Hexagonal Architecture, separating domain logic from external-facing adapters (web, persistence, messaging). Communication is primarily synchronous REST via the gateway, with some asynchronous capabilities demonstrated by messaging listeners (e.g., `FileConversionMessageListener`).\n\n**2. Proposed Solution: A New `pipeline-orchestrator-service`:**\nThe best approach is to create a new `pipeline-orchestrator-service`. This centralizes the complex workflow logic, making it explicit and manageable, which is preferable to a fragile, hard-to-trace event-driven choreography for this use case. This new service will be responsible for accepting a pipeline definition, executing the steps by calling the appropriate services, managing the state of the pipeline, and handling errors.\n\n**3. Data Flow for 'Anonymize then Convert' Pipeline:**\n    a.  A client sends a POST request to `/api/v1/pipelines` via the `api-gateway` with a body specifying the sequence of tasks (e.g., `[{ \"service\": \"anonymizer\", \"params\": {...} }, { \"service\": \"converter\", \"params\": {...} }]`) and the initial data.\n    b.  The `api-gateway` routes the request to the new `pipeline-orchestrator-service`.\n    c.  The orchestrator service creates a new pipeline job record in its database with a `PENDING` status, and immediately returns a `202 Accepted` response with a `jobId` to the client.\n    d.  Asynchronously, the orchestrator begins processing. It calls the `data-anonymizer-service` via a REST client (looking it up via service discovery).\n    e.  Upon successful completion, it takes the output and calls the `file-converter-service`.\n    f.  If any step fails, the orchestrator updates the job status to `FAILED` and stores the error details. If all steps succeed, it updates the status to `COMPLETED` and stores the final result's location.\n    g.  The client can poll the status endpoint `/api/v1/pipelines/{jobId}` to check progress and retrieve the final result upon completion.\n\n**4. Required Changes and New Files:**\n    *   **New Module:**\n        *   `opsforge-utility-nexus/utility-services/pipeline-orchestrator-service/`\n    *   **Key New Files:**\n        *   `.../pipeline-orchestrator-service/pom.xml` (dependencies on `common-library`, `spring-cloud-starter-netflix-eureka-client`, `spring-boot-starter-web`, `spring-boot-starter-data-jpa`).\n        *   `.../pipeline-orchestrator-service/src/main/java/.../PipelineOrchestratorApplication.java`\n        *   `.../adapter/in/web/PipelineController.java` (to handle `/pipelines` requests).\n        *   `.../domain/model/PipelineJob.java` (to model the state, steps, and status).\n        *   `.../domain/service/PipelineService.java` (implements `PipelineUseCase` port, contains core orchestration logic).\n        *   `.../domain/port/in/PipelineUseCase.java`\n        *   `.../domain/port/out/AnonymizerServicePort.java`, `.../domain/port/out/FileConverterServicePort.java` (output ports for communicating with other services).\n        *   `.../adapter/out/rest/AnonymizerServiceAdapter.java` (implements the port using a `RestTemplate` or `WebClient`).\n    *   **Existing Files to Modify:**\n        *   `opsforge-utility-nexus/pom.xml`: Add the new `pipeline-orchestrator-service` module.\n        *   `opsforge-utility-nexus/api-gateway/src/main/java/com/opsforge/nexus/gateway/config/GatewayRouteConfig.java`: Add a new route definition to direct traffic from `/api/v1/pipelines/**` to the `pipeline-orchestrator-service` (lb://pipeline-orchestrator-service).\n        *   `opsforge-utility-nexus/docker-compose.yml`: Add a service definition for the new orchestrator.",
      "evaluation_criteria": [
        "**Component Identification:** Correctly identifies the roles of the API Gateway, Service Discovery, Config Server, and individual microservices.",
        "**Pattern Recognition:** Explicitly recognizes and references the Microservices and Hexagonal Architecture patterns guiding the system's design.",
        "**Architectural Trade-off Analysis:** Accurately discusses the pros and cons of Orchestration vs. Choreography in the context of this specific problem.",
        "**Solution Cohesion:** Proposes a solution (the new service) that aligns with the existing architectural patterns (Hexagonal, uses discovery/config) rather than introducing a conflicting design.",
        "**Impact Analysis:** Correctly identifies the need for a new service module and pinpoints the key existing files (`pom.xml`, `GatewayRouteConfig.java`, `docker-compose.yml`) that must be modified.",
        "**Flow Description:** Provides a clear, logical, and technically sound description of the end-to-end data flow for the new feature, including its asynchronous nature."
      ],
      "language": "java",
      "score": 0.8698
    },
    {
      "id": "python_desktop_development_expert_021_cross_file_refactoring_expert_01",
      "task_category": "cross_file_refactoring",
      "difficulty": "expert",
      "title": "Refactor User State Management by Unifying Profile and Presence Services",
      "context_length": 1026075,
      "files_count": 73,
      "task_prompt": "Your task is to refactor the user management system by unifying `ProfileService` and `PresenceService`.\n\n1.  **Create a New Service:** Create a new file at `flockdesk/core/services/user_service.py`. Inside this file, define a new `UserService` class. This class will be the new single source of truth for all user data.\n\n2.  **Consolidate Logic:** Migrate all functionality from `flockdesk/core/services/profile_service.py` and `flockdesk/modules/presence/service.py` into the new `UserService`. The new service should manage both static profile data (e.g., from `shared/schemas/user_profile.py`) and dynamic status data (e.g., from `modules/presence/model/user_status.py`). Design a unified API, for example, a method `get_user_view(user_id)` that returns a combined object/dictionary with the user's name, avatar, and current online status.\n\n3.  **Update Consumers:** Systematically refactor all application components that currently depend on `ProfileService` or `PresenceService`. They must now import and use the new `UserService`. Key files to investigate include, but are not limited to:\n    *   `flockdesk/modules/chat/viewmodel/chat_vm.py`\n    *   `flockdesk/modules/presence/viewmodel/presence_vm.py`\n    *   `flockdesk/modules/presence/view/presence_widget.py`\n    *   `flockdesk/shared/widgets/avatar_widget.py`\n    *   `flockdesk/core/app.py` (for service initialization)\n\n4.  **Update Tests:** Adapt the existing tests to reflect this new architecture. You may need to merge tests from `tests/integration/test_profile_sync.py` and create a new test file like `tests/unit/core/test_user_service.py`. Ensure the core functionality remains tested.\n\n5.  **Cleanup:** Once all references have been updated and the application is stable, delete the now-redundant files: `flockdesk/core/services/profile_service.py` and `flockdesk/modules/presence/service.py`.",
      "ground_truth": "The final state of the codebase should reflect a successful consolidation:\n\n*   **File Creation:** The file `flockdesk/core/services/user_service.py` exists and contains the `UserService` class.\n*   **File Deletion:** The files `flockdesk/core/services/profile_service.py` and `flockdesk/modules/presence/service.py` have been deleted from the project.\n*   **Code Modification:**\n    *   The new `UserService` class integrates methods for fetching profile data and managing real-time presence.\n    *   Files like `chat_vm.py`, `presence_vm.py`, `presence_widget.py`, and `avatar_widget.py` have been modified to import and use `UserService` exclusively for user data needs.\n    *   The service initialization/registration logic in `flockdesk/core/app.py` (or equivalent) has been updated to remove `ProfileService` and `PresenceService` and add `UserService`.\n*   **Test Modification:**\n    *   A new test file, `tests/unit/core/test_user_service.py` (or similar), exists and provides coverage for the new service.\n    *   Existing tests that depended on the old services have been updated or removed. For instance, `tests/integration/test_profile_sync.py` is either deleted or refactored to test the new service.",
      "evaluation_criteria": [
        "**Correctness & Functionality:** The application must compile and run without errors. All user-related features (displaying avatars, names, online statuses) must work as before. All tests in the suite must pass.",
        "**Completeness of Refactoring:** All usages of `ProfileService` and `PresenceService` must be successfully replaced with `UserService`. The two old service files must be deleted.",
        "**Architectural Soundness:** The new `UserService` must be well-designed, providing a clean and unified API that effectively encapsulates the combined responsibilities of the former services.",
        "**Code Quality:** The new and modified code must adhere to existing project conventions, be readable, and not introduce new bugs or code smells.",
        "**Test Maintenance:** The agent must demonstrate an ability to manage the test suite by creating, modifying, and deleting test files and test cases as appropriate for the refactoring.",
        "**File System Operations:** The agent must correctly create the new service file in the specified location and correctly delete the old files without leaving orphans or deleting incorrect files."
      ],
      "language": "python",
      "score": 0.8698
    },
    {
      "id": "javascript_blockchain_nft_expert_035_architectural_understanding_expert_01",
      "task_category": "architectural_understanding",
      "difficulty": "expert",
      "title": "Analyze and Document the Smart Contract Upgrade Procedure and its Off-Chain Impact",
      "context_length": 737434,
      "files_count": 83,
      "task_prompt": "As the lead architect, you must produce a technical brief for the engineering team. Your analysis must address the following points:\n\n1.  **Identify the Upgrade Mechanism:** Based on the provided files (`contracts/`, `docs/`), determine and explain the specific smart contract upgradeability pattern being used for `ShowPass.sol`.\n\n2.  **Pinpoint Off-Chain Dependencies:** Identify all the critical files and components within the `packages/backend` service that are tightly coupled to the `ShowPass.sol` contract's Application Binary Interface (ABI) and storage layout. Explain *why* these components are dependent.\n\n3.  **Outline the Upgrade Sequence:** Create a detailed, ordered list of operations required to safely deploy an updated `ShowPass` contract logic. This sequence must cover both on-chain actions (e.g., deploying the new implementation, calling the proxy) and off-chain actions (e.g., updating backend configurations, service redeployment).\n\n4.  **Assess Risks:** Identify at least two potential risks associated with this upgrade process (e.g., data corruption, service downtime, inconsistent state) and propose a mitigation strategy for each.",
      "ground_truth": "### Key Insights\n\n1.  **Upgrade Mechanism:** The project uses the **UUPS (Universal Upgradeable Proxy Standard)**. This is evident from `ShowPassProxy.sol` which points to a logic implementation, and the presence of an `upgradeTo` function (or similar) managed by an admin or governance mechanism (`CarnivalGovernance.sol`). The `UPGRADEABILITY_GUIDE.md` document explicitly details this pattern.\n\n2.  **Off-Chain Dependencies:**\n    *   `packages/backend/src/infrastructure/adapters/blockchain/ethers.service.ts`: This service holds the primary dependency. It instantiates an `ethers.Contract` object using the `ShowPass` address and its ABI. Any change to the contract's functions (add, remove, modify signature) requires updating the ABI file used by this service and potentially the code that calls the functions.\n    *   `packages/backend/src/infrastructure/adapters/blockchain/contract.mapper.ts`: This mapper translates data from contract events and function return values into the backend's domain entities. If the upgrade changes event signatures or data structures, this file must be updated to prevent data mapping errors.\n    *   `packages/backend/src/application/observers/stage-event.observer.ts`: This observer listens for on-chain events. If the upgrade introduces new events or alters existing ones, this observer must be updated to handle them correctly.\n    *   **Implicit Dependency:** The compiled ABI JSON file (e.g., `ShowPass.json`), which is a build artifact consumed by the backend services. This file is the source of truth for the contract interface.\n\n3.  **Upgrade Sequence:**\n    1.  Develop and thoroughly test the new `ShowPassV2.sol` implementation contract.\n    2.  Deploy the new `ShowPassV2.sol` contract to the blockchain. This yields a new implementation address.\n    3.  Prepare the backend for the switch: Deploy a new version of the backend services with the updated ABI and any necessary logic changes to a staging environment or as a 'dark' deployment. The new services should not be active yet.\n    4.  Execute the upgrade transaction: Call the `upgradeTo(new_implementation_address)` function on the `ShowPassProxy` contract via a governance proposal or an admin key.\n    5.  Immediately after the on-chain transaction is confirmed, switch traffic to the new backend instances that are aware of the new ABI. This minimizes the window of inconsistency.\n    6.  Monitor all systems (backend logs, on-chain transaction monitoring) for errors.\n\n4.  **Risks and Mitigations:**\n    *   **Risk:** **Storage Layout Collision.** If the new implementation contract changes the order of state variables or introduces new ones incorrectly, it can corrupt the proxy's storage. \n        *   **Mitigation:** Use a linter/tool like `hardhat-upgrades` to validate storage layout compatibility between the old and new implementations during development. Follow best practices by only appending new state variables and never changing the order or type of existing ones.\n    *   **Risk:** **Backend/Contract Inconsistency.** For a brief period after the `upgradeTo` call and before the backend services are updated, the backend might attempt to call the proxy using an old ABI, leading to failed transactions.\n        *   **Mitigation:** Implement a 'maintenance mode' in the backend, activated just before the upgrade, that temporarily pauses all interactions with the `ShowPass` contract. Alternatively, use a blue-green deployment strategy for the backend, where the switch to the new services is timed to coincide exactly with the on-chain upgrade confirmation.",
      "evaluation_criteria": [
        "Correctly identifies the UUPS proxy pattern as the upgrade mechanism.",
        "Accurately lists the key backend files (`ethers.service.ts`, `contract.mapper.ts`, `stage-event.observer.ts`) and explains their dependency on the contract's ABI.",
        "Provides a logical and safe sequence of operations for the upgrade, correctly ordering on-chain and off-chain actions.",
        "Demonstrates a clear understanding of the tight coupling between on-chain contract logic and off-chain service code.",
        "Identifies relevant, expert-level risks such as storage layout collision, not just generic deployment issues.",
        "Proposes feasible and specific mitigation strategies for the identified risks."
      ],
      "language": "javascript",
      "score": 0.8693
    }
  ]
}