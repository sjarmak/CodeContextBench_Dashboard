# Ralph Progress Log
Started: Mon Feb  2 16:01:56 EST 2026

## Codebase Patterns
- Trial directories are identified by the presence of `result.json`; this is the primary discovery criterion
- Runs directory structure: `runs_dir / category(official|experiment|troubleshooting) / experiment / config_type / timestamp / trial_dir/`
- Harbor result.json timing fields are top-level (`started_at`, `finished_at`), agent timing in `agent_execution` dict
- Token counts come from `task_metrics.json` (preferred) or `result.json.agent_result.n_input_tokens` etc.
- Reward comes from `verifier/reward.txt` (preferred) or `result.json.verifier_result.rewards.reward`
- `dict.get(key, default)` does NOT protect against `None` values -- always use `or default` pattern
- All pipeline dataclasses must be `frozen=True` for immutability; use tuples not lists
- Pre-existing mypy errors in cost_calculator.py, task_schema.py, orchestrator.py are unrelated to pipeline work
- Test fixtures use `tmp_path` and helper functions like `_make_result_json()` for building trial structures
- `analyze._flatten_trials()` converts nested categories/experiments/trials to flat list; reuse in downstream steps
- Statistical tests: Welch's t-test for continuous, proportion z-test for rates; Cohen's d for effect sizes
- numpy bools (`np.True_`) fail `is True` assertions; always wrap with `bool()` when constructing frozen dataclasses from scipy/numpy results
- Variable name shadowing causes mypy errors in nested loops; use distinct names (e.g., `phase_trials` vs `config_trials`)
- `_group_by_benchmark()` and `_get_trial_benchmark()` are reusable helpers for any per-benchmark analysis step

---

## 2026-02-02 - US-003
- Implemented `scripts/ccb_pipeline/extract.py` with `__main__` entry point
- Created `scripts/ccb_pipeline/__init__.py` package
- Files created: `scripts/ccb_pipeline/__init__.py`, `scripts/ccb_pipeline/extract.py`, `tests/test_extract.py`
- Walks runs directory recursively finding trial directories (via result.json presence)
- Extracts: reward (reward.txt > result.json), timing (agent_execution > top-level), tokens (task_metrics.json > result.json), benchmark name, config detection (US-001), tool utilization (US-002)
- CLI: `python -m scripts.ccb_pipeline.extract --runs-dir <path> --output <path>`
- 42 unit tests, all passing; no typecheck errors in extract.py
- **Learnings for future iterations:**
  - Benchmark name can be extracted from task_metrics.json `benchmark` field or config.json `task.path` after `benchmarks/` segment
  - Task name resolution: result.json `task_name` field is most reliable, then config.json `task.path` leaf name, then directory name
  - The `_find_trials()` function recursively walks until it finds result.json; this handles variable nesting depths across different experiment types
---

## 2026-02-02 - US-004
- Implemented `scripts/ccb_pipeline/sdlc_mapping.py` with benchmark-to-SDLC phase mapping
- Files created: `scripts/ccb_pipeline/sdlc_mapping.py`, `tests/test_sdlc_mapping.py`
- `BENCHMARK_SDLC_MAP` dict covers all Table 2 benchmarks plus project-specific ones (big_code_mcp, github_mined, tac_mcp_value)
- `get_sdlc_phases()` uses fuzzy substring matching, sorted by pattern length descending for specificity
- `get_benchmark_name()` extracts benchmark from task paths by finding segment after "benchmarks/"
- 36 unit tests, all passing; mypy clean
- **Learnings for future iterations:**
  - Fuzzy matching sorted by pattern length descending ensures "swe-bench-pro" matches before "swe-bench"
  - `PurePosixPath` from pathlib handles cross-platform path parsing without OS dependency
  - SDLC phases stored as tuples in the map for immutability; `get_sdlc_phases()` returns a new list each call
---

## 2026-02-02 - US-005
- Implemented `scripts/ccb_pipeline/analyze.py` with `__main__` entry point
- Files created: `scripts/ccb_pipeline/analyze.py`, `tests/test_analyze.py`
- Consumes experiment_metrics.json from US-003 extract step
- Computes per-configuration aggregate metrics: mean reward (with SE), pass rate (with SE), median duration, mean input/output tokens
- Pairwise statistical tests for all config pairs: Welch's t-test for continuous metrics (reward, duration, input_tokens), proportion z-test for pass rate
- Cohen's d effect sizes for continuous metrics with interpretation (negligible/small/medium/large)
- Outputs analysis_results.json with sections: aggregate_metrics, pairwise_tests, effect_sizes
- CLI: `python -m scripts.ccb_pipeline.analyze --input experiment_metrics.json --output analysis_results.json`
- 46 unit tests, all passing; mypy clean
- **Learnings for future iterations:**
  - `scipy.stats.ttest_ind` with `equal_var=False` gives Welch's t-test (more robust for unequal variances)
  - Proportion z-test uses pooled proportion for standard error calculation
  - Cohen's d uses pooled standard deviation; interpretation thresholds: 0.2 small, 0.5 medium, 0.8 large
  - scipy may warn about precision loss with nearly-identical data; this is expected in tests with synthetic data
  - The `_flatten_trials()` helper is reusable for any step that needs to iterate all trials regardless of category/experiment nesting
---

## 2026-02-02 - US-006
- Added per-benchmark and per-SDLC-phase breakdown to `scripts/ccb_pipeline/analyze.py`
- Files modified: `scripts/ccb_pipeline/analyze.py`, `tests/test_analyze.py`
- New dataclasses: `BenchmarkConfigMetrics`, `BenchmarkSignificanceResult`, `SDLCPhaseMetrics`
- New functions: `_get_trial_benchmark()`, `_group_by_benchmark()`, `compute_per_benchmark_metrics()`, `compute_per_benchmark_significance()`, `_is_mcp_improvement()`, `compute_per_sdlc_phase_metrics()`
- Per-benchmark: computes pass rate and mean reward per config per benchmark, runs pairwise significance tests (z-test for pass rate, Welch's t-test for reward) per benchmark
- Per-SDLC-phase: maps benchmarks to SDLC phases via `get_sdlc_phases()`, aggregates trials across benchmarks sharing a phase, computes reward delta vs BASELINE
- Flags `mcp_improves=True` only when both significant (p<0.05) AND the MCP config outperforms
- Results added to analysis_results.json under `per_benchmark`, `per_benchmark_significance`, `per_sdlc_phase` sections
- 71 tests passing (25 new), mypy clean
- **Learnings for future iterations:**
  - numpy bools (`np.True_`) fail Python `is True` checks; always `bool()` wrap values from scipy/numpy before storing in frozen dataclasses
  - Variable name shadowing across loop scopes causes mypy type confusion; use distinct names in nested for-loops
  - Trials can map to multiple SDLC phases (e.g., swe-bench-pro -> Implementation + Testing); the phase grouping correctly counts them in each phase
  - `_is_mcp_improvement()` uses an ordinal ranking (BASELINE=0, MCP_BASE=1, MCP_FULL=2) to determine which config is "better"
---

## 2026-02-02 - US-007
- Added efficiency analysis to `scripts/ccb_pipeline/analyze.py`
- Files modified: `scripts/ccb_pipeline/analyze.py`, `tests/test_analyze.py`
- New dataclass: `EfficiencyMetrics` (frozen) with 14 fields covering token totals, means with SE, ratios, and overhead
- New function: `compute_efficiency_metrics()` computes per-config token consumption, input-to-output ratio, median wall-clock time, tokens-per-success, and MCP token overhead vs BASELINE
- MCP token overhead = mean total tokens for MCP config minus mean total tokens for BASELINE
- Tokens-per-success = total tokens (input + output) / number of passing tasks; None if no passing tasks
- Results added to analysis_results.json under `efficiency` section
- 89 tests passing (18 new), mypy clean
- **Learnings for future iterations:**
  - `tokens_per_success` uses total tokens (input + output) divided by pass count; cached tokens are informational only (subset of input)
  - MCP token overhead is computed as delta of mean totals, not delta of totals, to normalize for different trial counts
  - The `or 0` pattern for token fields handles both missing keys and None values from dict.get()
---
