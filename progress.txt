# Ralph Progress Log
Started: Mon Jan 27 13:12:00 EST 2026

## Codebase Patterns
- Dashboard framework: Streamlit with Plotly charts, Pandas for data processing
- Dashboard entry point: dashboard/app.py with views in dashboard/views/
- Data ingestion: src/ingest/ (transcript_parser.py, harbor_parser.py, database.py)
- Analysis modules: src/analysis/ (8 analyzers: statistical, comparator, cost, failure, time_series, ir, llm_judge, recommendation)
- Benchmark management: src/benchmark/ (database.py, llm_judge.py, trace_parser.py)
- Trace viewer parser: src/ingest/trace_viewer_parser.py - JSONL-native parser for claude-code.txt (parse_trace, compute_summary)
- Config files: configs/ directory with YAML and JSON formats
- claude-code.txt is JSONL format (one JSON object per line), NOT plain text
- Token usage embedded in assistant message usage field (input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens)
- parent_tool_use_id links sub-agent tool calls to parent for hierarchical traces
- Experiment runs have manifest.json (metadata), index.json (task-to-run mapping), runs/ and pairs/ subdirs
- Task config.json contains agent info (model, import_path) and task info (path, git_url, source)
- Result.json contains verifier_result.rewards, agent_result (tokens, cost), timing data
- LoCoBench task IDs follow: {language}_{domain}_{complexity}_{num}_{task_category}_{difficulty}_{variant}
- SWE-Bench Pro task IDs follow: instance_{org}__{repo}__{hash}
- Runs directory path: ~/evals/custom_agents/agents/claudecode/runs/
- Environment variable for runs dir: CCB_EXTERNAL_RUNS_DIR (was CCB_EXTERNAL_JOBS_DIR)
- Archive dir env var: CCB_EXTERNAL_ARCHIVE_DIR
- Run output default path: runs/{run_id} (was jobs/{run_id})
- Task detail panel lives in dashboard/utils/task_detail.py - reuse render_task_detail_panel() for any task detail view
- result.json structure: verifier_result.rewards (reward breakdown), agent_result (tokens, exit_code, cost), timing phases (environment_setup, agent_setup, agent_execution, verifier)
- Two task detail paths: show_paired_task_detail (paired experiments) and show_external_task_detail (single experiments) - both now use shared component
- verifier/output.json in instance_dir contains additional verifier data that supplements result.json
---

## 2026-01-27 - US-001: Update data source path from jobs/ to runs/
- What was implemented: Migrated all data source path references from `jobs/` to `runs/` across the entire codebase
- Files changed:
  - dashboard/views/run_results.py - EXTERNAL_JOBS_DIR → EXTERNAL_RUNS_DIR, CCB_EXTERNAL_JOBS_DIR → CCB_EXTERNAL_RUNS_DIR, jobs_dir param → runs_dir, fallback path updated
  - dashboard/views/analysis_hub.py - Same env var and variable rename, user-facing messages updated
  - dashboard/views/analysis_llm_judge.py - Same env var and variable rename
  - dashboard/app.py - Help text updated from jobs to runs
  - src/benchmark/run_orchestrator.py - Default output_dir changed from jobs/{run_id} to runs/{run_id} (two locations)
  - src/benchmark/log_streamer.py - Default job_dir path updated
  - src/benchmark/profile_runner.py - Default jobs_root updated to runs/benchmark_profiles
  - src/benchmark/oracle_validator.py - Result pattern updated to runs/
  - src/benchmark/metrics_extractor.py - Docstring examples updated
  - src/ingest/orchestrator.py - Path pattern comment and check updated
  - src/sync/pull.py - Docstring updated
  - cli/main.py - VM path changed from /jobs to /runs
  - scripts/analyze_evals_v2.py - jobs subdirectory → runs subdirectory
  - scripts/test_pipeline_e2e.py - evals_base path updated
  - scripts/import_swebench_to_dashboard.py - Default evals-dir updated
  - scripts/analyze_time_allocation.py, comprehension_analysis_v2.py, comprehension_success_analysis.py, extract_enterprise_metrics.py - Historical script paths updated
  - tests/test_benchmark_profile_runner.py - Test config updated
- **Learnings for future iterations:**
  - The env variable CCB_EXTERNAL_RUNS_DIR controls where the dashboard looks for experiment data
  - Three dashboard views (run_results, analysis_hub, analysis_llm_judge) each independently define EXTERNAL_RUNS_DIR - consider centralizing
  - archive/ and artifacts/ directories contain copies of code that were NOT updated (they're historical snapshots)
  - The `eval_runs_v2/` path was only in documentation (EVAL_FRAMEWORK.md, GCP_BENCHMARK_EXECUTION.md), not in Python code
  - ruff is available for linting; pre-existing issues exist but should not be introduced by changes
---

## 2026-01-27 - US-007: Task detail view - metadata panel
- What was implemented: Created a reusable task detail metadata panel component with 5 collapsible sections displayed via st.expander
- Files changed:
  - dashboard/utils/task_detail.py (NEW) - Reusable component with render_task_detail_panel() function, helper functions for timing, tokens, build env, verifier output extraction
  - dashboard/views/run_results.py - Imported task_detail module, updated show_external_task_detail() and show_paired_task_detail() to use shared render_task_detail_panel()
  - scripts/ralph/prd.json - Marked US-007 passes: true
  - prd.json - Marked US-007 passes: true
  - progress.txt - Added codebase patterns and this entry
- **Sections implemented:**
  1. Task Metadata (expanded) - task_id, benchmark source, language, difficulty, tags
  2. Build Environment (expanded) - docker image, model, agent name, environment type, task source
  3. Execution Metrics (expanded) - total duration, agent execution, env setup, verifier time, input/output/cached tokens, tool calls, reward score
  4. Agent Result (expanded) - pass/fail/error status badge, exit code, error details
  5. Verifier Output (collapsed) - test pass/fail/total counts, reward breakdown, test output
- **Learnings for future iterations:**
  - show_paired_task_detail loads instruction_content and task_metadata from task.toml - these are still loaded inline for LLM Judge Context tab
  - result.json agent_result may contain: n_input_tokens, n_output_tokens, n_cached_tokens, n_tool_calls, exit_code, cost
  - Instance dir layout varies: paired tasks at experiment/mode/timestamp/task_id/, single tasks at experiment/task_name/agent/
  - The benchmark_detection.detect_benchmark_set() utility works on experiment-level paths, not instance-level
  - run_results.py has many pre-existing ruff linting issues (F841, E722, F401) - do not fix these as part of feature work
---

## 2026-01-27 - US-002: Remove archived runs option from UI
- What was implemented: Verified that no "archived runs" UI toggle, checkbox, or filtering logic exists in the dashboard codebase
- Files changed:
  - prd.json - Marked US-002 passes: true with note
  - progress.txt - Added this entry
- **Investigation results:**
  - Searched dashboard/views/, dashboard/utils/, dashboard/components/ for any "archive" / "archived" / "ARCHIVE" references
  - Searched src/ directory as well - no matches
  - No checkbox/toggle, no filtering logic, no archived status field exists anywhere in the dashboard code
  - The only "archive" reference is the CCB_EXTERNAL_ARCHIVE_DIR env var mentioned in progress.txt codebase patterns (documentation only)
  - All acceptance criteria verified as already satisfied
- **Learnings for future iterations:**
  - The archived runs feature was either never implemented or was already removed in a prior cleanup
  - When a story asks to "remove" something, verify it exists first before making changes
  - CCB_EXTERNAL_ARCHIVE_DIR is documented in patterns but not used in dashboard code
---

## 2026-01-27 - US-003: Add benchmark set detection utility
- What was implemented: Verified that benchmark detection utility was already fully implemented in a prior session
- Files already present:
  - dashboard/utils/benchmark_detection.py - detect_benchmark_set() function with manifest-first, directory-fallback strategy
  - dashboard/tests/test_benchmark_detection.py - 32 passing tests covering all benchmark sets
- Files changed:
  - prd.json - Marked US-003 passes: true
  - progress.txt - Added this entry
- **Verification results:**
  - All 32 unit tests pass (14 manifest tests, 11 directory pattern tests, 7 integration tests)
  - Covers: LoCoBench, SWE-Bench Pro, SWE-Bench Verified, RepoQA, DIBench
  - Already integrated into run_results.py and task_detail.py
  - Typecheck passes
- **Learnings for future iterations:**
  - benchmark_detection.py uses BENCHMARK_ID_TO_NAME dict for manifest IDs and DIRECTORY_PATTERNS list of regex for fallback
  - detect_benchmark_set() accepts both str and Path, returns display name or "Unknown"
  - Already used by _group_experiments_by_benchmark() in run_results.py for US-004
  - When a story's code already exists, verify and mark as passing rather than reimplementing
---

## 2026-01-27 - US-004, US-005, US-006, US-008: Batch verification of already-implemented stories
- What was implemented: Verified that four user stories were already fully implemented in prior sessions
- Files already present:
  - dashboard/views/run_results.py - US-004 (benchmark grouping), US-005 (paired/individual mode toggle)
  - dashboard/utils/task_list.py - US-006 (task list with filtering, sorting, paired mode)
  - dashboard/utils/task_detail.py - US-008 (CLAUDE.md and task prompt display)
- Files changed:
  - prd.json - Marked US-004, US-005, US-006, US-008 as passes: true
  - progress.txt - Added this entry
- **Verification details:**
  - US-004: _group_experiments_by_benchmark() groups experiments, selectbox shows "Name (count)" format, "All Benchmarks" option available
  - US-005: Radio toggle "Individual Review" / "Paired Comparison", auto-detect pairs from manifest, manual pairing with common task detection
  - US-006: render_task_list() in task_list.py with NormalizedTask dataclass, status/language/task_type/difficulty filters, text search, 5 sort options, paired mode side-by-side columns
  - US-008: _extract_claude_md_content() searches 5+ paths, _extract_task_prompt() extracts from trace/config, both in collapsible expanders with "Not available" fallback
- **Learnings for future iterations:**
  - Multiple stories may be implemented in a single prior session - batch verify them
  - task_list.py is a standalone component (540 lines) imported by run_results.py - it handles all filtering/sorting/rendering
  - The TaskFilter and NormalizedTask dataclasses use frozen=True for immutability
  - _show_paired_mode() handles both auto-detected pairs (from manifest) and manual pairing
---

## 2026-01-27 - US-009: JSONL trace parser for claude-code.txt
- What was implemented: Created trace_viewer_parser.py with parse_trace() function, TraceMessage/TraceSummary dataclasses, and compute_summary()
- Files changed:
  - src/ingest/trace_viewer_parser.py (NEW) - JSONL parser with parse_trace(), compute_summary(), TraceMessage, TraceSummary, TokenUsage dataclasses
  - tests/test_trace_viewer_parser.py (NEW) - 23 unit/integration tests covering all message types and summary computation
  - prd.json - Marked US-009 passes: true
  - progress.txt - Added this entry
- **Implementation details:**
  - parse_trace() reads JSONL, skips malformed lines with warning, returns list of TraceMessage
  - TraceMessage fields: type, subtype, content, tool_name, tool_input, tool_result, token_usage, parent_tool_use_id, session_id, uuid, sequence_number
  - Handles system/init, assistant/text, assistant/tool_use, user/text, user/tool_result subtypes
  - Mixed content blocks (text + tool_use in same assistant line) produce multiple TraceMessages
  - compute_summary() returns: total_messages, total_tool_calls, unique_tools, total_tokens, tools_by_name, files_accessed
  - Token deduplication by uuid prevents double-counting from mixed content blocks
  - files_accessed tracks read_count, write_count, edit_count per file path
- **Learnings for future iterations:**
  - claude-code.txt JSONL format: system (init), assistant (text/tool_use), user (text/tool_result), queue-operation (skipped)
  - session_id field is "sessionId" in assistant/user messages, "session_id" in system messages
  - Assistant message usage field contains input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens
  - toolUseResult on user messages has content field (string or list of blocks)
  - parentUuid links messages in conversation tree, tool_use id links to tool_result
  - The existing src/ingest/transcript_parser.py parses claude-code.txt differently (text-based extraction) - trace_viewer_parser is JSONL-native
---

## 2026-01-27 - US-010: Trace summary overview panel
- What was implemented: Verified that trace summary overview panel was already fully implemented in a prior session
- Files already present:
  - dashboard/utils/trace_summary.py - render_trace_summary_panel() with metric cards, Plotly bar chart, and file access table
  - dashboard/tests/test_trace_summary.py - 18 passing tests covering all components
  - dashboard/views/run_results.py (lines 1530-1546) - Integration rendering panel at top of trace view
- Files changed:
  - prd.json - Marked US-010 passes: true
  - progress.txt - Added this entry
- **Verification results:**
  - All 18 tests pass (7 file access builder, 3 metric cards, 2 tool chart, 2 file table, 1 panel, 3 load-and-render)
  - Metric cards: total messages, tool calls, unique tools, total tokens, session duration (formatted as Xm Ys)
  - Plotly bar chart renders tool calls sorted descending with hover data
  - File access table with File Path, Reads, Writes, Edits, Total columns sorted by total descending
  - Panel rendered above st.tabs in show_claude_code_trace(), above the full trace
  - Typecheck passes
- **Learnings for future iterations:**
  - trace_summary.py uses render_trace_summary_panel(summary, session_duration) - takes pre-computed TraceSummary
  - load_and_render_trace_summary(claude_file, session_duration) is a convenience function that parses + renders
  - In run_results.py, the summary is built from inline-parsed data via _build_file_access_from_trace() rather than using the JSONL parser directly
  - The trace tab structure: summary panel -> st.tabs([Full Trace, Tool Calls, Code Changes, Bash Commands, Raw])
---

## 2026-01-27 - US-011: Full trace card rendering - message display
- What was implemented: Verified that trace card rendering was already fully implemented in a prior session
- Files already present:
  - dashboard/utils/trace_cards.py - render_trace_cards() with type-specific card renderers and pagination
  - dashboard/tests/test_trace_cards.py - 54 passing tests
  - dashboard/views/run_results.py (line 1570) - Integration in Full Trace tab
- Files changed:
  - prd.json - Marked US-011 passes: true
  - progress.txt - Added this entry
- **Verification results:**
  - All 54 tests pass (6 style key, 4 token format, 9 HTML rendering, 11 pagination, 4 dispatch, 2 system, 2 assistant, 2 tool_use, 4 tool_result, 7 render_trace_cards, 3 constants)
  - Card styles: system (gray #f0f0f0), assistant_text (white #ffffff), tool_use (light blue #eff6ff), tool_result (light green #f0fdf4)
  - Token usage format: "in: X | out: Y | cached: Z" on assistant messages
  - Pagination: 50 messages per page with top+bottom controls
  - Tool_result truncation at 100 lines with "Show more" expander
  - Typecheck passes
- **Learnings for future iterations:**
  - trace_cards.py uses render_trace_card(msg) for individual cards, render_trace_cards(messages, session_key) for paginated rendering
  - CARD_STYLES dict maps style keys to background, border_color, label, label_bg
  - Session state key for pagination defaults to "trace_page"
  - Tool input is rendered as syntax-highlighted JSON via st.code(json.dumps(...), language="json")
---
