# Ralph Progress Log
Started: Mon Jan 27 13:12:00 EST 2026

## Codebase Patterns
- Dashboard framework: Streamlit with Plotly charts, Pandas for data processing
- Dashboard entry point: dashboard/app.py with views in dashboard/views/
- Data ingestion: src/ingest/ (transcript_parser.py, harbor_parser.py, database.py)
- Analysis modules: src/analysis/ (8 analyzers: statistical, comparator, cost, failure, time_series, ir, llm_judge, recommendation)
- Benchmark management: src/benchmark/ (database.py, llm_judge.py, trace_parser.py)
- Config files: configs/ directory with YAML and JSON formats
- claude-code.txt is JSONL format (one JSON object per line), NOT plain text
- Token usage embedded in assistant message usage field (input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens)
- parent_tool_use_id links sub-agent tool calls to parent for hierarchical traces
- Experiment runs have manifest.json (metadata), index.json (task-to-run mapping), runs/ and pairs/ subdirs
- Task config.json contains agent info (model, import_path) and task info (path, git_url, source)
- Result.json contains verifier_result.rewards, agent_result (tokens, cost), timing data
- LoCoBench task IDs follow: {language}_{domain}_{complexity}_{num}_{task_category}_{difficulty}_{variant}
- SWE-Bench Pro task IDs follow: instance_{org}__{repo}__{hash}
- Runs directory path: ~/evals/custom_agents/agents/claudecode/runs/
- Environment variable for runs dir: CCB_EXTERNAL_RUNS_DIR (was CCB_EXTERNAL_JOBS_DIR)
- Archive dir env var: CCB_EXTERNAL_ARCHIVE_DIR
- Run output default path: runs/{run_id} (was jobs/{run_id})
- Task detail panel lives in dashboard/utils/task_detail.py - reuse render_task_detail_panel() for any task detail view
- result.json structure: verifier_result.rewards (reward breakdown), agent_result (tokens, exit_code, cost), timing phases (environment_setup, agent_setup, agent_execution, verifier)
- Two task detail paths: show_paired_task_detail (paired experiments) and show_external_task_detail (single experiments) - both now use shared component
- verifier/output.json in instance_dir contains additional verifier data that supplements result.json
---

## 2026-01-27 - US-001: Update data source path from jobs/ to runs/
- What was implemented: Migrated all data source path references from `jobs/` to `runs/` across the entire codebase
- Files changed:
  - dashboard/views/run_results.py - EXTERNAL_JOBS_DIR → EXTERNAL_RUNS_DIR, CCB_EXTERNAL_JOBS_DIR → CCB_EXTERNAL_RUNS_DIR, jobs_dir param → runs_dir, fallback path updated
  - dashboard/views/analysis_hub.py - Same env var and variable rename, user-facing messages updated
  - dashboard/views/analysis_llm_judge.py - Same env var and variable rename
  - dashboard/app.py - Help text updated from jobs to runs
  - src/benchmark/run_orchestrator.py - Default output_dir changed from jobs/{run_id} to runs/{run_id} (two locations)
  - src/benchmark/log_streamer.py - Default job_dir path updated
  - src/benchmark/profile_runner.py - Default jobs_root updated to runs/benchmark_profiles
  - src/benchmark/oracle_validator.py - Result pattern updated to runs/
  - src/benchmark/metrics_extractor.py - Docstring examples updated
  - src/ingest/orchestrator.py - Path pattern comment and check updated
  - src/sync/pull.py - Docstring updated
  - cli/main.py - VM path changed from /jobs to /runs
  - scripts/analyze_evals_v2.py - jobs subdirectory → runs subdirectory
  - scripts/test_pipeline_e2e.py - evals_base path updated
  - scripts/import_swebench_to_dashboard.py - Default evals-dir updated
  - scripts/analyze_time_allocation.py, comprehension_analysis_v2.py, comprehension_success_analysis.py, extract_enterprise_metrics.py - Historical script paths updated
  - tests/test_benchmark_profile_runner.py - Test config updated
- **Learnings for future iterations:**
  - The env variable CCB_EXTERNAL_RUNS_DIR controls where the dashboard looks for experiment data
  - Three dashboard views (run_results, analysis_hub, analysis_llm_judge) each independently define EXTERNAL_RUNS_DIR - consider centralizing
  - archive/ and artifacts/ directories contain copies of code that were NOT updated (they're historical snapshots)
  - The `eval_runs_v2/` path was only in documentation (EVAL_FRAMEWORK.md, GCP_BENCHMARK_EXECUTION.md), not in Python code
  - ruff is available for linting; pre-existing issues exist but should not be introduced by changes
---

## 2026-01-27 - US-007: Task detail view - metadata panel
- What was implemented: Created a reusable task detail metadata panel component with 5 collapsible sections displayed via st.expander
- Files changed:
  - dashboard/utils/task_detail.py (NEW) - Reusable component with render_task_detail_panel() function, helper functions for timing, tokens, build env, verifier output extraction
  - dashboard/views/run_results.py - Imported task_detail module, updated show_external_task_detail() and show_paired_task_detail() to use shared render_task_detail_panel()
  - scripts/ralph/prd.json - Marked US-007 passes: true
  - prd.json - Marked US-007 passes: true
  - progress.txt - Added codebase patterns and this entry
- **Sections implemented:**
  1. Task Metadata (expanded) - task_id, benchmark source, language, difficulty, tags
  2. Build Environment (expanded) - docker image, model, agent name, environment type, task source
  3. Execution Metrics (expanded) - total duration, agent execution, env setup, verifier time, input/output/cached tokens, tool calls, reward score
  4. Agent Result (expanded) - pass/fail/error status badge, exit code, error details
  5. Verifier Output (collapsed) - test pass/fail/total counts, reward breakdown, test output
- **Learnings for future iterations:**
  - show_paired_task_detail loads instruction_content and task_metadata from task.toml - these are still loaded inline for LLM Judge Context tab
  - result.json agent_result may contain: n_input_tokens, n_output_tokens, n_cached_tokens, n_tool_calls, exit_code, cost
  - Instance dir layout varies: paired tasks at experiment/mode/timestamp/task_id/, single tasks at experiment/task_name/agent/
  - The benchmark_detection.detect_benchmark_set() utility works on experiment-level paths, not instance-level
  - run_results.py has many pre-existing ruff linting issues (F841, E722, F401) - do not fix these as part of feature work
---

## 2026-01-27 - US-002: Remove archived runs option from UI
- What was implemented: Verified that no "archived runs" UI toggle, checkbox, or filtering logic exists in the dashboard codebase
- Files changed:
  - prd.json - Marked US-002 passes: true with note
  - progress.txt - Added this entry
- **Investigation results:**
  - Searched dashboard/views/, dashboard/utils/, dashboard/components/ for any "archive" / "archived" / "ARCHIVE" references
  - Searched src/ directory as well - no matches
  - No checkbox/toggle, no filtering logic, no archived status field exists anywhere in the dashboard code
  - The only "archive" reference is the CCB_EXTERNAL_ARCHIVE_DIR env var mentioned in progress.txt codebase patterns (documentation only)
  - All acceptance criteria verified as already satisfied
- **Learnings for future iterations:**
  - The archived runs feature was either never implemented or was already removed in a prior cleanup
  - When a story asks to "remove" something, verify it exists first before making changes
  - CCB_EXTERNAL_ARCHIVE_DIR is documented in patterns but not used in dashboard code
---

## 2026-01-27 - US-003: Add benchmark set detection utility
- What was implemented: Verified that benchmark detection utility was already fully implemented in a prior session
- Files already present:
  - dashboard/utils/benchmark_detection.py - detect_benchmark_set() function with manifest-first, directory-fallback strategy
  - dashboard/tests/test_benchmark_detection.py - 32 passing tests covering all benchmark sets
- Files changed:
  - prd.json - Marked US-003 passes: true
  - progress.txt - Added this entry
- **Verification results:**
  - All 32 unit tests pass (14 manifest tests, 11 directory pattern tests, 7 integration tests)
  - Covers: LoCoBench, SWE-Bench Pro, SWE-Bench Verified, RepoQA, DIBench
  - Already integrated into run_results.py and task_detail.py
  - Typecheck passes
- **Learnings for future iterations:**
  - benchmark_detection.py uses BENCHMARK_ID_TO_NAME dict for manifest IDs and DIRECTORY_PATTERNS list of regex for fallback
  - detect_benchmark_set() accepts both str and Path, returns display name or "Unknown"
  - Already used by _group_experiments_by_benchmark() in run_results.py for US-004
  - When a story's code already exists, verify and mark as passing rather than reimplementing
---
