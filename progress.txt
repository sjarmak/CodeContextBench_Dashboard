# Ralph Progress Log
Started: Mon Feb  2 16:01:56 EST 2026

## Codebase Patterns
- Trial directories are identified by the presence of `result.json`; this is the primary discovery criterion
- Runs directory structure: `runs_dir / category(official|experiment|troubleshooting) / experiment / config_type / timestamp / trial_dir/`
- Harbor result.json timing fields are top-level (`started_at`, `finished_at`), agent timing in `agent_execution` dict
- Token counts come from `task_metrics.json` (preferred) or `result.json.agent_result.n_input_tokens` etc.
- Reward comes from `verifier/reward.txt` (preferred) or `result.json.verifier_result.rewards.reward`
- `dict.get(key, default)` does NOT protect against `None` values -- always use `or default` pattern
- All pipeline dataclasses must be `frozen=True` for immutability; use tuples not lists
- Pre-existing mypy errors in cost_calculator.py, task_schema.py, orchestrator.py are unrelated to pipeline work
- Test fixtures use `tmp_path` and helper functions like `_make_result_json()` for building trial structures
- `analyze._flatten_trials()` converts nested categories/experiments/trials to flat list; reuse in downstream steps
- Statistical tests: Welch's t-test for continuous, proportion z-test for rates; Cohen's d for effect sizes
- numpy bools (`np.True_`) fail `is True` assertions; always wrap with `bool()` when constructing frozen dataclasses from scipy/numpy results
- Variable name shadowing causes mypy errors in nested loops; use distinct names (e.g., `phase_trials` vs `config_trials`)
- `_group_by_benchmark()` and `_get_trial_benchmark()` are reusable helpers for any per-benchmark analysis step
- Tool utilization data in trial dicts: `trial["tool_utilization"]` is a dict with keys `mcp_calls`, `deep_search_calls`, `total_tool_calls`, `context_fill_rate`, `deep_search_vs_keyword_ratio`, etc.
- `_get_tool_util(trial, field)` safely extracts numeric fields from tool_utilization dict; handles None/missing cases
- Pearson correlation requires >=3 observations and non-zero variance in both variables; check before calling `scipy.stats.pearsonr`
- matplotlib `plt.get_cmap("tab10")` is the colorblind-friendly palette; `tableau-colorblind10` is NOT a valid cmap name despite being referenced in docs
- Use `matplotlib.use("Agg")` before any plt calls for headless figure generation; `plt.style.context()` for temporary style application
- `publish_figures()` orchestrates all figure generators; each returns list of written paths for the caller

---

## 2026-02-02 - US-003
- Implemented `scripts/ccb_pipeline/extract.py` with `__main__` entry point
- Created `scripts/ccb_pipeline/__init__.py` package
- Files created: `scripts/ccb_pipeline/__init__.py`, `scripts/ccb_pipeline/extract.py`, `tests/test_extract.py`
- Walks runs directory recursively finding trial directories (via result.json presence)
- Extracts: reward (reward.txt > result.json), timing (agent_execution > top-level), tokens (task_metrics.json > result.json), benchmark name, config detection (US-001), tool utilization (US-002)
- CLI: `python -m scripts.ccb_pipeline.extract --runs-dir <path> --output <path>`
- 42 unit tests, all passing; no typecheck errors in extract.py
- **Learnings for future iterations:**
  - Benchmark name can be extracted from task_metrics.json `benchmark` field or config.json `task.path` after `benchmarks/` segment
  - Task name resolution: result.json `task_name` field is most reliable, then config.json `task.path` leaf name, then directory name
  - The `_find_trials()` function recursively walks until it finds result.json; this handles variable nesting depths across different experiment types
---

## 2026-02-02 - US-004
- Implemented `scripts/ccb_pipeline/sdlc_mapping.py` with benchmark-to-SDLC phase mapping
- Files created: `scripts/ccb_pipeline/sdlc_mapping.py`, `tests/test_sdlc_mapping.py`
- `BENCHMARK_SDLC_MAP` dict covers all Table 2 benchmarks plus project-specific ones (big_code_mcp, github_mined, tac_mcp_value)
- `get_sdlc_phases()` uses fuzzy substring matching, sorted by pattern length descending for specificity
- `get_benchmark_name()` extracts benchmark from task paths by finding segment after "benchmarks/"
- 36 unit tests, all passing; mypy clean
- **Learnings for future iterations:**
  - Fuzzy matching sorted by pattern length descending ensures "swe-bench-pro" matches before "swe-bench"
  - `PurePosixPath` from pathlib handles cross-platform path parsing without OS dependency
  - SDLC phases stored as tuples in the map for immutability; `get_sdlc_phases()` returns a new list each call
---

## 2026-02-02 - US-005
- Implemented `scripts/ccb_pipeline/analyze.py` with `__main__` entry point
- Files created: `scripts/ccb_pipeline/analyze.py`, `tests/test_analyze.py`
- Consumes experiment_metrics.json from US-003 extract step
- Computes per-configuration aggregate metrics: mean reward (with SE), pass rate (with SE), median duration, mean input/output tokens
- Pairwise statistical tests for all config pairs: Welch's t-test for continuous metrics (reward, duration, input_tokens), proportion z-test for pass rate
- Cohen's d effect sizes for continuous metrics with interpretation (negligible/small/medium/large)
- Outputs analysis_results.json with sections: aggregate_metrics, pairwise_tests, effect_sizes
- CLI: `python -m scripts.ccb_pipeline.analyze --input experiment_metrics.json --output analysis_results.json`
- 46 unit tests, all passing; mypy clean
- **Learnings for future iterations:**
  - `scipy.stats.ttest_ind` with `equal_var=False` gives Welch's t-test (more robust for unequal variances)
  - Proportion z-test uses pooled proportion for standard error calculation
  - Cohen's d uses pooled standard deviation; interpretation thresholds: 0.2 small, 0.5 medium, 0.8 large
  - scipy may warn about precision loss with nearly-identical data; this is expected in tests with synthetic data
  - The `_flatten_trials()` helper is reusable for any step that needs to iterate all trials regardless of category/experiment nesting
---

## 2026-02-02 - US-006
- Added per-benchmark and per-SDLC-phase breakdown to `scripts/ccb_pipeline/analyze.py`
- Files modified: `scripts/ccb_pipeline/analyze.py`, `tests/test_analyze.py`
- New dataclasses: `BenchmarkConfigMetrics`, `BenchmarkSignificanceResult`, `SDLCPhaseMetrics`
- New functions: `_get_trial_benchmark()`, `_group_by_benchmark()`, `compute_per_benchmark_metrics()`, `compute_per_benchmark_significance()`, `_is_mcp_improvement()`, `compute_per_sdlc_phase_metrics()`
- Per-benchmark: computes pass rate and mean reward per config per benchmark, runs pairwise significance tests (z-test for pass rate, Welch's t-test for reward) per benchmark
- Per-SDLC-phase: maps benchmarks to SDLC phases via `get_sdlc_phases()`, aggregates trials across benchmarks sharing a phase, computes reward delta vs BASELINE
- Flags `mcp_improves=True` only when both significant (p<0.05) AND the MCP config outperforms
- Results added to analysis_results.json under `per_benchmark`, `per_benchmark_significance`, `per_sdlc_phase` sections
- 71 tests passing (25 new), mypy clean
- **Learnings for future iterations:**
  - numpy bools (`np.True_`) fail Python `is True` checks; always `bool()` wrap values from scipy/numpy before storing in frozen dataclasses
  - Variable name shadowing across loop scopes causes mypy type confusion; use distinct names in nested for-loops
  - Trials can map to multiple SDLC phases (e.g., swe-bench-pro -> Implementation + Testing); the phase grouping correctly counts them in each phase
  - `_is_mcp_improvement()` uses an ordinal ranking (BASELINE=0, MCP_BASE=1, MCP_FULL=2) to determine which config is "better"
---

## 2026-02-02 - US-007
- Added efficiency analysis to `scripts/ccb_pipeline/analyze.py`
- Files modified: `scripts/ccb_pipeline/analyze.py`, `tests/test_analyze.py`
- New dataclass: `EfficiencyMetrics` (frozen) with 14 fields covering token totals, means with SE, ratios, and overhead
- New function: `compute_efficiency_metrics()` computes per-config token consumption, input-to-output ratio, median wall-clock time, tokens-per-success, and MCP token overhead vs BASELINE
- MCP token overhead = mean total tokens for MCP config minus mean total tokens for BASELINE
- Tokens-per-success = total tokens (input + output) / number of passing tasks; None if no passing tasks
- Results added to analysis_results.json under `efficiency` section
- 89 tests passing (18 new), mypy clean
- **Learnings for future iterations:**
  - `tokens_per_success` uses total tokens (input + output) divided by pass count; cached tokens are informational only (subset of input)
  - MCP token overhead is computed as delta of mean totals, not delta of totals, to normalize for different trial counts
  - The `or 0` pattern for token fields handles both missing keys and None values from dict.get()
---

## 2026-02-02 - US-008
- Added tool utilization analysis to `scripts/ccb_pipeline/analyze.py`
- Files modified: `scripts/ccb_pipeline/analyze.py`, `tests/test_analyze.py`
- New dataclasses: `ToolUtilizationConfigMetrics`, `ToolRewardCorrelation`, `BenchmarkToolUsage` (all frozen)
- New functions: `_get_tool_util()`, `compute_tool_utilization_config_metrics()`, `compute_tool_reward_correlation()`, `compute_benchmark_tool_usage()`
- Per-config: mean MCP calls (with SE), mean Deep Search calls (with SE), mean Deep Search vs keyword ratio, mean context fill rate (with SE)
- Pearson r correlation between MCP tool call count and task reward (requires >=3 observations, non-zero variance)
- Per-benchmark tool usage: groups by benchmark and config, computes mean MCP/Deep Search/total tool calls
- Results added to analysis_results.json under `tool_utilization`, `tool_reward_correlation`, `benchmark_tool_usage` sections
- 115 tests passing (26 new), mypy clean
- **Learnings for future iterations:**
  - Tool utilization data lives in `trial["tool_utilization"]` dict; use `_get_tool_util()` helper for safe extraction
  - `scipy.stats.pearsonr` requires non-zero variance in both variables; must check `len(set(values)) >= 2` before calling
  - Correlation analysis needs >= 3 observations; with fewer, results are meaningless
  - Reused `_group_by_benchmark()` and `_group_by_config()` for benchmark tool usage grouping
- LaTeX tables use booktabs formatting (toprule/midrule/bottomrule); each generator returns a string, `publish_tables()` orchestrates writing
- `_fmt_int()` with comma separators for token counts; `_fmt_pvalue()` uses `$< 0.001$` for very small p-values
- Significance markers: `*` p<0.05, `**` p<0.01, `***` p<0.001; per-benchmark table uses minimum p-value across pairs for a config
- `_config_display()` maps enum names to paper-friendly display names (BASELINE->Baseline, MCP_BASE->MCP-Base, MCP_FULL->MCP-Full)
---

## 2026-02-02 - US-009
- Implemented `scripts/ccb_pipeline/publish.py` with `__main__` entry point
- Files created: `scripts/ccb_pipeline/publish.py`, `tests/test_publish.py`
- Generates 4 LaTeX tables from analysis_results.json:
  - `table_aggregate.tex`: config | pass rate (with SE) | mean reward (with SE) | median duration | mean tokens
  - `table_per_benchmark.tex`: benchmark | config | n | pass rate | mean reward (with SE) | significance marker
  - `table_significance.tex`: pair | metric | p-value | Cohen's d | sig marker | interpretation
  - `table_efficiency.tex`: config | mean input tokens (with SE) | mean output tokens (with SE) | tokens/success | I/O ratio | MCP overhead
- All tables use booktabs formatting, proper LaTeX escaping, and comma-separated integers
- CLI: `python -m scripts.ccb_pipeline.publish --input analysis_results.json --output-dir output/`
- 39 unit tests, all passing; mypy clean
- **Learnings for future iterations:**
  - `publish_tables()` returns list of written file paths for caller to inspect; skips tables when input section is empty
  - Per-benchmark significance lookup uses minimum p-value across all config pairs for a given benchmark+config to find the "most significant" comparison
  - None values in efficiency metrics (tokens_per_success, mcp_token_overhead) formatted as "--" in LaTeX
  - `generate_table_per_benchmark()` groups by benchmark with first-row-only benchmark name display for cleaner tables
---

## 2026-02-02 - US-010
- Added matplotlib figure generators to `scripts/ccb_pipeline/publish.py`
- Files modified: `scripts/ccb_pipeline/publish.py`, `tests/test_publish.py`
- 4 figure generators:
  - `generate_fig_pass_rate()`: grouped bar chart of pass rate by config with SE error bars
  - `generate_fig_benchmark_heatmap()`: heatmap of reward delta (MCP-Full minus Baseline) per benchmark using RdYlGn diverging colormap
  - `_generate_fig_token_overhead()`: scatter plot of token overhead vs reward improvement per non-BASELINE config
  - `generate_fig_tool_utilization()`: stacked bar chart of tool call distribution (Local/Other, MCP keyword/NLS, Deep Search) per config
- `publish_figures()` orchestrator creates `output/figures/` and calls all generators
- All figures use `seaborn-v0_8-paper` style, `tab10` colorblind-friendly palette, Agg backend
- Each figure exported as both PDF and SVG
- `main()` updated to call `publish_figures()` alongside `publish_tables()`
- 65 tests passing (26 new figure tests), mypy clean
- **Learnings for future iterations:**
  - `tableau-colorblind10` is NOT a valid matplotlib colormap name; use `tab10` instead (the Tab10 colormap IS the Tableau colorblind-safe palette)
  - `matplotlib.use("Agg")` must be set before any figure operations for headless/CI environments
  - `plt.style.context(_STYLE)` is better than `plt.style.use()` as it scopes the style to a block without global side effects
  - `_ordered_configs()` sorts configs in canonical order (BASELINE, MCP_BASE, MCP_FULL) for consistent figure presentation
  - Heatmap with single column uses `aspect="auto"` to avoid extremely narrow figures; `figsize` height scales with benchmark count
  - Token overhead scatter requires both efficiency and aggregate data; returns empty if BASELINE reward is missing
---
