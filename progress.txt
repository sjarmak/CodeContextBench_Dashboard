# Ralph Progress Log
Started: Mon Feb  2 16:01:56 EST 2026

## Codebase Patterns
- Trial directories are identified by the presence of `result.json`; this is the primary discovery criterion
- Runs directory structure: `runs_dir / category(official|experiment|troubleshooting) / experiment / config_type / timestamp / trial_dir/`
- Harbor result.json timing fields are top-level (`started_at`, `finished_at`), agent timing in `agent_execution` dict
- Token counts come from `task_metrics.json` (preferred) or `result.json.agent_result.n_input_tokens` etc.
- Reward comes from `verifier/reward.txt` (preferred) or `result.json.verifier_result.rewards.reward`
- `dict.get(key, default)` does NOT protect against `None` values -- always use `or default` pattern
- All pipeline dataclasses must be `frozen=True` for immutability; use tuples not lists
- Pre-existing mypy errors in cost_calculator.py, task_schema.py, orchestrator.py are unrelated to pipeline work
- Test fixtures use `tmp_path` and helper functions like `_make_result_json()` for building trial structures

---

## 2026-02-02 - US-003
- Implemented `scripts/ccb_pipeline/extract.py` with `__main__` entry point
- Created `scripts/ccb_pipeline/__init__.py` package
- Files created: `scripts/ccb_pipeline/__init__.py`, `scripts/ccb_pipeline/extract.py`, `tests/test_extract.py`
- Walks runs directory recursively finding trial directories (via result.json presence)
- Extracts: reward (reward.txt > result.json), timing (agent_execution > top-level), tokens (task_metrics.json > result.json), benchmark name, config detection (US-001), tool utilization (US-002)
- CLI: `python -m scripts.ccb_pipeline.extract --runs-dir <path> --output <path>`
- 42 unit tests, all passing; no typecheck errors in extract.py
- **Learnings for future iterations:**
  - Benchmark name can be extracted from task_metrics.json `benchmark` field or config.json `task.path` after `benchmarks/` segment
  - Task name resolution: result.json `task_name` field is most reliable, then config.json `task.path` leaf name, then directory name
  - The `_find_trials()` function recursively walks until it finds result.json; this handles variable nesting depths across different experiment types
---

## 2026-02-02 - US-004
- Implemented `scripts/ccb_pipeline/sdlc_mapping.py` with benchmark-to-SDLC phase mapping
- Files created: `scripts/ccb_pipeline/sdlc_mapping.py`, `tests/test_sdlc_mapping.py`
- `BENCHMARK_SDLC_MAP` dict covers all Table 2 benchmarks plus project-specific ones (big_code_mcp, github_mined, tac_mcp_value)
- `get_sdlc_phases()` uses fuzzy substring matching, sorted by pattern length descending for specificity
- `get_benchmark_name()` extracts benchmark from task paths by finding segment after "benchmarks/"
- 36 unit tests, all passing; mypy clean
- **Learnings for future iterations:**
  - Fuzzy matching sorted by pattern length descending ensures "swe-bench-pro" matches before "swe-bench"
  - `PurePosixPath` from pathlib handles cross-platform path parsing without OS dependency
  - SDLC phases stored as tuples in the map for immutability; `get_sdlc_phases()` returns a new list each call
---
