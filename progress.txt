# Ralph Progress Log
Started: Thu Feb  5 08:09:21 EST 2026

## Codebase Patterns
- Run directory structure: `runs/<folder>/<run_name>/<config>/<batch_timestamp>/<task_id__hash>/task_metrics.json`
- Config directory names: `baseline`, `sourcegraph_base`, `sourcegraph_full` (use `_KNOWN_CONFIG_DIRS` tuple)
- Task ID extraction: directory names have hash suffix after `__` (e.g., `big-code-servo-001__jk4jiJS` -> `big-code-servo-001`), but `task_id` field in task_metrics.json is authoritative
- Use `json.loads(path.read_text())` for file reading (matches home_run_scanner.py pattern)
- Agent labels: use `dashboard/utils/agent_labels.py` `AGENT_DISPLAY_NAMES` for human-readable config names
- `task_context_length` and `instruction_length_chars` fields do NOT exist in actual task_metrics.json data - size bucketing must use `input_tokens` or similar
- All task_metrics.json fields can be `null` - always use `data.get('key') or default` pattern
- Session state cache key for comparison data: `comparison_data`
- Widget key prefixes: `ccmp_` for config comparison, `ccmp_filter_` for filters
- Pre-existing test failures (27) are unrelated to new code - tests in `test_analysis_*_v2.py` have collection errors
- `search_strategy_type` field doesn't exist directly in task_metrics.json - must be inferred from `search_calls_keyword`, `search_calls_nls`, `search_calls_deepsearch` fields
- `error_fingerprint` is a dict with keys `fingerprint_id`, `label`, `severity` (or null for no error)
- Size bucketing uses `input_tokens` as proxy since `task_context_length` doesn't exist in actual data
- Pandas converts `None` to `NaN` (float) in numeric columns — `val is None` checks fail; use `math.isnan()` as fallback in formatters
- `_CONFIG_NAMES` tuple at module level for the 3 config directory names — reuse instead of hardcoding lists
- Pandas Styler with `st.dataframe`: use `.style.apply(fn, axis=1)` for row-level backgrounds; return `["background-color: rgba(...)"] * len(row)` for each row
- `sdlc_phase` is a real field in task_metrics.json — populated from `selected_benchmark_tasks.json` registry, not inferred
- Streamlit `st.multiselect` returns `[]` when empty — use `sel or None` before passing to filter functions that treat `None` as "no filter"
---

## 2026-02-05 - US-001
- Implemented `dashboard/utils/comparison_loader.py`
- Files changed: `dashboard/utils/comparison_loader.py` (new), `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - Runs are in nested subdirectories (official/, experiment/, etc.) not just one folder - scanner must iterate all top-level folders
  - task_id from task_metrics.json is authoritative over directory name parsing
  - When merging across multiple runs, later runs should fill in missing config data (immutable merge via `{**existing, config: new}`)
  - Verified: 118 tasks load successfully with all 3 configs populated
  - `_KNOWN_CONFIG_DIRS` naming follows `home_run_scanner.py` pattern but uses `_CONFIG_DIRS` (configs not modes)
---

## 2026-02-05 - US-002
- Verified existing `dashboard/utils/comparison_metrics.py` (already committed)
- Updated PRD to mark US-002 as passes: true
- Files changed: `prd.json`
- **Learnings for future iterations:**
  - US-002 was already committed but PRD wasn't updated - always verify PRD state matches git history
---

## 2026-02-05 - US-003
- Implemented `dashboard/utils/comparison_filters.py`
- Files changed: `dashboard/utils/comparison_filters.py` (new), `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - `search_strategy_type` must be inferred from search call counts (`search_calls_keyword`, `search_calls_nls`, `search_calls_deepsearch`)
  - Error severity filtering checks across ALL configs for a task (OR within a task's configs) — a task matches if ANY config has matching severity
  - "no_error" is a pseudo-value for tasks with null `error_fingerprint` — useful for filtering to successful tasks
  - Size bucketing uses `input_tokens` since `task_context_length`/`instruction_length_chars` don't exist in real data
---

## 2026-02-05 - US-004
- Created `dashboard/views/config_comparison.py` with `show_config_comparison()`
- Registered view in `dashboard/app.py` — added to `nav_results` list and routing chain
- Files changed: `dashboard/views/config_comparison.py` (new), `dashboard/app.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - App.py uses dynamic imports in wrapper functions (e.g., `from views.config_comparison import ...`) — follow this pattern for all new views
  - `AGENT_DISPLAY_NAMES` maps canonical labels, not directory names — need to map through `AGENT_LABEL_MAP` first (baseline->baseline, sourcegraph_base->sourcegraph_no_ds, sourcegraph_full->sourcegraph_full)
  - Tokens are split as `input_tokens` + `output_tokens` in task_metrics.json — combine for total display
  - Use `_safe_float` / `_safe_int` helpers for null coercion since task_metrics fields can all be None
  - Em-dash character (`\u2014`) for null display values in dataframes
---

## 2026-02-05 - US-005
- Added computed effectiveness columns (benefit %, token cost %, efficiency) to config comparison table
- Imported `compute_mcp_benefit` and computed sg_base and sg_full benefit metrics vs baseline in `_build_comparison_df`
- Added `_fmt_pct` formatter for signed percentage display (e.g., `+41.2%`, `-5.3%`)
- Added formatted computed columns to `_format_display_df`: SG Base Benefit %, SG Full Benefit %, SG Full Token Cost %, SG Full Efficiency
- Files changed: `dashboard/views/config_comparison.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - Pandas converts `None` to `NaN` (float) in numeric DataFrame columns — `val is None` check fails for NaN. Must also check `math.isnan(val)` in all display formatters
  - Keep raw floats in the DataFrame, format only in display layer (`_format_display_df`) — this preserves numeric operations on raw_df
  - `_fmt_float` already handles the NaN case so can be used directly in apply() without wrapping lambda
---

## 2026-02-05 - US-006
- Added anomaly highlighting to comparison table with Pandas Styler
- Implemented `_classify_anomaly()` with priority-based classification: Regression > Token spike > Win
- Added `_build_anomaly_labels()` to compute per-row anomaly labels from raw DataFrame
- Added `_apply_anomaly_styles()` that returns a Pandas Styler with row-level background colors
- Added "Anomaly" label column to display DataFrame showing Win/Regression/Token spike
- Added `st.checkbox('Show highlights', value=True, key='ccmp_highlights')` toggle
- When highlights enabled, `st.dataframe` receives Styler object; when disabled, plain DataFrame
- Files changed: `dashboard/views/config_comparison.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - Pandas Styler `.apply(fn, axis=1)` applies row-wise — return list of CSS strings matching column count
  - Use `rgba()` colors for backgrounds so they work on both light and dark Streamlit themes
  - mypy requires explicit `is not None` checks after calling a helper like `_is_valid_number()` — the helper alone doesn't narrow the type
  - Anomaly classification priority matters: Regression is most critical to surface, then Token spike, then Win
  - `st.dataframe` accepts both `pd.DataFrame` and `pd.io.formats.style.Styler` — toggling between them with a checkbox works cleanly
---

## 2026-02-05 - US-007
- Added filter panel to config comparison view with 6 filter dimensions
- Added `sdlc_phase` filter parameter to `filter_comparison_tasks()` in `comparison_filters.py`
- Created `_extract_unique_values()` to dynamically populate filter dropdowns from loaded data
- Created `_render_filter_panel()` with 2 rows of 3 columns: Benchmark, Category, Difficulty, SDLC Phase, Search Strategy Type, Task Size (radio)
- Integrated filtering into `show_config_comparison()` — filters run before DataFrame build
- Added "Showing X of Y tasks" summary and empty state warning when no tasks match
- Files changed: `dashboard/views/config_comparison.py`, `dashboard/utils/comparison_filters.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - `sdlc_phase` is a real field in task_metrics.json (not inferred) — populated from `selected_benchmark_tasks.json` registry
  - `_get_search_strategy_type` from comparison_filters.py is useful in the view layer for extracting unique strategy values across all configs
  - Streamlit `st.multiselect` returns empty list `[]` when nothing is selected — convert to `None` with `sel or None` before passing to filter functions
  - `st.radio(horizontal=True)` works well for single-value filters like size bucket in compact layouts
  - Filter panel placement: above the data table, before metrics summary, so summary metrics reflect filtered data
---

## 2026-02-05 - US-008
- Added CSV export button to config comparison view
- Uses `display_df.to_csv(index=False)` to export filtered rows with human-readable column headers
- Filename format: `ccb-comparison-{timestamp}.csv` using UTC timestamp
- Files changed: `dashboard/views/config_comparison.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - `st.download_button` accepts `data` as a string directly — `pd.DataFrame.to_csv(index=False)` returns a string suitable for this
  - Use `datetime.now(tz=timezone.utc)` for timezone-aware timestamps (avoids deprecation warnings from naive `utcnow()`)
  - CSV export should use the display DataFrame (formatted strings) not the raw DataFrame, so users get the same values they see on screen
  - Widget key `ccmp_export_csv` follows the established prefix convention
---

## 2026-02-05 - US-009
- Added "Search Effectiveness" expander section to config_comparison.py
- Implemented `_build_search_effectiveness_df()` that aggregates across sourcegraph_base and sourcegraph_full configs only
- Groups by (category, search_strategy_type) using Pandas groupby + agg
- Displays: Category | Strategy | Avg Reward | Avg Tokens | N Tasks
- Sorted by Avg Reward descending within each category
- Used `_get_search_strategy_type` from comparison_filters.py to infer strategy from search call counts
- Files changed: `dashboard/views/config_comparison.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - `st.expander("title", expanded=False)` is cleaner than tabs for optional analysis sections — doesn't require tab switching
  - Pandas `groupby().agg()` with named aggregations (`avg_reward=("reward", "mean")`) produces clean column names directly
  - The `_safe_float` / `_safe_int` / `_is_valid_number` helpers from earlier in the file are reusable for formatting aggregated values
  - `_get_search_strategy_type` returns `None` for baseline configs (no search calls) — filtering `if strategy is None: continue` naturally excludes baseline
---

## 2026-02-05 - US-010
- Added "Search by Size" sub-section within the search effectiveness expander in `config_comparison.py`
- Implemented `_build_search_by_size_df()` to collect rows with (size_bucket, strategy, reward) from sourcegraph configs
- Implemented `_render_search_by_size()` with `pd.pivot_table()` for cross-tabulation and `Styler.background_gradient(cmap='RdYlGn')` for heatmap
- Imported `_get_size_bucket` from `comparison_filters.py` for consistent size classification
- Displays two tables: gradient-styled reward pivot and plain task count pivot
- Row order enforced: small, medium, large with human-readable labels
- Files changed: `dashboard/views/config_comparison.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - `pd.pivot_table()` with `aggfunc="mean"` and `aggfunc="count"` on same data produces parallel pivots for values and significance context
  - Reindexing with a known order list (`_SIZE_BUCKET_ORDER`) ensures consistent row ordering even when some buckets are missing
  - `_get_size_bucket` from `comparison_filters.py` is importable and reusable — no need to duplicate bucketing logic
  - Pandas Styler `.background_gradient(cmap='RdYlGn', axis=None)` applies gradient across the entire table (not just per-row or per-column)
  - `.format()` on Styler accepts a lambda for conditional formatting of NaN values
---

## 2026-02-05 - US-011
- Added "Error Analysis" expander section to `config_comparison.py`
- Implemented `_build_error_severity_df()` that aggregates across all 3 configs (baseline, sourcegraph_base, sourcegraph_full)
- Groups by error severity from `error_fingerprint` dict, with `null` mapped to "No Error"
- Displays: Error Severity | Count | % of Total | Avg Reward | Config Distribution
- Config Distribution shows per-config breakdown (e.g., "baseline: 50, sourcegraph_full: 48")
- Sorted by severity order: infra, api, task, mcp, verifier, setup, No Error
- Imported `_get_error_severity` from `comparison_filters.py` for consistent severity extraction
- Files changed: `dashboard/views/config_comparison.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - `_get_error_severity` from `comparison_filters.py` handles the null/dict type checking — reuse it instead of duplicating
  - In the current dataset, all `error_fingerprint` values are `null` — the section will show 100% "No Error" but is ready for when errors are populated
  - Pandas `value_counts()` on a Series produces a clean config distribution for display (e.g., "baseline: 50, sourcegraph_full: 48")
  - `_SEVERITY_ORDER` list ensures consistent row ordering even when not all severities are present in the data
  - Using `map()` with a rank dict + `sort_values` is cleaner than custom sort logic for ordered categorical display
---

## 2026-02-05 - US-012
- Added "Tool Errors" sub-section within the Error Analysis expander in `config_comparison.py`
- Implemented `_build_tool_errors_df()` that aggregates `tool_errors_by_name` across tasks/configs with error rate computation from `tool_calls_by_name`
- Implemented `_render_tool_errors()` with `st.multiselect` config filter (baseline/sourcegraph_base/sourcegraph_full)
- Displays: Tool Name | Error Count | Affected Task Count | Avg Error Rate
- Sorted by error count descending
- Handles null `tool_errors_by_name` and `tool_calls_by_name` gracefully with informative empty state message
- Files changed: `dashboard/views/config_comparison.py`, `prd.json`, `progress.txt`
- **Learnings for future iterations:**
  - `tool_errors_by_name` is not currently populated in any task_metrics.json (always null) — the section shows an info message for now but is ready for when error data is present
  - `tool_calls_by_name` IS populated in 723/859 tasks — maps tool name strings to integer call counts
  - When accumulating across tasks, use a `set()` for affected task tracking to avoid counting duplicates
  - Error rate formula: `errors / max(calls, 1) * 100` — uses `max(calls, 1)` to avoid division by zero when calls data is missing but errors exist
  - `_CONFIG_NAMES` tuple at module level avoids duplicating the list of config directory names
---
