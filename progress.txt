# Ralph Progress Log
Started: Mon Feb  2 16:01:56 EST 2026

## Codebase Patterns
- Trial directories are identified by the presence of `result.json`; this is the primary discovery criterion
- Runs directory structure: `runs_dir / category(official|experiment|troubleshooting) / experiment / config_type / timestamp / trial_dir/`
- Harbor result.json timing fields are top-level (`started_at`, `finished_at`), agent timing in `agent_execution` dict
- Token counts come from `task_metrics.json` (preferred) or `result.json.agent_result.n_input_tokens` etc.
- Reward comes from `verifier/reward.txt` (preferred) or `result.json.verifier_result.rewards.reward`
- `dict.get(key, default)` does NOT protect against `None` values -- always use `or default` pattern
- All pipeline dataclasses must be `frozen=True` for immutability; use tuples not lists
- Pre-existing mypy errors in cost_calculator.py, task_schema.py, orchestrator.py are unrelated to pipeline work
- Test fixtures use `tmp_path` and helper functions like `_make_result_json()` for building trial structures
- `analyze._flatten_trials()` converts nested categories/experiments/trials to flat list; reuse in downstream steps
- Statistical tests: Welch's t-test for continuous, proportion z-test for rates; Cohen's d for effect sizes

---

## 2026-02-02 - US-003
- Implemented `scripts/ccb_pipeline/extract.py` with `__main__` entry point
- Created `scripts/ccb_pipeline/__init__.py` package
- Files created: `scripts/ccb_pipeline/__init__.py`, `scripts/ccb_pipeline/extract.py`, `tests/test_extract.py`
- Walks runs directory recursively finding trial directories (via result.json presence)
- Extracts: reward (reward.txt > result.json), timing (agent_execution > top-level), tokens (task_metrics.json > result.json), benchmark name, config detection (US-001), tool utilization (US-002)
- CLI: `python -m scripts.ccb_pipeline.extract --runs-dir <path> --output <path>`
- 42 unit tests, all passing; no typecheck errors in extract.py
- **Learnings for future iterations:**
  - Benchmark name can be extracted from task_metrics.json `benchmark` field or config.json `task.path` after `benchmarks/` segment
  - Task name resolution: result.json `task_name` field is most reliable, then config.json `task.path` leaf name, then directory name
  - The `_find_trials()` function recursively walks until it finds result.json; this handles variable nesting depths across different experiment types
---

## 2026-02-02 - US-004
- Implemented `scripts/ccb_pipeline/sdlc_mapping.py` with benchmark-to-SDLC phase mapping
- Files created: `scripts/ccb_pipeline/sdlc_mapping.py`, `tests/test_sdlc_mapping.py`
- `BENCHMARK_SDLC_MAP` dict covers all Table 2 benchmarks plus project-specific ones (big_code_mcp, github_mined, tac_mcp_value)
- `get_sdlc_phases()` uses fuzzy substring matching, sorted by pattern length descending for specificity
- `get_benchmark_name()` extracts benchmark from task paths by finding segment after "benchmarks/"
- 36 unit tests, all passing; mypy clean
- **Learnings for future iterations:**
  - Fuzzy matching sorted by pattern length descending ensures "swe-bench-pro" matches before "swe-bench"
  - `PurePosixPath` from pathlib handles cross-platform path parsing without OS dependency
  - SDLC phases stored as tuples in the map for immutability; `get_sdlc_phases()` returns a new list each call
---

## 2026-02-02 - US-005
- Implemented `scripts/ccb_pipeline/analyze.py` with `__main__` entry point
- Files created: `scripts/ccb_pipeline/analyze.py`, `tests/test_analyze.py`
- Consumes experiment_metrics.json from US-003 extract step
- Computes per-configuration aggregate metrics: mean reward (with SE), pass rate (with SE), median duration, mean input/output tokens
- Pairwise statistical tests for all config pairs: Welch's t-test for continuous metrics (reward, duration, input_tokens), proportion z-test for pass rate
- Cohen's d effect sizes for continuous metrics with interpretation (negligible/small/medium/large)
- Outputs analysis_results.json with sections: aggregate_metrics, pairwise_tests, effect_sizes
- CLI: `python -m scripts.ccb_pipeline.analyze --input experiment_metrics.json --output analysis_results.json`
- 46 unit tests, all passing; mypy clean
- **Learnings for future iterations:**
  - `scipy.stats.ttest_ind` with `equal_var=False` gives Welch's t-test (more robust for unequal variances)
  - Proportion z-test uses pooled proportion for standard error calculation
  - Cohen's d uses pooled standard deviation; interpretation thresholds: 0.2 small, 0.5 medium, 0.8 large
  - scipy may warn about precision loss with nearly-identical data; this is expected in tests with synthetic data
  - The `_flatten_trials()` helper is reusable for any step that needs to iterate all trials regardless of category/experiment nesting
---
