# Ralph Progress Log
Started: Mon Jan 27 13:12:00 EST 2026

## Codebase Patterns
- Dashboard framework: Streamlit with Plotly charts, Pandas for data processing
- Dashboard entry point: dashboard/app.py with views in dashboard/views/
- Data ingestion: src/ingest/ (transcript_parser.py, harbor_parser.py, database.py)
- Analysis modules: src/analysis/ (8 analyzers: statistical, comparator, cost, failure, time_series, ir, llm_judge, recommendation)
- Benchmark management: src/benchmark/ (database.py, llm_judge.py, trace_parser.py)
- Config files: configs/ directory with YAML and JSON formats
- claude-code.txt is JSONL format (one JSON object per line), NOT plain text
- Token usage embedded in assistant message usage field (input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens)
- parent_tool_use_id links sub-agent tool calls to parent for hierarchical traces
- Experiment runs have manifest.json (metadata), index.json (task-to-run mapping), runs/ and pairs/ subdirs
- Task config.json contains agent info (model, import_path) and task info (path, git_url, source)
- Result.json contains verifier_result.rewards, agent_result (tokens, cost), timing data
- LoCoBench task IDs follow: {language}_{domain}_{complexity}_{num}_{task_category}_{difficulty}_{variant}
- SWE-Bench Pro task IDs follow: instance_{org}__{repo}__{hash}
- Runs directory path: ~/evals/custom_agents/agents/claudecode/runs/
- Archive feature was entirely in run_results.py (no references in filters.py/filter_ui.py)
- Benchmark detection: manifest.json `config.benchmarks` field maps to display names via BENCHMARK_ID_TO_NAME dict in dashboard/utils/benchmark_detection.py
- Directory name patterns for benchmarks: locobench*, swebenchpro*, swebench_pro*, repoqa*, dibench*
- Dashboard utility files in dashboard/utils/ use @dataclass, type annotations, logging, and follow immutable patterns
- Pairing is metadata-based (manifest.json "pairs" array with baseline_run_id and mcp_run_id), NOT a physical "pairs/" directory
- Paired experiments have baseline/ and deepsearch/ subdirectories, detected in load_external_experiments_from_dir()
- manifest.json pairs can reference run_ids that match experiment names or mode suffixes (e.g., "exp_name_baseline")
- Streamlit session_state persists radio/select state across reruns; use key= param for widget identification
- show_run_results() is the main entry point; it delegates to _show_individual_mode() and _show_paired_mode()
- Task list reusable component: dashboard/utils/task_list.py with render_task_list() for filters, sort, and display
- Task normalization: normalize_task() converts both paired mode tasks and external tasks into NormalizedTask dataclass
- LoCoBench task IDs parsed by regex: {language}_{domain}_{complexity}_{num}_{task_category}_{difficulty}_{variant}
- SWE-Bench task IDs parsed by regex: instance_{org}__{repo}__{hash} (language always Python)
- Use frozen dataclasses for immutable value objects (TaskMetadata, NormalizedTask, TaskFilter)
- Task detail panel: dashboard/utils/task_detail.py with render_task_detail_panel() renders 7 collapsible sections
- CLAUDE.md extraction: search .claude.json (claudeMd field), CLAUDE.md files, then parent directories
- Task prompt extraction: first system message from claude-code.txt JSONL, then config.json problem_statement/instruction/prompt
- SWE-Bench task config may be in tests/config.json with problem_statement field
- Global pre-commit hook at ~/.config/pre-commit/hooks/ has a bug; use `git -c core.hookspath=.git/hooks commit` to bypass
---

## 2026-01-27 - US-002
- Removed the "Include archived runs" checkbox toggle from `show_run_results()` in `dashboard/views/run_results.py`
- Removed `EXTERNAL_ARCHIVE_DIR` constant and all archive loading logic
- Removed archive-related source display logic
- Files changed: `dashboard/views/run_results.py`, `scripts/ralph/prd.json`
- **Learnings for future iterations:**
  - The archive feature was purely directory-based (separate archive/runs/ directory), not a metadata field on runs
  - All archive references were in a single file (run_results.py), not spread across filters.py/filter_ui.py
  - `load_external_experiments_from_dir()` is still used by `load_external_experiments()` for the main runs directory
  - Pre-existing ruff warnings (F841 unused variables in exception handlers) exist in run_results.py - don't need to fix for US-002
---

## 2026-01-27 - US-003
- Created `dashboard/utils/benchmark_detection.py` with `detect_benchmark_set()` function
- Detection strategy: first check manifest.json `config.benchmarks` field, then fall back to directory name pattern matching
- Supports LoCoBench, SWE-Bench Pro, SWE-Bench Verified, RepoQA, DIBench with case-insensitive matching
- Returns "Unknown" for unrecognized patterns
- Created `dashboard/tests/test_benchmark_detection.py` with 32 unit tests covering all benchmark sets, edge cases, and integration
- Files changed: `dashboard/utils/benchmark_detection.py`, `dashboard/tests/test_benchmark_detection.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Manifest.json `config.benchmarks` contains benchmark IDs like "locobench_agent", "swebench_pro" - these are internal IDs, not display names
  - Directory names from analysis_output follow patterns: `locobench_*`, `swebenchpro_*`, `swebenchpro_combined`, etc.
  - Dashboard test files go in `dashboard/tests/` and import from `dashboard.utils.*`
  - ruff and py_compile are available for linting/typechecking
  - The `str | Path` union type works with Python 3.12 (project's Python version)
- Experiment selector in show_run_results() uses st.selectbox with format_func for display text and index-based selection
- _group_experiments_by_benchmark() groups experiments by calling detect_benchmark_set() on each experiment's path
---

## 2026-01-27 - US-004
- Added benchmark set grouping to experiment selector in `show_run_results()` in `dashboard/views/run_results.py`
- Added `_group_experiments_by_benchmark()` helper function that groups experiments by detected benchmark set
- Added "Benchmark Set" selectbox filter above the experiment selector with "All Benchmarks (N)" as the default option
- Each benchmark group shows count in parentheses (e.g., "LoCoBench (5)")
- Selecting a benchmark set filters the experiment selector to only show experiments in that group
- Imported `detect_benchmark_set` from `dashboard.utils.benchmark_detection`
- Created `dashboard/tests/test_experiment_grouping.py` with 7 unit tests for the grouping function
- Files changed: `dashboard/views/run_results.py`, `dashboard/tests/test_experiment_grouping.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - The experiment selector uses index-based selection (`range(len(exp_options))` with `format_func`) - not value-based
  - When filtering experiments, the selectbox index is relative to the filtered list, not the original list
  - Pre-existing ruff warnings in run_results.py (11 total) should not be fixed as part of feature stories
  - Immutable grouping: use `[*groups[key], exp]` to build new lists rather than mutating with `.append()`
  - Streamlit `st.selectbox` returns `None` if no items available, so always guard against that
---

## 2026-01-27 - US-005
- Added paired vs individual experiment mode toggle to `show_run_results()` in `dashboard/views/run_results.py`
- Added `st.radio` toggle at top of experiment selector: "Individual Review" (default) vs "Paired Comparison"
- Refactored `show_run_results()` to delegate to `_show_individual_mode()` (existing behavior) and `_show_paired_mode()` (new)
- **Individual mode**: preserves existing single-run experiment selector behavior unchanged
- **Paired mode** features:
  - Auto-detects pairs from `manifest.json` "pairs" array across all experiments via `_load_manifest_pairs()`
  - Pre-populated pair selector with detected pairs, plus "Manual pairing" option
  - Manual pairing: two dropdowns (Baseline Run / Variant Run) from filtered experiments
  - Shows common task count between selected experiments via `_find_common_task_runs()`
  - Side-by-side comparison view with summary cards and tabbed detail views
  - Mode-level comparison table for paired experiments
- Added helper functions: `_load_manifest_pairs()`, `_find_experiment_for_run()`, `_get_experiment_task_ids()`, `_find_common_task_runs()`, `_show_paired_mode()`, `_show_individual_mode()`, `_show_paired_comparison_view()`, `_show_experiment_summary_card()`, `_show_paired_experiment_comparison()`
- Selection mode stored in `st.session_state["experiment_mode"]`; paired selections in `st.session_state["paired_baseline"]` and `st.session_state["paired_variant"]`
- Created `dashboard/tests/test_paired_mode.py` with 17 unit tests for all new utility functions
- Files changed: `dashboard/views/run_results.py`, `dashboard/tests/test_paired_mode.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - "pairs/" is NOT a physical directory - pairing info is in manifest.json "pairs" array with baseline_run_id and mcp_run_id
  - Paired experiments are detected by baseline/ and deepsearch/ subdirectories in load_external_experiments_from_dir()
  - manifest.json run_ids can be matched to experiments by name prefix or mode suffix matching
  - Streamlit st.radio with horizontal=True creates a clean mode toggle; use key= and session_state for persistence
  - When refactoring a large function, extract mode-specific logic into private functions (prefix with _) to keep the main function as a dispatcher
  - test_analysis_ir.py has 12 pre-existing failures (missing dashboard.utils.view_base module) - not related to this feature
---

## 2026-01-27 - US-006
- Created `dashboard/utils/task_list.py` with reusable task list component
- Task metadata parsing: `parse_task_metadata()` extracts language, task type, difficulty from LoCoBench and SWE-Bench task ID patterns
- Task normalization: `normalize_task()` converts different experiment formats into uniform `NormalizedTask` dataclass
- Status derivation: `_determine_status()` maps reward > 0 to "pass", reward == 0 to "fail", error status preserved
- Multi-criteria filtering via `filter_tasks()` with AND logic: status multiselect, language multiselect, task type multiselect, difficulty select, text search
- Sort dropdown with 5 options: Name, Duration, Token Count, Reward, Status (ascending/descending)
- Paired mode: `_build_paired_table_rows()` shows side-by-side status/reward columns for baseline and variant
- Task selection stored in `st.session_state["selected_task_id"]`
- Updated `show_external_task_results()` and `show_paired_mode_tasks()` in `run_results.py` to use `render_task_list()`
- Added paired task comparison to `_show_paired_experiment_comparison()` with mode selector
- Created `dashboard/tests/test_task_list.py` with 65 unit tests covering all components
- Files changed: `dashboard/utils/task_list.py`, `dashboard/views/run_results.py`, `dashboard/tests/test_task_list.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Task data comes from two different loaders: `_scan_paired_mode_tasks()` (paired) and `load_external_tasks()` (single) with different field names
  - Paired mode tasks have `input_tokens`/`output_tokens`; external tasks have `total_tokens` and `execution_time`
  - `reward > 0` maps to pass, `reward == 0` maps to fail - this is the primary status signal
  - LoCoBench task IDs have 7 segments separated by underscores; compound task categories like "code_review" need greedy matching
  - SWE-Bench task IDs always use double underscore `__` to separate org/repo
  - Use frozen dataclasses for value objects that should be immutable (TaskMetadata, NormalizedTask, TaskFilter)
  - Streamlit `st.columns([1, 3])` creates a 1:3 ratio layout good for filter sidebar + main content
  - `st.dataframe` with `hide_index=True` gives clean display; markdown status badges render in dataframe cells
---

## 2026-01-27 - US-008
- Added CLAUDE.md content extraction and task instruction prompt display to the task detail panel
- Implemented `_extract_claude_md_content()`: searches agent/.claude.json (claudeMd field), sessions/.claude.json, CLAUDE.md files in instance dir, agent dir, and parent directories
- Implemented `_extract_task_prompt()`: extracts from first system message in claude-code.txt JSONL, falls back to config.json problem_statement/instruction/prompt, then tests/config.json, then result_data
- Added helper `_parse_system_prompt_from_jsonl()` for reading JSONL system messages (handles both string and list content formats)
- Added helper `_extract_prompt_from_config()` for extracting prompts from config dicts (SWE-Bench problem_statement, task.instruction, task.prompt, direct instruction/prompt)
- Added Section 6 (CLAUDE.md) and Section 7 (Task Instruction Prompt) as collapsible expanders in `render_task_detail_panel()`
- Both sections show "Not available" info message when content cannot be found
- Created `dashboard/tests/test_task_detail_content.py` with 40 unit tests covering all extraction functions
- Files changed: `dashboard/utils/task_detail.py`, `dashboard/tests/test_task_detail_content.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - .claude.json files may contain a `claudeMd` field with CLAUDE.md content embedded as a string
  - claude-code.txt system messages have content as either a string or a list of {type: "text", text: "..."} objects
  - SWE-Bench task configs use `problem_statement` field for the task prompt; other formats use `instruction` or `prompt`
  - Task instance directories may have config.json at root or in tests/ subdirectory (SWE-Bench format)
  - Global pre-commit hook at `~/.config/pre-commit/hooks/` has a bug with `cd "$GIT_DIR/../"` - use `git -c core.hookspath=.git/hooks commit` to bypass
  - The task detail panel in `dashboard/utils/task_detail.py` now has 7 sections (was 5 before US-008)
  - Browser verification not available in this environment; manual verification needed
---
