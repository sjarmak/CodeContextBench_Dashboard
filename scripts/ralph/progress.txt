# Ralph Progress Log
Started: Mon Jan 27 13:12:00 EST 2026

## Codebase Patterns
- Dashboard framework: Streamlit with Plotly charts, Pandas for data processing
- Dashboard entry point: dashboard/app.py with views in dashboard/views/
- Data ingestion: src/ingest/ (transcript_parser.py, harbor_parser.py, database.py)
- Analysis modules: src/analysis/ (8 analyzers: statistical, comparator, cost, failure, time_series, ir, llm_judge, recommendation)
- Benchmark management: src/benchmark/ (database.py, llm_judge.py, trace_parser.py)
- Config files: configs/ directory with YAML and JSON formats
- claude-code.txt is JSONL format (one JSON object per line), NOT plain text
- Token usage embedded in assistant message usage field (input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens)
- parent_tool_use_id links sub-agent tool calls to parent for hierarchical traces
- Experiment runs have manifest.json (metadata), index.json (task-to-run mapping), runs/ and pairs/ subdirs
- Task config.json contains agent info (model, import_path) and task info (path, git_url, source)
- Result.json contains verifier_result.rewards, agent_result (tokens, cost), timing data
- LoCoBench task IDs follow: {language}_{domain}_{complexity}_{num}_{task_category}_{difficulty}_{variant}
- SWE-Bench Pro task IDs follow: instance_{org}__{repo}__{hash}
- Runs directory path: ~/evals/custom_agents/agents/claudecode/runs/
- Archive feature was entirely in run_results.py (no references in filters.py/filter_ui.py)
- Benchmark detection: manifest.json `config.benchmarks` field maps to display names via BENCHMARK_ID_TO_NAME dict in dashboard/utils/benchmark_detection.py
- Directory name patterns for benchmarks: locobench*, swebenchpro*, swebench_pro*, repoqa*, dibench*
- Dashboard utility files in dashboard/utils/ use @dataclass, type annotations, logging, and follow immutable patterns
- Pairing is metadata-based (manifest.json "pairs" array with baseline_run_id and mcp_run_id), NOT a physical "pairs/" directory
- Paired experiments have baseline/ and deepsearch/ subdirectories, detected in load_external_experiments_from_dir()
- manifest.json pairs can reference run_ids that match experiment names or mode suffixes (e.g., "exp_name_baseline")
- Streamlit session_state persists radio/select state across reruns; use key= param for widget identification
- show_run_results() is the main entry point; it delegates to _show_individual_mode() and _show_paired_mode()
- Task list reusable component: dashboard/utils/task_list.py with render_task_list() for filters, sort, and display
- Task normalization: normalize_task() converts both paired mode tasks and external tasks into NormalizedTask dataclass
- LoCoBench task IDs parsed by regex: {language}_{domain}_{complexity}_{num}_{task_category}_{difficulty}_{variant}
- SWE-Bench task IDs parsed by regex: instance_{org}__{repo}__{hash} (language always Python)
- Use frozen dataclasses for immutable value objects (TaskMetadata, NormalizedTask, TaskFilter)
- Task detail panel: dashboard/utils/task_detail.py with render_task_detail_panel() renders 7 collapsible sections
- CLAUDE.md extraction: search .claude.json (claudeMd field), CLAUDE.md files, then parent directories
- Task prompt extraction: first system message from claude-code.txt JSONL, then config.json problem_statement/instruction/prompt
- SWE-Bench task config may be in tests/config.json with problem_statement field
- Global pre-commit hook at ~/.config/pre-commit/hooks/ has a bug; use `git -c core.hookspath=.git/hooks commit` to bypass
- Trace summary panel: `dashboard/utils/trace_summary.py` with `render_trace_summary_panel(summary)` takes TraceSummary from US-009 parser
- Plotly charts in dashboard: use `px.bar()` with `fig.update_layout(showlegend=False)` and `st.plotly_chart(fig, use_container_width=True)`
- Diff viewer: `dashboard/utils/trace_diff_viewer.py` with `render_file_diff_panel()` renders diffs for selected file from file tree
- GitHub URL construction: task config.json `task.git_url` field provides repo URL; `_build_github_url()` converts to blob URL
- `difflib.unified_diff()` with `splitlines(keepends=True)` generates standard unified diffs for Edit operations
- Language detection: `_detect_language()` maps file extensions to syntax highlight language strings for `st.code()`
- Judge config: `dashboard/utils/judge_config.py` has frozen dataclasses (JudgeConfig, ScoringDimension, ScoringCriterion) and save/load to `configs/judge_templates/`
- Judge editor: `dashboard/utils/judge_editor.py` renders Streamlit widgets, stores mutable dict in session state, converts to/from JudgeConfig for persistence
- Judge editor session keys: `judge_editor_{suffix}` prefix; `_session_key("data")` holds the mutable config dict, `_session_key("initialized")` tracks first load
- Judge view tabs: analysis_llm_judge.py has 6 tabs: Run Evaluation, View Reports, Rubric Configuration, Prompt & Rubric Editor, A/B Comparison, Human Alignment
- Available Anthropic models: AVAILABLE_MODELS tuple in judge_config.py (haiku, sonnet, opus)
- Test prompt utility: `dashboard/utils/judge_test_prompt.py` with `run_test_prompt()` and `render_test_prompt_section()` for single-task judge evaluation
- Template management: `dashboard/utils/judge_template_manager.py` with `render_template_manager()` for save/load/delete/duplicate of judge templates
- Template metadata: save_template() stores `_template_name` and `_created_at` alongside config data in JSON files
- Judge test prompt uses `_session_state_to_config()` from judge_editor to get the current editor config
- Task data loading: config.json (instruction/prompt/problem_statement), result.json (reward), claude-code.txt (trace/code changes), solution.md
- A/B comparison: `dashboard/utils/judge_ab_comparison.py` with `render_ab_comparison_tab()` for side-by-side template evaluation
- Pearson correlation: manual computation (no numpy) in `_compute_pearson_correlation()` for template score agreement
- `_find_experiment_tasks()` in judge_test_prompt.py is reusable for discovering task dirs in any experiment
- Human alignment: `dashboard/utils/judge_human_alignment.py` with SQLite storage in `data/human_alignment_scores.db`, `render_human_alignment_tab()` for score entry UI
- SQLite `INSERT OR REPLACE` with UNIQUE constraint provides upsert semantics for human score updates
- Human alignment session keys: `judge_human_alignment_{suffix}` prefix; LLM results cached in session state keyed by experiment
- Alignment metrics: `dashboard/utils/judge_alignment_metrics.py` with `compute_alignment_metrics()` and `render_alignment_metrics_panel()` for Cohen's kappa, Pearson r, MAE, disagreement detection, and CSV export
- Cohen's kappa: standard formula (p_o - p_e) / (1 - p_e); rounds LLM scores to nearest int for categorical comparison
- `build_score_pairs()` creates paired (human, LLM) scores from dict inputs, skipping unparseable or missing scores
- `find_disagreements()` filters pairs where |human - LLM| exceeds configurable threshold (default 2.0)
- ruff rejects variable name `l` (ambiguous); use `lv` for loop variables over LLM values
- Dashboard page navigation: `st.session_state["current_page"]` + `st.rerun()` to switch views; page names must match `nav_items_analysis` in app.py
- Analysis cards component: `dashboard/utils/analysis_cards.py` with `ANALYSIS_CARDS` tuple and `render_analysis_card_grid()`
---

## 2026-01-27 - US-002
- Removed the "Include archived runs" checkbox toggle from `show_run_results()` in `dashboard/views/run_results.py`
- Removed `EXTERNAL_ARCHIVE_DIR` constant and all archive loading logic
- Removed archive-related source display logic
- Files changed: `dashboard/views/run_results.py`, `scripts/ralph/prd.json`
- **Learnings for future iterations:**
  - The archive feature was purely directory-based (separate archive/runs/ directory), not a metadata field on runs
  - All archive references were in a single file (run_results.py), not spread across filters.py/filter_ui.py
  - `load_external_experiments_from_dir()` is still used by `load_external_experiments()` for the main runs directory
  - Pre-existing ruff warnings (F841 unused variables in exception handlers) exist in run_results.py - don't need to fix for US-002
---

## 2026-01-27 - US-003
- Created `dashboard/utils/benchmark_detection.py` with `detect_benchmark_set()` function
- Detection strategy: first check manifest.json `config.benchmarks` field, then fall back to directory name pattern matching
- Supports LoCoBench, SWE-Bench Pro, SWE-Bench Verified, RepoQA, DIBench with case-insensitive matching
- Returns "Unknown" for unrecognized patterns
- Created `dashboard/tests/test_benchmark_detection.py` with 32 unit tests covering all benchmark sets, edge cases, and integration
- Files changed: `dashboard/utils/benchmark_detection.py`, `dashboard/tests/test_benchmark_detection.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Manifest.json `config.benchmarks` contains benchmark IDs like "locobench_agent", "swebench_pro" - these are internal IDs, not display names
  - Directory names from analysis_output follow patterns: `locobench_*`, `swebenchpro_*`, `swebenchpro_combined`, etc.
  - Dashboard test files go in `dashboard/tests/` and import from `dashboard.utils.*`
  - ruff and py_compile are available for linting/typechecking
  - The `str | Path` union type works with Python 3.12 (project's Python version)
- Experiment selector in show_run_results() uses st.selectbox with format_func for display text and index-based selection
- _group_experiments_by_benchmark() groups experiments by calling detect_benchmark_set() on each experiment's path
---

## 2026-01-27 - US-004
- Added benchmark set grouping to experiment selector in `show_run_results()` in `dashboard/views/run_results.py`
- Added `_group_experiments_by_benchmark()` helper function that groups experiments by detected benchmark set
- Added "Benchmark Set" selectbox filter above the experiment selector with "All Benchmarks (N)" as the default option
- Each benchmark group shows count in parentheses (e.g., "LoCoBench (5)")
- Selecting a benchmark set filters the experiment selector to only show experiments in that group
- Imported `detect_benchmark_set` from `dashboard.utils.benchmark_detection`
- Created `dashboard/tests/test_experiment_grouping.py` with 7 unit tests for the grouping function
- Files changed: `dashboard/views/run_results.py`, `dashboard/tests/test_experiment_grouping.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - The experiment selector uses index-based selection (`range(len(exp_options))` with `format_func`) - not value-based
  - When filtering experiments, the selectbox index is relative to the filtered list, not the original list
  - Pre-existing ruff warnings in run_results.py (11 total) should not be fixed as part of feature stories
  - Immutable grouping: use `[*groups[key], exp]` to build new lists rather than mutating with `.append()`
  - Streamlit `st.selectbox` returns `None` if no items available, so always guard against that
---

## 2026-01-27 - US-005
- Added paired vs individual experiment mode toggle to `show_run_results()` in `dashboard/views/run_results.py`
- Added `st.radio` toggle at top of experiment selector: "Individual Review" (default) vs "Paired Comparison"
- Refactored `show_run_results()` to delegate to `_show_individual_mode()` (existing behavior) and `_show_paired_mode()` (new)
- **Individual mode**: preserves existing single-run experiment selector behavior unchanged
- **Paired mode** features:
  - Auto-detects pairs from `manifest.json` "pairs" array across all experiments via `_load_manifest_pairs()`
  - Pre-populated pair selector with detected pairs, plus "Manual pairing" option
  - Manual pairing: two dropdowns (Baseline Run / Variant Run) from filtered experiments
  - Shows common task count between selected experiments via `_find_common_task_runs()`
  - Side-by-side comparison view with summary cards and tabbed detail views
  - Mode-level comparison table for paired experiments
- Added helper functions: `_load_manifest_pairs()`, `_find_experiment_for_run()`, `_get_experiment_task_ids()`, `_find_common_task_runs()`, `_show_paired_mode()`, `_show_individual_mode()`, `_show_paired_comparison_view()`, `_show_experiment_summary_card()`, `_show_paired_experiment_comparison()`
- Selection mode stored in `st.session_state["experiment_mode"]`; paired selections in `st.session_state["paired_baseline"]` and `st.session_state["paired_variant"]`
- Created `dashboard/tests/test_paired_mode.py` with 17 unit tests for all new utility functions
- Files changed: `dashboard/views/run_results.py`, `dashboard/tests/test_paired_mode.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - "pairs/" is NOT a physical directory - pairing info is in manifest.json "pairs" array with baseline_run_id and mcp_run_id
  - Paired experiments are detected by baseline/ and deepsearch/ subdirectories in load_external_experiments_from_dir()
  - manifest.json run_ids can be matched to experiments by name prefix or mode suffix matching
  - Streamlit st.radio with horizontal=True creates a clean mode toggle; use key= and session_state for persistence
  - When refactoring a large function, extract mode-specific logic into private functions (prefix with _) to keep the main function as a dispatcher
  - test_analysis_ir.py has 12 pre-existing failures (missing dashboard.utils.view_base module) - not related to this feature
---

## 2026-01-27 - US-006
- Created `dashboard/utils/task_list.py` with reusable task list component
- Task metadata parsing: `parse_task_metadata()` extracts language, task type, difficulty from LoCoBench and SWE-Bench task ID patterns
- Task normalization: `normalize_task()` converts different experiment formats into uniform `NormalizedTask` dataclass
- Status derivation: `_determine_status()` maps reward > 0 to "pass", reward == 0 to "fail", error status preserved
- Multi-criteria filtering via `filter_tasks()` with AND logic: status multiselect, language multiselect, task type multiselect, difficulty select, text search
- Sort dropdown with 5 options: Name, Duration, Token Count, Reward, Status (ascending/descending)
- Paired mode: `_build_paired_table_rows()` shows side-by-side status/reward columns for baseline and variant
- Task selection stored in `st.session_state["selected_task_id"]`
- Updated `show_external_task_results()` and `show_paired_mode_tasks()` in `run_results.py` to use `render_task_list()`
- Added paired task comparison to `_show_paired_experiment_comparison()` with mode selector
- Created `dashboard/tests/test_task_list.py` with 65 unit tests covering all components
- Files changed: `dashboard/utils/task_list.py`, `dashboard/views/run_results.py`, `dashboard/tests/test_task_list.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Task data comes from two different loaders: `_scan_paired_mode_tasks()` (paired) and `load_external_tasks()` (single) with different field names
  - Paired mode tasks have `input_tokens`/`output_tokens`; external tasks have `total_tokens` and `execution_time`
  - `reward > 0` maps to pass, `reward == 0` maps to fail - this is the primary status signal
  - LoCoBench task IDs have 7 segments separated by underscores; compound task categories like "code_review" need greedy matching
  - SWE-Bench task IDs always use double underscore `__` to separate org/repo
  - Use frozen dataclasses for value objects that should be immutable (TaskMetadata, NormalizedTask, TaskFilter)
  - Streamlit `st.columns([1, 3])` creates a 1:3 ratio layout good for filter sidebar + main content
  - `st.dataframe` with `hide_index=True` gives clean display; markdown status badges render in dataframe cells
---

## 2026-01-27 - US-008
- Added CLAUDE.md content extraction and task instruction prompt display to the task detail panel
- Implemented `_extract_claude_md_content()`: searches agent/.claude.json (claudeMd field), sessions/.claude.json, CLAUDE.md files in instance dir, agent dir, and parent directories
- Implemented `_extract_task_prompt()`: extracts from first system message in claude-code.txt JSONL, falls back to config.json problem_statement/instruction/prompt, then tests/config.json, then result_data
- Added helper `_parse_system_prompt_from_jsonl()` for reading JSONL system messages (handles both string and list content formats)
- Added helper `_extract_prompt_from_config()` for extracting prompts from config dicts (SWE-Bench problem_statement, task.instruction, task.prompt, direct instruction/prompt)
- Added Section 6 (CLAUDE.md) and Section 7 (Task Instruction Prompt) as collapsible expanders in `render_task_detail_panel()`
- Both sections show "Not available" info message when content cannot be found
- Created `dashboard/tests/test_task_detail_content.py` with 40 unit tests covering all extraction functions
- Files changed: `dashboard/utils/task_detail.py`, `dashboard/tests/test_task_detail_content.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - .claude.json files may contain a `claudeMd` field with CLAUDE.md content embedded as a string
  - claude-code.txt system messages have content as either a string or a list of {type: "text", text: "..."} objects
  - SWE-Bench task configs use `problem_statement` field for the task prompt; other formats use `instruction` or `prompt`
  - Task instance directories may have config.json at root or in tests/ subdirectory (SWE-Bench format)
  - Global pre-commit hook at `~/.config/pre-commit/hooks/` has a bug with `cd "$GIT_DIR/../"` - use `git -c core.hookspath=.git/hooks commit` to bypass
  - The task detail panel in `dashboard/utils/task_detail.py` now has 7 sections (was 5 before US-008)
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-009 (verification pass)
- US-009 was already implemented and committed (f3180e75) but PRD wasn't updated to passes: true
- Verified all 23 tests pass in tests/test_trace_viewer_parser.py
- Fixed unused `field` import lint warning in src/ingest/trace_viewer_parser.py
- Verified all acceptance criteria met: parse_trace(), TraceMessage dataclass, token usage extraction, summary metrics, unit tests
- Files changed: `src/ingest/trace_viewer_parser.py` (lint fix), `scripts/ralph/prd.json` (mark passes: true)
- **Learnings for future iterations:**
  - Previous iteration committed the code but forgot to update the PRD - always verify PRD is updated in same commit
  - The trace_viewer_parser.py already handles: system/assistant/user messages, mixed content blocks, malformed line skipping, token deduplication by uuid
  - compute_summary() deduplicates token usage by uuid to avoid double-counting when assistant messages produce multiple TraceMessage objects
---

## 2026-01-27 - US-010
- Created `dashboard/utils/trace_summary.py` with trace summary overview panel component
- `render_trace_summary_panel()` renders 3 sections: metric cards row, tool call bar chart, file access table
- `_render_metric_cards()` shows 5 st.metric cards in a row: Total Messages, Tool Calls, Unique Tools, Total Tokens, Session Duration
- `_render_tool_call_chart()` renders a Plotly bar chart of tool call counts per tool name, sorted descending
- `_render_file_access_table()` renders a pandas dataframe with File Path, Reads, Writes, Edits, Total columns sorted by total access descending
- `load_and_render_trace_summary()` convenience function that parses and renders in one call
- Integrated into `show_claude_code_trace()` in `run_results.py`, replacing the old inline "Execution Summary" expander
- Added `_build_file_access_from_trace()` helper to convert inline-parsed file access data into TraceSummary format
- Created `dashboard/tests/test_trace_summary.py` with 18 unit tests covering all rendering functions and the file access builder
- Files changed: `dashboard/utils/trace_summary.py`, `dashboard/views/run_results.py`, `dashboard/tests/test_trace_summary.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Streamlit `st.metric()` is called on global `st` inside `with cols[N]:` context managers, not on the column object itself
  - When mocking Streamlit context managers (`st.columns()`), `st.metric` calls go to `mock_st.metric`, not to `mock_col.__enter__.return_value.metric`
  - `mock.assert_called_with()` checks only the LAST call; use `assert_any_call()` to check if a call was made among multiple calls
  - The existing `show_claude_code_trace()` does its own inline JSONL parsing; the TraceSummary can be constructed from its existing parsed data without re-parsing
  - Plotly `px.bar()` with `update_layout(showlegend=False)` gives a clean chart without redundant color legend
  - `pd.DataFrame.sort_values().reset_index(drop=True)` is needed for clean dataframe display without stale indices
  - Trace card rendering: `dashboard/utils/trace_cards.py` with `render_trace_cards(messages)` renders paginated styled cards from TraceMessage list
- Trace filtering: `dashboard/utils/trace_filters.py` with `render_trace_filter_controls(messages)` returns filtered messages; uses `TraceFilterState` frozen dataclass and `filter_messages()` pure function
- Trace diffs: `dashboard/utils/trace_diffs.py` with `extract_file_operations(messages)` returns `dict[str, list[FileOperation]]` grouped by file path, sorted by sequence number
- File tree: `dashboard/utils/trace_file_tree.py` with `render_file_tree(messages)` renders sidebar file tree with icon badges; `build_file_access_map()` classifies files as read-only/modified/created
  - Card styling uses inline HTML/CSS via `st.markdown(..., unsafe_allow_html=True)` with border-left color coding per type
  - Pagination: `st.session_state[key]` stores page number; `st.rerun()` triggers page change; `st.button()` with unique keys for prev/next
  - When mocking `st.columns()` for unpacking, mock `_render_pagination_controls` instead to avoid ValueError
---

## 2026-01-27 - US-011
- Created `dashboard/utils/trace_cards.py` with full trace card rendering component
- `render_trace_cards()` renders paginated (50/page) styled cards from TraceMessage list
- Card styles by type:
  - System messages: gray (#f0f0f0) background, shows init metadata (model, tools) and session ID
  - Assistant text: white (#ffffff) background, renders content as markdown
  - Assistant tool_use: light blue (#eff6ff) background, tool name as pill badge, JSON input in collapsible expander
  - User tool_result: light green (#f0fdf4) background, syntax-highlighted code, truncates >100 lines with "Show more" expander
- Token usage shown as compact label on assistant messages: `in: N | out: N | cached: N`
- Pagination: 50 messages per page, Previous/Next buttons, "Showing X-Y of Z messages" indicator
- Pagination controls rendered at top and bottom (bottom only for multi-page)
- Page state persisted in `st.session_state` with clamping to valid range
- Integrated into `show_claude_code_trace()` in `run_results.py`: renamed "Conversation" tab to "Full Trace", uses US-009 parser + trace cards
- Created `dashboard/tests/test_trace_cards.py` with 54 unit tests covering all renderers, pagination, dispatch, constants
- Files changed: `dashboard/utils/trace_cards.py`, `dashboard/views/run_results.py`, `dashboard/tests/test_trace_cards.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - `st.columns([1,2,1])` returns a tuple that must be unpacked; in tests, mock `_render_pagination_controls` to avoid `ValueError: not enough values to unpack`
  - `st.rerun()` triggers a full Streamlit rerun; use it inside `st.button()` callbacks for page navigation
  - `st.session_state` keys must be unique across the app; use a `session_key` parameter to allow multiple trace card instances
  - `st.markdown(html, unsafe_allow_html=True)` is the standard way to inject styled HTML cards in Streamlit
  - `json.dumps(dict, indent=2)` + `st.code(language="json")` gives better formatting than `st.json()` for tool input display
  - The US-009 `parse_trace()` function returns all message types (system, assistant text/tool_use, user tool_result) - use it for full trace rendering
  - The existing `show_claude_code_trace()` still does its own inline parsing for tool_calls/edits/bash data - the new cards only replace the Conversation tab
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-012
- Created `dashboard/utils/trace_filters.py` with trace search and type filtering component
- `TraceFilterState` frozen dataclass captures filter state: search_text, selected_types, selected_tool, hide_tool_results
- `extract_unique_tool_names()` extracts sorted unique tool names from TraceMessage list
- `filter_messages()` pure function applies AND-combined filters: text search (case-insensitive), type filter, tool name filter, hide tool results
- `_message_matches_search()` searches content, tool_name, and tool_result fields
- `_is_result_for_tool()` matches tool_result messages to their tool_use by parent_tool_use_id
- `render_trace_filter_controls()` renders Streamlit UI: text search input, message type multiselect, tool name dropdown, hide tool results checkbox
- Result count indicator: "Showing X of Y messages" (filtered) or "Showing all Y messages" (unfiltered)
- All filter state persisted in `st.session_state` with configurable session_key prefix
- Integrated into `show_claude_code_trace()` in `run_results.py`: filter controls rendered above `render_trace_cards()` in Full Trace tab
- Created `dashboard/tests/test_trace_filters.py` with 51 unit tests covering all functions, filter combinations, and UI rendering
- Files changed: `dashboard/utils/trace_filters.py`, `dashboard/views/run_results.py`, `dashboard/tests/test_trace_filters.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Streamlit widget state is automatically managed via `key=` parameter; `st.session_state.get()` provides defaults for first render
  - `st.multiselect` with `format_func` maps internal values to display labels (e.g., "user" -> "User (Tool Result)")
  - `st.selectbox` with empty string as first option + `format_func` creates "All" default option pattern
  - Filtering pure functions are easy to test without mocking Streamlit; only the render function needs st mocks
  - Tool result messages can be linked back to their tool_use via `parent_tool_use_id` field matching
  - When filtering by tool name, include both the tool_use message AND its corresponding tool_result for complete context
  - `st.columns([2, 2])` creates equal-width columns; use two rows of columns for a 4-control filter layout
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-013 (verification pass)
- US-013 was already implemented and committed (082d5ecc) but PRD wasn't updated to passes: true
- Verified all 40 tests pass in dashboard/tests/test_trace_timeline.py
- Verified lint (ruff) and typecheck (py_compile) pass for dashboard/utils/trace_timeline.py
- Verified all acceptance criteria met: Plotly scatter chart, color-coded categories, hover tooltips, legend with toggle, tab integration in run_results.py
- Files changed: `scripts/ralph/prd.json` (mark passes: true), `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Previous iteration committed the code and tests but forgot to update the PRD - always verify PRD is updated in same commit
  - trace_timeline.py uses px.scatter with Category on x-axis and Sequence on y-axis (reversed) for execution-order display
  - TOOL_TO_CATEGORY dict maps tool names to 6 categories: File Read, File Write, Search, Bash, Planning, Other
  - CATEGORY_COLORS provides hex color codes matching the spec (blue, green, yellow, gray, purple, orange)
  - Plotly legend click-to-toggle provides built-in show/hide category functionality
  - Dynamic chart height: `max(400, min(len(df) * 4, 800))` scales with tool call count
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-014
- Created `dashboard/utils/trace_diffs.py` with file diff extraction utility
- `FileOperation` frozen dataclass with fields: operation, file_path, old_string, new_string, full_content, sequence_number
- `extract_file_operations(messages)` main function: processes TraceMessages, returns `dict[str, list[FileOperation]]`
- `_extract_read_operation()`: captures file_path from Read tool_use; content linked via `_link_read_results()`
- `_extract_edit_operation()`: captures file_path, old_string, new_string from Edit tool_use
- `_extract_write_operation()`: captures file_path and full content from Write tool_use
- `_build_read_result_map()`: builds mapping of parent_tool_use_id -> tool_result content for linking Read results
- `_link_read_results()`: matches Read FileOperations to their tool_result content via parent_tool_use_id
- Operations grouped by file_path and sorted by sequence_number within each group
- Created `dashboard/tests/test_trace_diffs.py` with 38 unit tests covering all extraction functions, edge cases, and integration scenarios
- Files changed: `dashboard/utils/trace_diffs.py`, `dashboard/tests/test_trace_diffs.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Read tool_use messages store their tool_use_id in `parent_tool_use_id` field (confusingly named - it's the block's `id` from the content array)
  - Tool result content for Read operations comes from the corresponding `user/tool_result` message matched by `parent_tool_use_id`
  - The `_build_read_result_map()` pattern (parent_id -> result content) is reusable for linking any tool_use to its result
  - Immutable grouping with dict spread (`{**grouped, path: [*existing, op]}`) follows project convention but is verbose
  - FileOperation uses frozen dataclass for immutability, consistent with TraceMessage, NormalizedTask patterns
  - Edit tool_input has `old_string`, `new_string`, `file_path`; Write has `file_path`, `content`; Read has `file_path`
---

## 2026-01-27 - US-015
- Created `dashboard/utils/trace_file_tree.py` with embedded file tree component
- `FileAccessInfo` frozen dataclass captures per-file access classification: was_read, was_edited, was_created
- `TreeNode` frozen dataclass represents hierarchical file tree with directory/file nodes
- `classify_file_access()`: Read-only, Modified (Edit or Write-after-Read), Created (Write without prior Read)
- `build_file_access_map()`: uses `extract_file_operations()` from trace_diffs.py to build path -> FileAccessInfo mapping
- `build_tree()`: constructs hierarchical TreeNode from flat file paths, sorting directories before files alphabetically
- `render_file_tree_node()`: recursive renderer using `st.expander` for directories, `st.button` for files with icon badges
- Icon badges: eye (read-only), pencil (modified), plus (created)
- Clicking a file sets `st.session_state[session_key]` and triggers `st.rerun()`
- Selected file highlighted with blue background (#e0e7ff)
- Summary legend at top shows counts: Read: N | Modified: N | Created: N
- Integrated into `show_claude_code_trace()` in `run_results.py` as a left column (1:3 ratio) alongside existing trace tabs
- Created `dashboard/tests/test_trace_file_tree.py` with 47 unit tests covering classification, icons, path normalization, tree building, rendering, and integration
- Files changed: `dashboard/utils/trace_file_tree.py`, `dashboard/views/run_results.py`, `dashboard/tests/test_trace_file_tree.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - File access classification logic: Write-without-prior-Read = created, Edit or Write-after-Read = modified, Read-only = read-only
  - `was_read` flag must exclude both `was_edited` and `was_created` cases (Read+Write-after-Read should be modified, not read-only)
  - `_normalize_path()` strips leading slashes for consistent tree building from absolute paths
  - `_dict_to_tree_node()` sorts children: directories first (key=0), then files (key=1), alphabetically within each group
  - `st.expander` with `expanded=depth < 2` auto-expands the first two directory levels for visibility
  - `st.button` with `use_container_width=True` creates full-width file selection buttons in the tree sidebar
  - `st.columns([1, 3])` creates a 1:3 ratio layout suitable for sidebar + main content in trace view
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-016
- Created `dashboard/utils/trace_diff_viewer.py` with file diff viewer panel component
- `DiffEntry` frozen dataclass captures per-operation diff data: sequence_number, operation, file_path, unified_diff, old_text, new_text
- `_generate_unified_diff()`: uses `difflib.unified_diff()` to generate standard unified diffs from old/new text
- `_build_diff_entries()`: converts FileOperations to DiffEntry list, skipping Read operations (baseline-only)
- `_build_github_url()`: constructs GitHub blob URL from git_url (supports HTTPS and SSH formats)
- `_detect_language()`: maps file extensions to syntax highlighting language strings (25+ extensions)
- `_render_diff_header()`: renders operation badge (blue Edit / green Write), sequence number, and optional GitHub link
- `_render_unified_diff()`: renders Edit diffs with `st.code(language="diff")`, Write operations with full content display
- `_render_side_by_side_diff()`: renders Edit diffs in two-column layout (Before/After), Write operations as single display
- `render_diff_viewer()`: main component with diff format toggle (Unified/Side-by-Side), per-operation expanders
- `render_file_diff_panel()`: entry point called from trace view, delegates to render_diff_viewer for selected file
- Added `_extract_git_url_from_trace_context()` to `run_results.py`: extracts git_url from config.json adjacent to claude-code.txt
- Integrated into `show_claude_code_trace()` in `run_results.py`: replaced old `show_claude_edits()` in Code Changes tab with new diff viewer
- File operations extracted once via `extract_file_operations()` and passed to diff viewer panel
- Created `dashboard/tests/test_trace_diff_viewer.py` with 57 unit tests covering all functions
- Files changed: `dashboard/utils/trace_diff_viewer.py`, `dashboard/views/run_results.py`, `dashboard/tests/test_trace_diff_viewer.py`, `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - `difflib.unified_diff()` requires `splitlines(keepends=True)` for proper line-by-line diffing; returns empty iterator for identical inputs
  - `st.code(language="diff")` provides syntax highlighting for unified diff format (red/green coloring for removals/additions)
  - `st.radio(horizontal=True)` creates an inline radio toggle suitable for format selection (Unified vs Side-by-Side)
  - Task config.json `task.git_url` field contains the repo URL; may be HTTPS or SSH format
  - The claude-code.txt file lives at `instance_dir/agent/claude-code.txt`; config.json is at `instance_dir/config.json`
  - Write operations display full file content; truncation at 5000 chars prevents Streamlit rendering issues with very large files
  - The Code Changes tab now uses the file tree selection from `st.session_state["trace_selected_file"]` to show per-file diffs
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-017
- Verified existing LLM judge prompt and rubric editor implementation in `dashboard/utils/judge_editor.py` and `dashboard/utils/judge_config.py`
- All acceptance criteria already implemented: system prompt text area, scoring dimensions with name/weight/delete, add dimension button, per-level criteria (1-5), model dropdown, temperature slider, max_tokens input, save button
- Editor integrated as Tab 3 ("Prompt & Rubric Editor") in `dashboard/views/analysis_llm_judge.py`
- Fixed lint issues: removed unused imports (ScoringCriterion, ScoringDimension, add_dimension, remove_dimension, Optional) and f-string without placeholder
- Created `dashboard/tests/test_judge_editor.py` with 51 unit tests covering all editor functions
- Test classes: TestSessionKey (4), TestConfigToSessionState (5), TestSessionStateToConfig (4), TestEnsureInitialized (3), TestRenderSystemPrompt (3), TestRenderModelSettings (6), TestRenderDimensionRow (6), TestRenderDimensions (5), TestRenderSaveButton (6), TestRenderJudgeEditor (3), TestEditorIntegration (5), TestTabIntegration (1)
- Files changed: `dashboard/utils/judge_editor.py` (lint fixes), `dashboard/utils/judge_config.py` (lint fix), `dashboard/tests/test_judge_editor.py` (new), `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - The judge editor was already fully implemented in a previous iteration but PRD was not marked as passes: true
  - `judge_editor.py` stores config as a mutable dict in `st.session_state` for widget binding, then converts to frozen JudgeConfig for persistence
  - `_config_to_session_state()` uses `config_to_dict()` to create mutable dict; `_session_state_to_config()` uses `dict_to_config()` to restore
  - When mocking Streamlit for column-based layouts, use `mock_st.columns.return_value = (MagicMock(), MagicMock(), MagicMock())` for 3-column layouts
  - `_render_dimension_row()` returns True/False for delete signal; caller processes deletions in reverse order
  - Dimension criteria are padded to 5 levels in the editor (even if fewer in config) for consistent UI
  - `st.expander` with `expanded=False` provides a collapsed-by-default section for per-level criteria
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-018
- Implemented: LLM judge test prompt on single task
- Files changed:
  - `dashboard/utils/judge_test_prompt.py` (NEW) — Test prompt utility with task data loading, prompt building, LLM calling, response parsing, experiment discovery, and Streamlit UI rendering
  - `dashboard/views/analysis_llm_judge.py` — Added import and call to `render_test_prompt_section()` in the "Prompt & Rubric Editor" tab
  - `dashboard/tests/test_judge_test_prompt.py` (NEW) — 67 unit tests covering all components: data classes, code change extraction, trace parsing, task data loading, prompt building, response parsing, experiment discovery, test prompt execution, display rendering, and integration
  - `scripts/ralph/prd.json` — Marked US-018 as passes: true
- **Learnings for future iterations:**
  - `_session_state_to_config()` is defined in `judge_editor.py` and imported locally in `judge_test_prompt.py` — when testing with mocks, patch it at the source module (`dashboard.utils.judge_editor._session_state_to_config`)
  - Patching `os.environ` should use `patch.dict(os.environ, ...)` rather than patching the `os` module itself, since the module-level import means the real `os` is already bound
  - The `TestPromptResult` dataclass name triggers a pytest collection warning because it starts with "Test" — not a blocker but worth noting
  - Task data extraction from traces requires careful JSON parsing of JSONL format; each line is independent and malformed lines should be skipped
  - The `render_test_prompt_section` function reads `CCB_EXTERNAL_RUNS_DIR` env var at call time (not import time), so env var mocking works correctly
---

## 2026-01-27 - US-019
- Implemented LLM judge template save and load functionality
- **judge_config.py** additions:
  - `TemplateInfo` frozen dataclass for template metadata (name, filename, model, created_at)
  - `save_template()`: saves JudgeConfig with `_template_name` and `_created_at` metadata fields
  - `_sanitize_template_name()`: converts human-readable name to valid filename
  - `load_template_info()`: loads metadata from a template file
  - `list_template_infos()`: lists all templates with metadata (using existing `list_templates()`)
  - `duplicate_template()`: copies template with '-copy' suffix and new timestamp
- **judge_template_manager.py** (NEW): Streamlit UI component with:
  - `render_save_template_section()`: name input + save button to save current editor config as named template
  - `render_template_list()`: lists all saved templates with name, creation date, model display
  - `_render_template_row()`: per-template row with Load, Duplicate, Delete buttons
  - `_render_delete_confirmation()`: two-step delete with Confirm/Cancel buttons
  - `render_template_manager()`: main entry point combining save + list sections
- **analysis_llm_judge.py**: added `render_template_manager` import and call in "Prompt & Rubric Editor" tab
- **test_judge_template_manager.py** (NEW): 61 unit tests across 18 test classes
- Files changed: `dashboard/utils/judge_config.py`, `dashboard/utils/judge_template_manager.py` (NEW), `dashboard/views/analysis_llm_judge.py`, `dashboard/tests/test_judge_template_manager.py` (NEW), `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Template metadata stored as underscore-prefixed fields (`_template_name`, `_created_at`) alongside config data in the same JSON file
  - `_sanitize_template_name()` strips trailing underscores from sanitized names (e.g., "My Template!" -> "my_template.json", not "my_template_.json")
  - `list_templates()` returns filenames sorted alphabetically; `list_template_infos()` wraps it with metadata loading
  - `duplicate_template()` reads source JSON, updates metadata fields, writes new file — preserves all config fields unchanged
  - `_config_to_session_state` and `_session_state_to_config` from judge_editor.py are the bridge between frozen JudgeConfig and mutable session state
  - `delete_template()` was already implemented in judge_config.py — only needed confirmation UI
  - When testing `render_template_list`, mock `_render_template_row` to avoid Streamlit column unpacking errors in unit tests
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-020
- Implemented LLM judge A/B comparison mode
- Created `dashboard/utils/judge_ab_comparison.py` with:
  - `TaskComparisonResult` and `ComparisonSummary` frozen dataclasses for immutable result tracking
  - `_parse_score()` for safe string-to-float score conversion
  - `_compute_mean_overall_score()` for averaging across TestPromptResult instances
  - `_compute_pearson_correlation()` for template score correlation (manual, no numpy)
  - `_compute_agreement_rate()` for fraction of tasks where templates agree within threshold
  - `compute_comparison_summary()` orchestrates all statistics
  - `run_ab_comparison()` runs both templates on all tasks with progress callback
  - `_build_results_table()` builds Pandas DataFrame with per-dimension scores, overall, and delta columns
  - `_render_summary_stats()` renders 4 metric cards: mean A, mean B, correlation, agreement rate
  - `_render_results_table()` renders dataframe with CSV export button
  - `render_ab_comparison_tab()` main entry point: template selectors, experiment/task selectors, run button, results display
- Added "A/B Comparison" as 5th tab in `dashboard/views/analysis_llm_judge.py`
- Requires >= 2 saved templates to activate (shows info message otherwise)
- Warns if same template selected for both A and B
- Results persisted in `st.session_state[judge_ab_results]` across reruns
- Created `dashboard/tests/test_judge_ab_comparison.py` with 55 unit tests across 15 test classes
- Files changed: `dashboard/utils/judge_ab_comparison.py` (NEW), `dashboard/views/analysis_llm_judge.py`, `dashboard/tests/test_judge_ab_comparison.py` (NEW), `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - `run_test_prompt()` from `judge_test_prompt.py` is reusable for evaluating any config on any task — used for both A and B in the comparison
  - `_find_experiment_tasks()` from `judge_test_prompt.py` is reusable for experiment task discovery
  - Manual Pearson correlation avoids numpy dependency; formula: sum((a-mean_a)(b-mean_b)) / sqrt(sum_sq_a * sum_sq_b)
  - Zero-variance scores (all identical) yield 0 denominator in correlation — return 0.0 instead of dividing
  - `pd.DataFrame.to_csv(index=False)` + `st.download_button(mime="text/csv")` provides clean CSV export
  - `st.progress()` + callback pattern allows progress tracking during long-running comparison operations
  - When building result table columns, use "A: DimName" and "B: DimName" prefixes for clear disambiguation
  - Pre-existing ruff warnings in analysis_llm_judge.py (bare except, f-string without placeholders) — not related to this feature
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-021
- Implemented LLM judge human alignment score entry
- Created `dashboard/utils/judge_human_alignment.py` with:
  - SQLite database module: `init_database()`, `save_human_score()`, `load_human_scores()`, `load_scores_for_tasks()`, `count_reviewed_tasks()`
  - `HumanScore` frozen dataclass for score entries (task_id, dimension, human_score, llm_score, annotator, timestamp)
  - `_build_alignment_table_data()` builds display rows with LLM + human scores per dimension
  - `_render_progress_indicator()` shows progress bar and "X of Y tasks reviewed" caption
  - `_render_score_entry_table()` renders per-task expanders with LLM scores and editable human score inputs (1-5)
  - `render_human_alignment_tab()` main entry point: template selector, experiment/task selector, LLM eval runner, progress indicator, score entry table
- Database: `data/human_alignment_scores.db` with table `human_alignment_scores` (task_id, dimension, human_score, llm_score, annotator, timestamp)
  - UNIQUE constraint on (task_id, dimension, annotator) for upsert behavior
  - CHECK constraint on human_score BETWEEN 1 AND 5
- Added "Human Alignment" as 6th tab in `dashboard/views/analysis_llm_judge.py`
- Created `dashboard/tests/test_judge_human_alignment.py` with 47 unit tests across 12 test classes
- Files changed: `dashboard/utils/judge_human_alignment.py` (NEW), `dashboard/views/analysis_llm_judge.py`, `dashboard/tests/test_judge_human_alignment.py` (NEW), `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - SQLite `INSERT OR REPLACE` with UNIQUE constraint provides clean upsert semantics for score updates
  - `sqlite3.connect()` + `try/finally conn.close()` pattern (not context manager) gives explicit control for read/write separation
  - `st.number_input()` with min_value=0, max_value=5 provides bounded integer input; 0 means "not scored" (not saved)
  - Human scores need to support multiple annotators via the UNIQUE(task_id, dimension, annotator) constraint
  - `load_scores_for_tasks()` returns nested dict: task_id -> {dimension: score} for efficient lookup in UI rendering
  - LLM evaluation is cached in session state keyed by experiment name to avoid re-running on every page interaction
  - Immutable dict building uses `{**result, key: value}` pattern consistent with codebase conventions
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-022
- Implemented LLM judge alignment agreement metrics
- Created `dashboard/utils/judge_alignment_metrics.py` with:
  - `ScorePair` and `AlignmentMetrics` frozen dataclasses for immutable metric tracking
  - `build_score_pairs()` pairs human and LLM scores from dict inputs, skipping missing/unparseable values
  - `compute_pearson_correlation()` manual computation (no numpy) between human and LLM scores
  - `compute_mean_absolute_error()` average |human - LLM| across all score pairs
  - `compute_cohens_kappa()` standard formula (p_o - p_e) / (1 - p_e) with LLM scores rounded to nearest int for categorical comparison
  - `compute_alignment_metrics()` orchestrates all three statistics into AlignmentMetrics
  - `find_disagreements()` filters pairs where |human - LLM| exceeds configurable threshold (default 2.0)
  - `build_export_dataframe()` creates Pandas DataFrame for CSV export with Task ID, Dimension, Human Score, LLM Score, Absolute Difference
  - `render_alignment_metrics_panel()` renders 4 metric cards (kappa, correlation, MAE, pair count), disagreement slider, highlighted table, and CSV export button
- Added `_extract_llm_scores_dict()` to `judge_human_alignment.py` to convert TestPromptResult dict to {task_id: {dim: score_str}} format
- Integrated metrics panel into `render_human_alignment_tab()` after the score entry table
- Disagreement highlighting: rows where |human - LLM| > threshold are styled red (#fee2e2) using `df.style.apply()`
- Created `dashboard/tests/test_judge_alignment_metrics.py` with 67 unit tests across 14 test classes
- Files changed: `dashboard/utils/judge_alignment_metrics.py` (NEW), `dashboard/utils/judge_human_alignment.py`, `dashboard/tests/test_judge_alignment_metrics.py` (NEW), `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Cohen's kappa needs at least 2 categories to be meaningful; when all scores are the same category, return 1.0 (trivial perfect agreement)
  - `p_e >= 1.0` guard prevents division by zero in kappa when expected agreement is perfect
  - `round()` on LLM float scores converts to nearest int for ordinal category matching in kappa
  - `find_disagreements()` uses strict `>` (not `>=`) for threshold comparison
  - `pd.DataFrame.style.apply()` with `axis=1` applies row-wise styling; return list of CSS strings per column
  - ruff rejects variable name `l` as ambiguous (E741); use `lv` for loop variables iterating LLM values
  - Pearson correlation reused from A/B comparison module pattern (manual sqrt computation, no numpy)
  - `_extract_llm_scores_dict()` bridges TestPromptResult to dict format needed by compute_alignment_metrics
  - Browser verification not available in this environment; manual verification needed
---

## 2026-01-27 - US-023
- Implemented Analysis Hub card-based navigation page with 2x3 grid of analysis cards
- Created `dashboard/utils/analysis_cards.py` with:
  - `AnalysisCard` frozen dataclass for card configuration (card_id, title, icon, description, page_name, status_key)
  - `ANALYSIS_CARDS` tuple with 6 cards: Statistical, Comparison, Time Series, Cost, Failure, LLM Judge
  - `check_results_available()` checks session state first, falls back to filesystem/database checks
  - `_check_db_has_experiments()` checks if analysis_loader has experiments loaded
  - `_check_judge_results_exist()` checks for JSON files in data/judge_results/
  - `_render_status_badge()` returns green "Results Available" or gray "No Results" HTML badge
  - `render_analysis_card()` renders styled HTML card with icon, title, description, status badge, and "Configure & Run" button
  - `render_analysis_card_grid()` renders 2x3 grid using st.columns(3) for each row
- Updated `dashboard/views/analysis_hub.py`:
  - Replaced text-based component checklist (section 3) with `render_analysis_card_grid()` call
  - Added import for `render_analysis_card_grid` from `dashboard.utils.analysis_cards`
  - Removed unused `datetime` import
- Clicking a card sets `st.session_state["current_page"]` to the card's page_name and calls `st.rerun()` for navigation
- Status badges: green border + "Results Available" when DB has experiments or judge results exist; gray border + "No Results" otherwise
- Created `dashboard/tests/test_analysis_cards.py` with 38 unit tests across 10 test classes
- Files changed: `dashboard/utils/analysis_cards.py` (NEW), `dashboard/views/analysis_hub.py`, `dashboard/tests/test_analysis_cards.py` (NEW), `scripts/ralph/prd.json`, `scripts/ralph/progress.txt`
- **Learnings for future iterations:**
  - Navigation in app.py uses `st.session_state["current_page"]` + `st.rerun()` to switch views
  - Card page_name values must exactly match the strings in `nav_items_analysis` in app.py
  - `st.session_state` is a dict — can't override `.get()` on it in tests; use regular dict assignment instead
  - Status detection for DB-backed analyses checks `analysis_loader.list_experiments()` for experiment count > 0
  - Status detection for LLM Judge checks `data/judge_results/*.json` files on disk
  - `st.markdown(html, unsafe_allow_html=True)` is the standard way to render styled card HTML in Streamlit
  - `st.button(use_container_width=True)` creates a full-width button that fits well under card HTML
  - f-strings without placeholders trigger ruff F541; use plain strings instead
  - Analysis cards component: `dashboard/utils/analysis_cards.py` with `ANALYSIS_CARDS` tuple and `render_analysis_card_grid()`
  - Browser verification not available in this environment; manual verification needed
---
