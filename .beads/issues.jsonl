{"id":"CodeContextBench-09h","title":"Fix Harbor framework installation for reproducible benchmarking","description":"Harbor CLI 0.3.0 (pip package) is broken: typer incompatibility, unmaintained. Blocks reproducibility of benchmark results. Solution: Install official Harbor framework (harborai) from harborframework.com instead. This will enable: (1) real task execution with reproducible metrics, (2) shareable setup for others to replicate, (3) publication-ready results. Current direct_benchmark.py uses synthetic mocks. Plan: Install harborai package, validate agent integration, re-run 10-task pilot with real Harbor, document setup for reproducibility. See .beads/ccb-harbor-fix.md for implementation plan (4 hours, 1 session).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T20:42:34.874834-05:00","updated_at":"2025-12-19T12:27:59.111446-05:00","closed_at":"2025-12-19T12:27:59.111446-05:00"}
{"id":"CodeContextBench-0f3","title":"Complete DEVELOPMENT.md \u0026 documentation suite","description":"Update documentation suite to reflect mining strategy, benchmark execution, and enterprise codebase insights.\n\nSTATUS (as of 2025-12-20):\n✓ docs/ENTERPRISE_CODEBASES.md: Created (209 lines) - comprehensive enterprise codebase characteristics\n✓ docs/DEVELOPMENT.md: Exists (110 lines) - basic dev commands and setup\n✓ docs/TROUBLESHOOTING.md: Exists (113 lines) - common issues\n✓ docs/BENCHMARK_DESIGN_GUIDE.md: Updated with enterprise principles (CodeContextBench-1wi)\n\nREMAINING WORK:\n- README.md: Needs quick-start for github_mined tasks\n- DEVELOPMENT.md: Needs mining pipeline details, agent setup expansion\n- Reference MINING_PLAN.md as canonical source throughout","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:51.499071-05:00","updated_at":"2025-12-21T15:06:21.224969-05:00","closed_at":"2025-12-21T15:06:21.224969-05:00"}
{"id":"CodeContextBench-0ji","title":"Replace DI-Bench test.sh with Python-based dependency validation (no Docker-in-Docker)","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T10:21:47.710133-05:00","updated_at":"2025-12-20T10:25:20.192592-05:00","closed_at":"2025-12-20T10:25:20.192592-05:00"}
{"id":"CodeContextBench-0ro","title":"Generate DAComp tasks under benchmarks and add repo_path","description":"Regenerate DAComp tasks into benchmarks/dacomp/tasks and add repo_path file to task structure for consistency with benchmark template.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T16:01:01.298641-05:00","updated_at":"2025-12-22T16:01:49.524398-05:00","closed_at":"2025-12-22T16:01:49.524398-05:00"}
{"id":"CodeContextBench-0sz","title":"Update DEVELOPMENT.md: fix outdated agent patterns and test references","description":"DEVELOPMENT.md contains outdated patterns:\n\nOUTDATED REFERENCES:\n1. Line 30: References BasePatchAgent - agents now extend ClaudeCode from Harbor\n2. Line 31-35: References Jinja2 templates pattern (install-\u003cagent\u003e.sh.j2) not used\n3. Line 14: References tests/test_claude_agents.py (doesn't exist)\n4. Line 25: References tests/test_agent_comparison.py (doesn't exist)\n\nFIXES NEEDED:\n1. Update agent implementation section to reflect current pattern (extend ClaudeCode)\n2. Remove or update Jinja2 template references\n3. Fix test file references to actual test files (test_agent_env_injection.py, test_mcp_agent_setup.py)","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-21T07:32:56.54921-05:00","updated_at":"2025-12-21T07:46:28.366423-05:00","closed_at":"2025-12-21T07:46:28.366423-05:00"}
{"id":"CodeContextBench-0za","title":"Build post-run evaluation pipeline with LLM judging","description":"Wrap manifest writing, metrics extraction, tool/token summaries, environment failure tagging, and optional multi-LLM judge voting into a single postprocess script that produces evaluation_report.json and REPORT.md per experiment.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-24T13:29:17.134661-05:00","updated_at":"2025-12-25T12:49:33.246144-05:00","closed_at":"2025-12-25T12:49:33.246144-05:00","dependencies":[{"issue_id":"CodeContextBench-0za","depends_on_id":"CodeContextBench-onl","type":"blocks","created_at":"2025-12-25T12:38:19.56048-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-0za","depends_on_id":"CodeContextBench-8qb","type":"blocks","created_at":"2025-12-25T12:38:24.717903-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-0za","depends_on_id":"CodeContextBench-m13","type":"blocks","created_at":"2025-12-25T12:38:29.873782-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-0za","depends_on_id":"CodeContextBench-ubt","type":"blocks","created_at":"2025-12-25T12:38:35.043266-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-0za","depends_on_id":"CodeContextBench-u8x","type":"blocks","created_at":"2025-12-25T12:38:40.187923-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-0zy","title":"Reference bundle management (excerpt pack editor)","description":"For grounded LLM judge evaluations.\n\nFORMAT (JSON):\n{\n  'ref_bundle_id': 'k8s-ssa-refpack-v1',\n  'sources': [{ref, url, hash}],\n  'excerpts': [{id, ref, title, text}]\n}\n\nRULES:\n- 10-30 excerpts per bundle, each 1-6 paragraphs\n- Each excerpt has stable ID (SSA-001, SSA-002, etc.)\n- Judges must cite excerpt IDs only (no free-form claims)\n\nDASHBOARD UI:\n1. List existing ref bundles\n2. Create new bundle (name, sources)\n3. Add/edit/delete excerpts\n4. Compute source hashes for versioning\n5. Link bundles to checklists\n\nStore in configs/ref_bundles/ and database ref_bundles table.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-27T11:51:34.374927-05:00","updated_at":"2025-12-27T11:51:34.374927-05:00","dependencies":[{"issue_id":"CodeContextBench-0zy","depends_on_id":"CodeContextBench-m6c","type":"blocks","created_at":"2025-12-27T11:56:48.153024-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-11e","title":"Implement checklist evaluation system for doc tasks","description":"Core differentiator for Kubernetes docs benchmark.\n\nCOMPONENTS:\n1. src/evaluation/checklist.py\n   - ChecklistItem dataclass (id, category, statement, refs, severity)\n   - ChecklistEvaluator class\n   - Scoring: coverage = covered/total, accuracy = correct/covered\n   - weighted_score = 0.6*accuracy + 0.4*coverage - contradiction_penalty\n\n2. configs/checklists/ssa-doc-v1.yaml\n   - 55 items from design spec (SSA.C1-C7, SSA.O1-O11, SSA.M1-M14, etc.)\n   - Categories: intro, concepts, ownership, merge, conflicts, api, migration, controllers\n\n3. Tier A (fast heuristics): regex/keyword detection for 'covered'\n4. Tier B (grounded judge): LLM labels correct/incorrect with excerpt citations\n\n5. Dashboard: Add checklist panel to Run Detail view","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-27T11:50:52.780304-05:00","updated_at":"2025-12-27T13:50:22.296234-05:00","closed_at":"2025-12-27T13:50:22.296234-05:00","dependencies":[{"issue_id":"CodeContextBench-11e","depends_on_id":"CodeContextBench-m6c","type":"blocks","created_at":"2025-12-27T11:56:43.014052-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-11h","title":"Reorganize dashboard navigation into tab structure","description":"Improve dashboard navigation and page structure per design spec workflow.\n\nAPPROACH: Keep sidebar navigation (better UX), add tabs within pages for sub-features.\n\nCOMPLETED in CodeContextBench-53m:\n- Exposed all hidden views\n- Organized into sections (Benchmarks, Analysis, Advanced)\n\nREMAINING WORK:\n1. Add tabs within Run Results for: Overview | Trace | Judge | Report\n2. Add tabs within Comparison for: Table | Charts | pass@k | Cost\n3. Consolidate redundant pages (comparison.py vs comparison_enhanced.py)\n4. Add placeholder pages for: Eval Config, Admin\n\nThis is lower priority than P1 features (database tables, checklist eval).\nConsider deferring to P2 after core features are implemented.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-27T11:50:42.193428-05:00","updated_at":"2025-12-27T12:00:36.837822-05:00","dependencies":[{"issue_id":"CodeContextBench-11h","depends_on_id":"CodeContextBench-53m","type":"blocks","created_at":"2025-12-27T11:56:37.87152-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-13j","title":"Design new benchmark: Process quality metrics (tool usage patterns)","description":"Create benchmark evaluating HOW agents work, capturing process quality beyond binary pass/fail.\n\nMETRICS TO CAPTURE:\n- MCP: Deep search patterns, retrieval efficiency, token usage\n- Baseline: Context utilization, exploration patterns\n- Tool effectiveness: Search query refinement, navigation paths\n\nDEVELOPER TIME ALLOCATION (Real-world data):\n- 58% on code reading/comprehension\n- 35% on navigation/search\n- 19% on external documentation\n- 23min to regain focus after interrupt\n\nEVALUATE:\n- Code search frequency and success rate\n- Time to locate relevant code\n- Number of files read before understanding\n- Search query evolution (refinement patterns)\n- Context switch recovery efficiency\n\nReference: Google's internal code search metrics (thousands of daily queries), Stripe productivity surveys, context switching research.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T13:49:28.836717-05:00","updated_at":"2025-12-24T13:25:46.222023-05:00","closed_at":"2025-12-24T13:25:46.222023-05:00"}
{"id":"CodeContextBench-1md","title":"Optimize MCP agent: Increase Deep Search usage and reduce token overhead","description":"MCP agent underutilizes Deep Search (only 11 calls in 1248s). Despite having MCP available, agent still uses 36% fewer tool calls but INCREASES token usage (1.16x). Analysis: (1) Review system prompt - encourage Deep Search usage; (2) Optimize context window sizes; (3) Reduce over-reasoning on complex tasks (sgt-009 was 3x slower with MCP). Goal: Achieve speedup without token overhead.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-19T17:34:30.159726-05:00","updated_at":"2025-12-21T09:20:34.020096-05:00","closed_at":"2025-12-21T09:20:34.020096-05:00","dependencies":[{"issue_id":"CodeContextBench-1md","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:30.160185-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-1rp","title":"Build experiment results browser","description":"Create page to browse experiment directories, view evaluation_report.json data, and display REPORT.md content.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:50.549834-05:00","updated_at":"2025-12-25T12:44:56.115158-05:00","closed_at":"2025-12-25T12:44:56.115158-05:00"}
{"id":"CodeContextBench-1tn","title":"Create RepoQA adapter in ~/harbor/adapters/repoqa/","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T10:47:15.807183-05:00","updated_at":"2025-12-20T10:53:41.015287-05:00","closed_at":"2025-12-20T10:53:41.015287-05:00","dependencies":[{"issue_id":"CodeContextBench-1tn","depends_on_id":"CodeContextBench-9sn","type":"discovered-from","created_at":"2025-12-20T10:47:15.807916-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-1tq","title":"Build standalone Harbor evaluations dashboard","description":"Create simple Streamlit app for running and monitoring Harbor evaluations with 3 MCP configs (Baseline, Sourcegraph, Deep Search). Support telemetry capture, results viewing, cross-eval comparison.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-29T18:42:10.900844-05:00","updated_at":"2025-12-29T18:42:27.974775-05:00","closed_at":"2025-12-29T18:42:27.974775-05:00"}
{"id":"CodeContextBench-1v2","title":"Experiment: Wide-impact commit simulation (100+ files)","description":"Test agent performance on wide-impact changes like Uber's 100+ service commits:\n\nEXPERIMENT DESIGN:\n- Create tasks requiring changes to 10, 50, 100+ files\n- Examples: Library version upgrade, API signature change, config migration\n- Test completeness and consistency\n\nREFERENCE (ENTERPRISE_CODEBASES.md):\n- Uber: 1.4% of commits touch 100+ services\n- 0.3% touch 1000+ services\n- Common in monorepo patterns\n\nTASKS TO CREATE:\n1. Upgrade shared library across 50 consumers\n2. Rename function called in 100+ places\n3. Config format migration (100+ config files)\n4. Deprecate old API (update 75+ call sites)\n\nMETRICS:\n- Completeness: % of required changes found\n- Correctness: Are changes compatible?\n- Consistency: Are patterns uniform?\n- Efficiency: How many search iterations?\n\nEVALUATION:\n- Search for old patterns: should return zero\n- All tests pass after changes\n- No regressions introduced\n\nHYPOTHESIS:\n- MCP: Systematic search finds all occurrences\n- Baseline: Misses scattered references\n\nDELIVERABLE:\n- Wide-impact task dataset\n- Completeness analysis\n- Best practices for systematic changes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T18:16:28.971447-05:00","updated_at":"2025-12-24T13:26:42.44591-05:00","closed_at":"2025-12-24T13:26:42.44591-05:00","dependencies":[{"issue_id":"CodeContextBench-1v2","depends_on_id":"CodeContextBench-zyq","type":"blocks","created_at":"2025-12-20T18:16:40.769746-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-1wi","title":"Update benchmark design docs with enterprise codebase insights","description":"Update docs/BENCHMARK_DESIGN_GUIDE.md and related docs to reference ENTERPRISE_CODEBASES.md. Integrate insights: 58% time on comprehension, 35% on navigation, 23min context switch cost, monorepo vs multi-repo patterns, scale considerations (Google 2B LOC → small projects). Add recommendations section citing real-world metrics.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T17:55:46.096266-05:00","updated_at":"2025-12-20T22:05:57.091243-05:00","closed_at":"2025-12-20T22:05:57.091243-05:00"}
{"id":"CodeContextBench-1x7","title":"Single-task direct comparison: claude-code vs claude-code-mcp with real test validation and streaming output","description":"Run ONE task with both agents. Requirements:\n1. Use Claude streaming JSON output (not --output-format json flag, actual streaming API)\n2. System prompt: explicitly state non-interactive, MUST complete task, no placeholders\n3. Capture full multi-turn conversation (all 44 turns if applicable)\n4. Real test validation (task has actual test command, not empty make test)\n5. Show actual code changes (git diff must have content)\n6. Capture Deep Search queries and responses (for MCP agent)\n7. Output: complete trace JSON with reasoning, queries, results\n\nCompare:\n- Baseline: claude-code (no MCP)\n- MCP: claude-code-sourcegraph-mcp (with Deep Search)\n\nGoal: Clear evidence of whether MCP actually helps or if Phase 3 results were measurement error.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T08:50:01.112183-05:00","updated_at":"2025-12-18T19:03:46.434571-05:00","closed_at":"2025-12-18T19:03:46.434571-05:00"}
{"id":"CodeContextBench-1y6","title":"Evaluation report storage and display","description":"Generate and store comprehensive evaluation reports for each task.\n\nImplementation:\n- Generate report for each task with metrics, judge assessment, code quality metrics, tool usage\n- Database table: evaluation_reports (run_id, task_name, report_data JSON, timestamp)\n- Store as JSON alongside result.json or in database\n- Display in Run Results view\n- Export capability (JSON/CSV/Markdown)\n\nAcceptance criteria:\n- Comprehensive report generated for each task\n- Reports stored persistently\n- Reports viewable in UI\n- Can export reports in multiple formats","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-25T19:13:35.397324-05:00","updated_at":"2025-12-26T09:13:16.05524-05:00","closed_at":"2025-12-26T09:13:16.05524-05:00","dependencies":[{"issue_id":"CodeContextBench-1y6","depends_on_id":"CodeContextBench-69v","type":"blocks","created_at":"2025-12-25T19:13:35.398634-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-1y6","depends_on_id":"CodeContextBench-8el","type":"blocks","created_at":"2025-12-25T19:14:16.432122-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-2bq","title":"Experiment: Kubernetes Documentation Generation Benchmark","description":"Evaluate agent code understanding by generating documentation for undocumented code.\n\nGOAL:\nTest agents' ability to understand complex distributed systems by asking them to write documentation for Kubernetes packages where original docs have been stripped.\n\nMETHODOLOGY:\n1. **Setup**: Create a 'stripped' version of Kubernetes (remove doc.go, READMEs, KEPs).\n2. **Task**: Agent must write the missing documentation (e.g., \"Write doc.go for pkg/kubelet/cm\").\n3. **Evaluation**: Use an LLM Judge to compare the Agent-Generated Docs against the Original (Ground Truth) Docs.\n\nWHY THIS DESIGN?\n- **Prevents Cheating**: Agent cannot just read the docs; it must synthesize understanding from code.\n- **Measures Context**: Hypothesis is that MCP agents will find related context (KEPs, other packages) via Deep Search, while Baseline agents will lack this broader context.\n- **Quantifiable**: LLM Judge provides structured scoring (Accuracy, Completeness, Clarity) vs Ground Truth.\n\nDELIVERABLES:\n- Kubernetes documentation task suite (benchmarks/kubernetes_docs/)\n- Evaluation pipeline with LLM Judge\n- Comparison report: Baseline vs MCP quality","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T18:16:29.229505-05:00","updated_at":"2025-12-24T13:26:32.221799-05:00","closed_at":"2025-12-24T13:26:32.221799-05:00"}
{"id":"CodeContextBench-2bt","title":"Reduce AGENTS.md from 980 to ~500 lines - extract adapter docs","description":"AGENTS.md is 980 lines but its own guidance (line 5) says keep to ~500 lines.\n\nCONTENT TO EXTRACT:\n- DI-Bench adapter details (lines 181-213) -\u003e docs/ADAPTERS.md\n- RepoQA execution guides (lines 355-437) -\u003e docs/REPOQA_GUIDE.md\n- Big Code task templates (lines 439-532) -\u003e docs/BIG_CODE_GUIDE.md\n- Deep Search CLI docs (lines 600-649) -\u003e docs/DEEP_SEARCH.md or remove (external tool)\n- Landing the plane procedures (lines 820-975) -\u003e can stay but condense\n\nTARGET: ~500 lines of high-signal quick reference content.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T07:32:56.366324-05:00","updated_at":"2025-12-21T07:45:36.984603-05:00","closed_at":"2025-12-21T07:45:36.984603-05:00"}
{"id":"CodeContextBench-2cv","title":"Industry validation: Partner with company for benchmark validation","description":"Establish partnership with tech company for benchmark validation (without exposing proprietary code).\n\nPOTENTIAL PARTNERS:\n- Companies with open engineering blogs (Stripe, Uber, Netflix)\n- Companies investing in AI tooling (Stripe: 65-70% using AI assistants)\n- Academic partnership programs (Meta, Google, Microsoft)\n\nVALIDATION APPROACH:\n1. Company runs benchmark internally on proprietary codebase\n2. Share aggregated metrics only (no code exposure)\n3. Validate against their internal productivity metrics\n4. Compare tool performance to their internal tools\n\nREFERENCE METRICS:\n- Build time improvements\n- Developer productivity survey results\n- Onboarding time reduction\n- Code search success rates\n\nOUTCOME:\n- Industry-validated benchmark\n- Real-world performance baselines\n- Credibility for research publication\n- Feedback loop for benchmark improvement","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-20T17:56:48.985828-05:00","updated_at":"2025-12-24T13:27:54.129958-05:00","closed_at":"2025-12-24T13:27:54.129958-05:00","dependencies":[{"issue_id":"CodeContextBench-2cv","depends_on_id":"CodeContextBench-jdm","type":"blocks","created_at":"2025-12-20T17:58:21.046861-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-2cv","depends_on_id":"CodeContextBench-qp1","type":"blocks","created_at":"2025-12-20T17:58:21.577651-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-2cv","depends_on_id":"CodeContextBench-xiz","type":"blocks","created_at":"2025-12-20T17:58:21.777855-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-2il","title":"Re-run 50-task benchmark with corrected comparison script","description":"Use the fixed run_10task_comparison.sh with timestamped directories and validation script. Expand to 50-task benchmark to get statistically significant results. Use validate_comparison_results.py to verify data integrity before analysis. Real 10-task data showed only 1.05x speedup with 1.16x higher token cost - need larger sample to confirm pattern.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T17:34:24.856229-05:00","updated_at":"2025-12-21T15:10:13.811303-05:00","closed_at":"2025-12-21T15:10:13.811303-05:00","dependencies":[{"issue_id":"CodeContextBench-2il","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:24.856716-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-2pw","title":"Comparative analysis: CodeContextBench vs AgentCompany/EnterpriseBench","description":"Document how CodeContextBench compares to similar benchmarks:\n\nBENCHMARKS TO COMPARE:\n1. AgentCompany: Simulates small software company with interrelated tasks\n2. EnterpriseBench: Synthesizes enterprise data (repos, tickets, wikis, org charts)\n3. SWE-Bench: GitHub issue resolution\n4. DevBench: Software engineering task completion\n\nANALYSIS DIMENSIONS:\n- Task diversity and realism\n- Scale (codebase size, task complexity)\n- Process quality metrics vs outcome metrics\n- Tool usage evaluation depth\n- Enterprise workflow fidelity\n\nOUTCOME:\n- Positioning document: What makes CodeContextBench unique?\n- Gap analysis: What we cover that others don't\n- Integration opportunities: Where we can complement existing benchmarks\n- Research paper material","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T17:56:48.777626-05:00","updated_at":"2025-12-24T13:28:04.36531-05:00","closed_at":"2025-12-24T13:28:04.36531-05:00","dependencies":[{"issue_id":"CodeContextBench-2pw","depends_on_id":"CodeContextBench-1wi","type":"blocks","created_at":"2025-12-20T17:58:21.978005-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-2s1","title":"Integrate existing enterprise-scale benchmarks: DeathStarBench, SWE-Bench","description":"Integrate or adapt existing benchmarks for CodeContextBench:\n\nBENCHMARKS TO INTEGRATE:\n1. DeathStarBench: Realistic microservice applications (e-commerce, media) with service meshes\n2. d'Aragona dataset: 378 open-source microservice projects with labeled architectures\n3. SWE-Bench: GitHub issues and bug reports from large projects\n4. DevBench: Software engineering tasks on real-world code\n\nAPPROACH:\n- Use DeathStarBench for multi-service scenarios\n- Mine d'Aragona projects for microservice dependency patterns\n- Adapt SWE-Bench tasks for tool comparison (baseline vs MCP)\n- Create hybrid scenarios combining multiple benchmarks\n\nOUTCOME:\n- Validated against existing research standards\n- Comparable to academic benchmarks\n- Real architectural complexity from open-source","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-20T17:56:48.568885-05:00","updated_at":"2025-12-24T13:28:14.59285-05:00","closed_at":"2025-12-24T13:28:14.59285-05:00"}
{"id":"CodeContextBench-2wz","title":"Design new benchmark: Diverse task types validation","description":"Create benchmark mixing diverse task types to validate genuine codebase understanding vs task-type expertise.\n\nTASK TYPES (from enterprise workflows):\n1. Bug fixing across multiple services\n2. Feature implementation with cross-cutting changes\n3. Refactoring and tech debt reduction\n4. Performance optimization\n5. Code review and comprehension\n6. Documentation and knowledge sharing\n7. Onboarding and learning new subsystems\n\nREQUIREMENTS:\n- Equal file access (both agents pre-cloned repos)\n- Mix of monorepo and multi-repo scenarios\n- Tasks requiring cross-service coordination\n- Wide-impact changes (affecting 100+ modules, like Uber commits)\n\nEVALUATION CRITERIA:\n- Correctness across task types\n- Exploration efficiency by task type\n- Tool usage patterns (search, navigation, build/test)\n- Time to comprehension\n\nReference: Uber's wide-impact commits (1.4% touch 100+ services), enterprise task diversity research.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T13:49:28.678841-05:00","updated_at":"2025-12-24T13:25:56.462368-05:00","closed_at":"2025-12-24T13:25:56.462368-05:00"}
{"id":"CodeContextBench-34b","title":"Run full 10-task MCP pilot and compare with baseline (80% success)","description":"DATA FIXED: All 50 main set tasks now properly configured with real commits (pre_fix_rev and ground_truth_rev from mining results). Test scripts corrected to compare against pre_fix_rev instead of HEAD. Ready for 10-task baseline vs MCP comparison on properly configured mined tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T07:20:56.902137-05:00","updated_at":"2025-12-19T14:39:21.007706-05:00","closed_at":"2025-12-19T14:39:21.007706-05:00"}
{"id":"CodeContextBench-3de","title":"Enhanced agent trace parsing in Run Results view","description":"Parse claude.txt to extract and display agent execution trace with tool calls, responses, and diffs in a readable format.\n\nImplementation:\n- Parse claude.txt to extract user messages, assistant responses, tool calls, tool results\n- Extract code diffs from Edit/Write operations\n- Display with collapsible sections per turn\n- Syntax highlighting for code/diffs\n- Icons for different tool types\n- Filter options: show only tool calls, show only diffs, search within trace\n\nAcceptance criteria:\n- Agent trace is easy to read and navigate\n- Tool calls and diffs are clearly highlighted\n- Can filter to see specific types of actions\n- Works with actual Harbor output format","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-25T19:13:05.791188-05:00","updated_at":"2025-12-25T19:17:23.344037-05:00","closed_at":"2025-12-25T19:17:23.344037-05:00"}
{"id":"CodeContextBench-3id","title":"Implement judge@k with pairwise blinded evaluation","description":"Improved LLM judge system per design spec.\n\nCURRENT: Single judge evaluation\nTARGET: judge@k with k=3 or k=5\n\nFEATURES:\n1. Pairwise A/B judge (candidate vs baseline, or candidate vs gold)\n2. k runs with different seeds\n3. Aggregate by majority + trimmed mean\n4. Blinded comparison (judge doesn't know which is which)\n\nGROUNDING REQUIREMENTS:\n- Judge must cite excerpt IDs from ref_bundle_id\n- 'unsupported' label for claims not in excerpts\n- 'contradiction' for opposing claims\n\nPROMPT: Use JSON-only pairwise prompt format.\n\nUpdate src/benchmark/llm_judge.py","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-27T11:52:48.109771-05:00","updated_at":"2025-12-27T11:52:48.109771-05:00","dependencies":[{"issue_id":"CodeContextBench-3id","depends_on_id":"CodeContextBench-0zy","type":"blocks","created_at":"2025-12-27T11:57:08.71853-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-3js","title":"Fix invalid benchmark results: Tests are fake (make test undefined), code_changes empty","description":"Fix and validate benchmark results.\n\n## VALIDATION COMPLETE (Dec 22, 2025)\n\n### github_mined benchmark (10-task PyTorch PRs)\n✅ VALID - Tests are real (pytest/CI-based)\n✅ VALID - code_changes populated (7/10 baseline, 5/9 MCP)\n✅ VALID - Rewards captured (100% baseline, 90% MCP)\n✅ VALID - Deep Search visible in MCP trajectory logs\n\n**Data locations:**\n- jobs/baseline-10task-20251219/ (10 successful runs)\n- jobs/mcp-10task-20251219/ (9 successful runs)  \n- artifacts/comparison_analysis_real_data.md (analysis)\n\n### big_code_mcp benchmark (VS Code, K8s, Servo, TensorRT)\n❌ INVALID - All runs failed with RewardFileNotFoundError\n❌ INVALID - Docker containers not cloning real repos (22-byte package.json instead of real VS Code)\n❌ INVALID - Tests cannot run (npm test on fake repo)\n\n**Root cause:** Harbor Docker build not properly executing git clone in Dockerfile, or using cached images that predate the Dockerfile fixes.\n\n## NEXT STEPS\n\n1. **IMMEDIATE:** Use github_mined data for trg comparison (valid data exists)\n2. **DEFER:** big_code_mcp Docker fixes to new bead (infrastructure issue)\n\n## RECOMMENDATION\n\nClose this bead as PARTIAL SUCCESS. The github_mined benchmark provides valid baseline vs MCP comparison data. Create new bead for big_code_mcp Docker infrastructure fixes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-18T08:49:56.738041-05:00","updated_at":"2025-12-22T15:23:49.501397-05:00","closed_at":"2025-12-22T15:23:49.501397-05:00"}
{"id":"CodeContextBench-3p0","title":"Fix broken documentation references (API.md, PAPER_DRAFT, LITERATURE_REVIEW)","description":"Broken doc references found during Dec 21 2025 audit:\n\nMISSING FILES:\n- docs/API.md (referenced in TROUBLESHOOTING.md:42)\n- docs/PAPER_DRAFT_121625.md (referenced in README.md:96)\n- docs/LITERATURE_REVIEW.md (referenced in README.md:97)\n\nMISSING TEST FILES:\n- tests/test_claude_agents.py (referenced in DEVELOPMENT.md:14)\n- tests/test_agent_comparison.py (referenced in DEVELOPMENT.md:25, TROUBLESHOOTING.md:9)\n\nFIX: Either create the files or remove/update the references.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-21T07:32:56.178358-05:00","updated_at":"2025-12-21T07:41:52.519525-05:00","closed_at":"2025-12-21T07:41:52.519525-05:00"}
{"id":"CodeContextBench-4m5","title":"Run 10-task baseline vs MCP comparison on SWE-Bench (real tasks, proven infrastructure)","description":"Run baseline (Claude Code) and MCP (Claude+Sourcegraph) agents on 10 SWE-Bench tasks. SWE-Bench is fully configured in Harbor at /Users/sjarmak/harbor/adapters/swebench/. This validates MCP improves agent performance on real software engineering benchmarks before committing to custom mining infrastructure. Compare metrics: task completion rate, tokens used, execution time.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T14:16:44.126037-05:00","updated_at":"2025-12-20T18:02:16.739686-05:00","closed_at":"2025-12-20T18:02:16.739686-05:00"}
{"id":"CodeContextBench-4nq","title":"Build context switching simulation framework","description":"Create framework simulating developer interruptions and context switches:\n\nSIMULATION APPROACH:\n- Agent starts task A (partial completion)\n- Forced context switch: present unrelated task B\n- Agent completes task B\n- Agent returns to complete task A\n\nMETRICS (23min recovery reference):\n- Time to resume task A effectively\n- Redundant work (re-reading files)\n- Quality of notes/documentation left\n- Success rate on task A after switch\n\nSCENARIOS:\n1. Simple switch: similar tasks, same codebase\n2. Hard switch: different codebases, different domains\n3. Multi-switch: A → B → C → back to A\n4. Interrupted during critical reasoning\n\nHYPOTHESIS:\n- MCP tools help recover context faster (via search)\n- Baseline relies more on conversation history\n- Documentation quality matters more for baseline\n\nREFERENCE: ENTERPRISE_CODEBASES.md - 23min focus recovery time","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-20T18:15:23.140435-05:00","updated_at":"2025-12-24T13:27:23.396539-05:00","closed_at":"2025-12-24T13:27:23.396539-05:00"}
{"id":"CodeContextBench-4re","title":"Implement simple observability: manifest_writer.py and metrics_collector.py","description":"Replace NeMo with lightweight JSON manifests. Write run_manifest.json with harness, tool_profile, result, retrieval_metrics. Parse Harbor logs for tool usage.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:09.976846-05:00","updated_at":"2025-12-17T16:52:42.199699-05:00","closed_at":"2025-12-17T16:52:42.199699-05:00","dependencies":[{"issue_id":"CodeContextBench-4re","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:09.977265-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-4vb","title":"Write enterprise benchmarking research paper","description":"Prepare research paper on enterprise-informed AI agent benchmarking:\n\nPAPER STRUCTURE:\n1. Introduction\n   - Gap: existing benchmarks miss enterprise realities\n   - Contribution: enterprise-informed metrics and tasks\n2. Background\n   - Enterprise codebase characteristics (ENTERPRISE_CODEBASES.md)\n   - Developer productivity challenges\n   - Prior benchmarks (SWE-Bench, AgentCompany, etc.)\n3. Methodology\n   - Task design principles\n   - Metric selection rationale\n   - Evaluation approach\n4. Benchmarks\n   - Scale testing (10k→1M LOC)\n   - Monorepo vs multi-repo\n   - Process quality metrics\n   - Context switching, onboarding, wide-impact\n5. Experiments\n   - Baseline vs MCP results\n   - Enterprise metric findings\n   - Task type performance matrix\n6. Analysis\n   - Where tools matter most\n   - Scale crossover points\n   - ROI modeling\n7. Discussion\n   - Implications for tool design\n   - Industry validation\n   - Limitations and future work\n8. Conclusion\n   - Enterprise benchmarking best practices\n   - Open datasets and reproducibility\n\nTARGET VENUES:\n- ICSE (International Conference on Software Engineering)\n- FSE (Foundations of Software Engineering)\n- ASE (Automated Software Engineering)\n- EMSE (Empirical Software Engineering and Measurement)\n\nDEPENDENCIES: All experiments complete, results analyzed\n\nDELIVERABLE:\n- Full research paper draft\n- Supplementary materials\n- Dataset release plan","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T18:16:29.732799-05:00","updated_at":"2025-12-24T13:26:06.701385-05:00","closed_at":"2025-12-24T13:26:06.701385-05:00","dependencies":[{"issue_id":"CodeContextBench-4vb","depends_on_id":"CodeContextBench-b4m","type":"blocks","created_at":"2025-12-20T18:16:41.204248-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-4vb","depends_on_id":"CodeContextBench-lxl","type":"blocks","created_at":"2025-12-20T18:16:41.417165-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-4vb","depends_on_id":"CodeContextBench-1v2","type":"blocks","created_at":"2025-12-20T18:16:41.62989-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-4vb","depends_on_id":"CodeContextBench-vqr","type":"blocks","created_at":"2025-12-20T18:16:41.84664-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-53m","title":"Expose hidden dashboard views to navigation","description":"Currently 5 views in navigation but 10+ exist. Add missing pages:\n- Deep Search Analytics (deep_search.py)\n- Experiment Results (results.py)\n- Agent Comparison charts (comparison.py)\n- Add Benchmark (add_benchmark.py)\n- Manifests viewer (manifests.py)\n\nQuick win - code exists, just needs nav entries.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-27T11:50:36.894234-05:00","updated_at":"2025-12-27T11:59:18.040079-05:00","closed_at":"2025-12-27T11:59:18.040079-05:00"}
{"id":"CodeContextBench-55p","title":"Add pass@k curve visualization","description":"Show success rate vs. number of attempts (k).\n\nIMPLEMENTATION:\n1. Multiple seeds per task (k=3 or k=5)\n2. Calculate pass@k: probability of at least one success in k attempts\n3. Chart: X-axis = k (1 to max), Y-axis = success rate\n4. Compare curves across tool stacks\n\nIntegrate into Compare tab.\nUse Plotly line chart with tool stack as color dimension.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-27T11:51:29.089856-05:00","updated_at":"2025-12-27T11:51:29.089856-05:00","dependencies":[{"issue_id":"CodeContextBench-55p","depends_on_id":"CodeContextBench-bze","type":"blocks","created_at":"2025-12-27T11:57:13.851077-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-69v","title":"LLM judge integration in Run Results view","description":"Integrate LLM-as-judge evaluation for individual tasks with database storage.\n\nImplementation:\n- Add judge runner in Run Results view with model selector\n- Judge prompt template for code quality assessment\n- Run button for individual tasks\n- Database table: judge_evaluations (task_id, judge_model, score, reasoning, timestamp)\n- Display judge results with score and reasoning\n- Compare judge assessment vs oracle reward\n\nAcceptance criteria:\n- Can run LLM judge on completed task from UI\n- Judge results stored in database\n- Judge assessment displayed with reasoning\n- Can compare across different judge models","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-25T19:13:18.057159-05:00","updated_at":"2025-12-26T09:08:45.83503-05:00","closed_at":"2025-12-26T09:08:45.83503-05:00","dependencies":[{"issue_id":"CodeContextBench-69v","depends_on_id":"CodeContextBench-8el","type":"blocks","created_at":"2025-12-25T19:14:11.285715-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-6f2","title":"Apply autonomous environment variables to baseline Claude Code agent","description":"DISCOVERY: MCP agent successfully made code changes (2 lines detected) despite failing the test due to environment setup. This proves the autonomous environment variables (FORCE_AUTO_BACKGROUND_TASKS=1, ENABLE_BACKGROUND_TASKS=1) injected in create_run_agent_commands() are working correctly.\n\nOBSERVATION: Baseline agent made 0 changes, while MCP agent made changes. The only difference is:\n- MCP: Uses ClaudeCodeSourcegraphMCPAgent which injects autonomous env vars\n- Baseline: Uses built-in harbor claude-code agent without these env vars\n\nSOLUTION: Create a baseline-compatible agent (or extend built-in) that also injects the autonomous env vars without the MCP overhead. This allows fair comparison:\n- BaselineClaudeCodeAgent: Injects autonomous vars, no MCP\n- ClaudeCodeSourcegraphMCPAgent: Injects autonomous vars + MCP tools\n\nIMPLEMENTATION:\n1. Create agents/claude_baseline_agent.py extending ClaudeCode\n2. Inject FORCE_AUTO_BACKGROUND_TASKS=1 and ENABLE_BACKGROUND_TASKS=1 in create_run_agent_commands()\n3. Run baseline tests using: --agent-import-path agents.claude_baseline_agent:BaselineClaudeCodeAgent\n4. This enables fair Phase 4 comparison: Baseline vs MCP on equal footing\n\nThis is critical for valid benchmarking - both agents must have access to autonomous operation mode.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:07:29.215662-05:00","updated_at":"2025-12-19T10:08:07.827608-05:00","closed_at":"2025-12-19T10:08:07.827608-05:00"}
{"id":"CodeContextBench-6oq","title":"Ingestion Pipeline - Harbor JSON and transcript parsing","description":"Parse Harbor JSON logs and claude-code.txt transcripts from synced VM results. Extract tool usage, timing, costs. Store in SQLite/DuckDB for dashboard queries. Files: src/ingest/harbor_parser.py, transcript_parser.py, metrics_extractor.py, database.py","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-02T09:57:55.563951-05:00","updated_at":"2026-01-03T12:37:05.125561-05:00","closed_at":"2026-01-03T12:37:05.125561-05:00","labels":["epic:CodeContextBench-7t0"]}
{"id":"CodeContextBench-6pl","title":"MCP prompt experiment: strategic vs aggressive Deep Search usage","description":"Current Deep Search prompt is too aggressive - tells agent to use it for EVERY question. \n\nHYPOTHESIS: A strategic prompt that explains WHEN to use Deep Search will be more effective:\n- Use Deep Search at task start to understand architecture/context\n- Use it when hitting an information gap (new subsystem, unclear dependencies)\n- Don't use it for every small question - leverage the context already gathered\n\nEXPERIMENT DESIGN:\n1. Strategic Prompt Agent - Use Deep Search strategically (context-gathering at key moments)\n2. Current Aggressive Agent - Use Deep Search for every question\n3. Baseline (no MCP)\n\nPROMPT PHILOSOPHY:\nDeep Search is for context-gathering, not micro-questions. One good Deep Search call should inform many subsequent decisions.\n\nTEST ON:\n- big_code_mcp/big-code-vsc-001\n- github_mined tasks (if baseline runs exist)\n\nMETRICS:\n- Deep Search calls per task\n- Success rate\n- Time to first edit\n- Context quality (qualitative)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T17:35:32.227004-05:00","updated_at":"2025-12-22T17:57:51.362934-05:00","closed_at":"2025-12-22T17:57:51.362934-05:00"}
{"id":"CodeContextBench-6r8","title":"Add DAComp Harbor adapter","description":"Create Harbor adapter for DAComp DA/DE tasks with templates, CLI runner, tests, and benchmark docs updates.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T15:25:19.389975-05:00","updated_at":"2025-12-22T15:25:26.071367-05:00","closed_at":"2025-12-22T15:25:26.071367-05:00"}
{"id":"CodeContextBench-6rf","title":"Consolidate duplicate analysis scripts (comparison, LLM judge, metrics)","description":"Multiple overlapping scripts with unclear precedence:\n\nCOMPARISON ANALYSIS (3 scripts):\n- scripts/analyze_comparison_results.py\n- scripts/detailed_comparison_analysis.py\n- runners/compare_results.py\n\nLLM JUDGE (4 scripts):\n- scripts/llm_judge_evaluation.py\n- scripts/llm_judge_big_code.py\n- scripts/judge_vsc_rerun.py\n- scripts/evaluate_vsc_001.py\n\nMETRICS (3 scripts):\n- scripts/collect_metrics.py\n- scripts/extract_enterprise_metrics.py\n- scripts/comprehensive_metrics_analysis.py\n\nACTION: Audit each, identify canonical versions, consolidate or document when to use each.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T07:32:56.736129-05:00","updated_at":"2025-12-21T07:50:19.371888-05:00","closed_at":"2025-12-21T07:50:19.371888-05:00"}
{"id":"CodeContextBench-6vn","title":"Port task_schema.py from sg_benchmark","description":"Copy src/task_schema.py with JSON schema validation and TaskSpecification dataclass. This becomes the canonical validator for all tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.133496-05:00","updated_at":"2025-12-17T16:14:40.13573-05:00","closed_at":"2025-12-17T16:14:40.13573-05:00","dependencies":[{"issue_id":"CodeContextBench-6vn","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.13406-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-6wi","title":"Redesign big_code_mcp: Use pre-cloned repos for equal file access","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:49:09.389094-05:00","updated_at":"2025-12-20T21:12:04.243406-05:00","closed_at":"2025-12-20T21:12:04.243406-05:00"}
{"id":"CodeContextBench-704","title":"Analyze baseline vs MCP results, validate hypothesis, generate report","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T07:20:59.005639-05:00","updated_at":"2025-12-19T16:27:04.987168-05:00","closed_at":"2025-12-19T16:27:04.987168-05:00","dependencies":[{"issue_id":"CodeContextBench-704","depends_on_id":"CodeContextBench-34b","type":"discovered-from","created_at":"2025-12-18T07:20:59.008337-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-78b","title":"Complete DependEval and DI-Bench adapter setup for benchmark suite","description":"Phase 3: Set up DependEval and DI-Bench adapters for complete benchmark suite\n\nSTATUS (as of 2025-12-20):\n✓ Adapters exist in ~/harbor/adapters/:\n  - dependeval/ (17 items)\n  - dibench/ (19 items)\n\nREMAINING:\n- Validate DependEval adapter works end-to-end\n- Validate DI-Bench adapter works end-to-end\n- Run comparison: baseline vs MCP on these benchmarks\n- Integrate with enterprise metrics collection\n\nNOTE: Parent bead (9sn) tracks overall adapter setup. This bead focuses on DependEval/DI-Bench specifically.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T13:42:49.582397-05:00","updated_at":"2025-12-21T15:06:21.18447-05:00","closed_at":"2025-12-21T15:06:21.18447-05:00","dependencies":[{"issue_id":"CodeContextBench-78b","depends_on_id":"CodeContextBench-9sn","type":"discovered-from","created_at":"2025-12-20T13:42:49.582819-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-7pu","title":"Phase 1: Core Infrastructure - config, translators, sync","description":"COMPLETED: Implemented abstract config layer (src/config/), agent translators for Claude/OpenCode/OpenHands (src/translate/), sync layer (src/sync/), and migrated IR-SDLC components (src/ir_sdlc/). Created CLI (./ccb) and sample configs in configs/.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-02T09:59:09.45853-05:00","updated_at":"2026-01-02T09:59:24.156974-05:00","closed_at":"2026-01-02T09:59:24.156974-05:00","labels":["epic:CodeContextBench-7t0"]}
{"id":"CodeContextBench-7qs","title":"Update ARCHITECTURE.md and AGENTS.md for unified CodeContextBench","description":"Consolidate docs from sg_benchmark and sourcegraph-benchmarks. Make Claude-first, MCP-first. Keep Amp as optional/legacy.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:23.002458-05:00","updated_at":"2025-12-17T17:55:26.998927-05:00","closed_at":"2025-12-17T17:55:26.998927-05:00"}
{"id":"CodeContextBench-7sp","title":"Add thread safety when calling ncclCommGetAsyncError","description":"","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-19T08:19:33.714087-05:00","updated_at":"2025-12-19T08:22:17.725184-05:00","closed_at":"2025-12-19T08:22:17.725184-05:00"}
{"id":"CodeContextBench-7t0","title":"Observability Platform Refactor","description":"Transform CodeContextBench from local benchmark runner into observability/control plane for VM-based Harbor evaluations. VM runs benchmarks, local interprets results. Includes abstract agent configs, bidirectional sync, ingestion pipeline, analysis layer, and recommendation engine.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-01-02T09:57:34.865157-05:00","updated_at":"2026-01-03T12:24:00.20419-05:00","closed_at":"2026-01-03T12:24:00.20419-05:00"}
{"id":"CodeContextBench-7tn","title":"Validate Harbor + Daytona + Claude Code + Sourcegraph MCP integration","description":"Validate Harbor + Daytona + Claude Code integration. Confirmed baseline works with 6M+ cached tokens. MCP pattern: use --agent claude-code with task Dockerfile calling sourcegraph_mcp_setup.sh.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-18T11:46:16.014003-05:00","updated_at":"2025-12-18T11:49:20.557685-05:00","closed_at":"2025-12-18T11:49:20.557685-05:00"}
{"id":"CodeContextBench-7wq","title":"Fix task Dockerfiles to have real test commands (not empty make test)","description":"Fix task Dockerfiles to have real test commands (not empty make test)\n\nRELATED: This addresses part of CodeContextBench-3js (Phase 3 Results Invalid)\n\nCurrent tasks have 'make test' with no target defined. Need to:\n1. Audit each task: does it have actual test validation?\n2. Update task Dockerfiles to run real tests\n3. Examples: pytest, cargo test, npm test, make check\n4. Verify tests actually fail if code is wrong\n\nKNOWN TASKS NEEDING FIXES:\n- big_code tasks: verify test commands work\n- github_mined tasks: ensure test.sh runs actual tests\n- RepoQA: verifier path issues fixed (CodeContextBench-8dx)\n\nWithout this, we can't know if 100% success is real or measurement error.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T08:50:04.268838-05:00","updated_at":"2025-12-21T15:06:21.14516-05:00","closed_at":"2025-12-21T15:06:21.14516-05:00"}
{"id":"CodeContextBench-7xz","title":"Port BasePatchAgent from sourcegraph-benchmarks to agents/base.py","description":"Copy BasePatchAgent from amp_agent.py, generalize for any CLI agent (remove Amp-specific logic). Keep Harbor integration and repo discovery.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.257834-05:00","updated_at":"2025-12-17T15:52:31.770754-05:00","closed_at":"2025-12-17T15:52:31.770754-05:00","dependencies":[{"issue_id":"CodeContextBench-7xz","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.258373-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-81d","title":"Port Harbor runners and benchmark scripts","description":"Migrate run-full-benchmark.sh → harbor_benchmark.sh, compare-harbor-results.py → compare_results.py. Generalize to accept --agent and --benchmark flags.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.224614-05:00","updated_at":"2025-12-17T16:17:07.325137-05:00","closed_at":"2025-12-17T16:17:07.325137-05:00","dependencies":[{"issue_id":"CodeContextBench-81d","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.225009-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-820","title":"Implement tool_profiles.py for MCP tool configuration","description":"Define none, search_only, code_intel, deep_search profiles. Configure which MCP tools are exposed per profile.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T15:27:23.153115-05:00","updated_at":"2025-12-17T18:36:10.118045-05:00","closed_at":"2025-12-17T18:36:10.118045-05:00","dependencies":[{"issue_id":"CodeContextBench-820","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:23.153528-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-848","title":"Run 4-task 10Figure smoke test: Claude baseline vs Claude+MCP","description":"End-to-end validation: run 4 tasks (cross_file, refactor, api_upgrade, bug_localization) with both agent conditions. Compare results and validate hypothesis.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:22.922545-05:00","updated_at":"2025-12-17T16:32:16.244379-05:00","closed_at":"2025-12-17T16:32:16.244379-05:00","dependencies":[{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-7xz","type":"blocks","created_at":"2025-12-17T15:27:22.922992-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-ews","type":"blocks","created_at":"2025-12-17T15:27:22.92344-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-mk9","type":"blocks","created_at":"2025-12-17T15:27:22.923923-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-81d","type":"blocks","created_at":"2025-12-17T15:27:22.924285-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-uzn","type":"blocks","created_at":"2025-12-17T15:27:22.92461-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-8dx","title":"Phase 2 Complete: Fix RepoQA verifier test.sh path issues and verify end-to-end scoring","description":"Phase 2 Complete: Fixed RepoQA verifier test.sh path issues\n\nCOMPLETED:\n- Fixed test.sh to look for /app/solution.json (shared mount between agent \u0026 verifier)\n- Updated instruction_sr-qa.md to write to /app/ instead of /logs/verifier/\n- Added debug output to test.sh for diagnosing missing files\n- Verified verifier logic works: scores 1.0 for correct answers, 0.0 for wrong\n- Updated AGENTS.md with container architecture documentation\n\nVERIFIED WORKING:\n- Ground truth loading: /tests/ground_truth.json found correctly\n- Verifier test.sh creates reward.json successfully\n- Harbor collects verifier output correctly\n- Verifier scoring logic produces correct scores (tested with manual solutions)\n\nREMAINING BLOCKER:\n- Docker container runs out of disk space during repo clone in Dockerfile\n- This prevents agent execution, but paths are now fixed for when it works\n\nStatus: Ready for next phase once Docker disk space resolved.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:42:46.97736-05:00","updated_at":"2025-12-20T18:34:33.950881-05:00","closed_at":"2025-12-20T18:34:33.950881-05:00","dependencies":[{"issue_id":"CodeContextBench-8dx","depends_on_id":"CodeContextBench-9sn","type":"discovered-from","created_at":"2025-12-20T13:42:46.978211-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-8el","title":"Database schema updates for judge and reports","description":"Add database tables to support LLM judge evaluations and evaluation reports.\n\nSchema additions:\n- judge_evaluations table (id, run_id, task_name, agent_name, judge_model, score, reasoning, created_at)\n- evaluation_reports table (id, run_id, task_name, report_data JSON, created_at)\n- Foreign keys to evaluation_runs table\n\nImplementation:\n- Update src/benchmark/database.py with new tables\n- Add manager classes: JudgeRegistry, ReportRegistry\n- Migration script for existing database\n\nAcceptance criteria:\n- New tables created in database\n- Manager classes with CRUD operations\n- Migration script tested on existing db","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T19:14:00.020312-05:00","updated_at":"2025-12-26T08:44:59.70525-05:00","closed_at":"2025-12-26T08:44:59.70525-05:00"}
{"id":"CodeContextBench-8jq","title":"Extend comprehension analysis to big_code_mcp and kubernetes benchmarks","description":"The initial 58% comprehension hypothesis was tested on github_mined (PyTorch) tasks only. Need to:\n\n1. FINDINGS TO VALIDATE ACROSS BENCHMARKS:\n   - 'Other' category (44-46% of steps) is primarily reasoning/planning without tool calls\n   - If reasoning is counted as comprehension, agents match human 58% pattern\n   - MCP tools used sparingly (13 calls / 481 total = 2.7%)\n\n2. BENCHMARKS TO TEST:\n   - big_code_mcp: VS Code, Kubernetes, Servo, TensorRT-LLM tasks\n   - kubernetes-specific tasks (if available in github_mined)\n   - Compare MCP usage patterns across codebase sizes\n\n3. ANALYSIS TO PERFORM:\n   - Tool usage breakdown by benchmark type\n   - Reasoning step content analysis (categorize 'other' into subtypes)\n   - Correlation between comprehension ratio and task success by difficulty\n   - MCP tool adoption patterns (why only 2.7% usage?)\n\n4. DELIVERABLES:\n   - Updated comprehension_hypothesis_report.md with cross-benchmark findings\n   - Tool usage heatmap by benchmark\n   - Reasoning category taxonomy","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T17:22:20.159913-05:00","updated_at":"2025-12-24T13:23:58.75076-05:00","closed_at":"2025-12-24T13:23:58.75076-05:00"}
{"id":"CodeContextBench-8px","title":"Evaluation config management UI","description":"Dashboard page for managing evaluation configuration.\n\nFEATURES:\n1. Ref bundle viewer/editor (link to bundle management)\n2. Checklist YAML editor with validation\n3. Scoring weight configuration:\n   - hard_score weight\n   - checklist_coverage weight\n   - checklist_accuracy weight\n   - grounding_score weight\n   - judge_score weight\n4. Threshold configuration (pass/fail cutoffs)\n\n5. 'Re-score stored runs' button:\n   - Re-run evaluation pipeline on stored artifacts\n   - No agent rerun, just re-evaluation\n   - Compare score deltas before/after config change\n\nCreate dashboard/views/eval_config.py","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-27T11:52:12.127003-05:00","updated_at":"2025-12-27T11:52:12.127003-05:00","dependencies":[{"issue_id":"CodeContextBench-8px","depends_on_id":"CodeContextBench-11e","type":"blocks","created_at":"2025-12-27T11:56:58.424432-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-8px","depends_on_id":"CodeContextBench-0zy","type":"blocks","created_at":"2025-12-27T11:57:03.564373-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-8qb","title":"Integrate LLM judge into postprocess pipeline","description":"Refactor llm_judge_experiment.py into reusable module. Integrate multi-LLM voting logic. Make it callable from postprocess script.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:07.387685-05:00","updated_at":"2025-12-25T12:44:56.114083-05:00","closed_at":"2025-12-25T12:44:56.114083-05:00"}
{"id":"CodeContextBench-8tz","title":"Generate DependEval benchmark tasks for 150 indexed repos (DR, RC, ME)","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T12:44:47.332064-05:00","updated_at":"2025-12-20T12:53:51.335443-05:00","closed_at":"2025-12-20T12:53:51.335443-05:00"}
{"id":"CodeContextBench-94j","title":"Implement repo grounding anchor validator","description":"Exposes MCP advantages in measurable way.\n\nANCHOR FORMAT (in generated docs):\nCode pointers:\n- staging/src/k8s.io/apimachinery/pkg/util/managedfields/fieldmanager.go: FieldManager interface\n- pkg/apis/core/types.go: ObjectMeta struct\n\nVALIDATION:\n1. Parse anchors from generated README\n2. Verify file exists in repo\n3. Verify symbol/string exists in file (ripgrep)\n4. Count distinct files referenced\n5. Check required areas hit (e.g., managedfields path + api types)\n\nMETRICS:\n- anchors_total, anchors_valid\n- missing_areas list\n- invalid_anchors list\n\nStore in grounding_validations table.\nAdd grounding panel to Run Detail view.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-27T11:51:39.643135-05:00","updated_at":"2025-12-27T11:51:39.643135-05:00","dependencies":[{"issue_id":"CodeContextBench-94j","depends_on_id":"CodeContextBench-m6c","type":"blocks","created_at":"2025-12-27T11:56:53.288819-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-9sn","title":"Set up adapters for DI-Bench, RepoQA, DependEval + baseline/MCP comparison pipeline","description":"Set up Harbor adapters for three core benchmarks.\n\nSTATUS (as of 2025-12-20):\n✓ RepoQA adapter: Created in ~/harbor/adapters/repoqa/ (CodeContextBench-1tn closed)\n✓ RepoQA verifier: Fixed path issues, verified working (CodeContextBench-8dx closed)\n✓ DependEval adapter: Exists in ~/harbor/adapters/dependeval/\n✓ DI-Bench adapter: Exists in ~/harbor/adapters/dibench/\n\nREMAINING:\n- Test DependEval adapter end-to-end\n- Test DI-Bench adapter end-to-end\n- Create comparison pipeline: Claude baseline vs Claude+Sourcegraph MCP\n- Resolve Docker disk space issue blocking agent execution\n\nMetrics to capture: code quality/accuracy, token use/cost, execution time, dependency reasoning quality.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:36:22.114-05:00","updated_at":"2025-12-21T15:37:10.014793-05:00","closed_at":"2025-12-21T15:37:10.014793-05:00"}
{"id":"CodeContextBench-a2g","title":"Create infrastructure/datasets.yaml for 10Figure corpus","description":"Define external dataset contract: harbor-10figure:base image, TEN_FIGURE_CODEBASES_PATH env var. Document in docs/10FIGURE.md.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:09.901-05:00","updated_at":"2025-12-17T16:44:07.714566-05:00","closed_at":"2025-12-17T16:44:07.714566-05:00","dependencies":[{"issue_id":"CodeContextBench-a2g","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:09.901469-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-aeg","title":"Create Harbor adapter for DI-Bench","description":"Created a complete Harbor adapter for Microsoft's DI-Bench (Dependency Inference Benchmark).\n\n## What was accomplished:\n\n### Core Adapter Implementation\n- **adapter.py** (319 lines): Main adapter with DIBenchInstance data model, DIBenchLoader for JSONL datasets, and DIBenchAdapter for task conversion\n- **run_adapter.py** (147 lines): CLI tool with filtering by language, instance ID, and limits\n- **__init__.py**: Package initialization\n\n### Template System\n- **instruction.md**: Task instructions template with environment specs and output format requirements\n- **task.toml**: Harbor configuration with metadata, timeouts, and resource limits\n- **Dockerfile**: Multi-language environment supporting Python, Rust, C#, JavaScript, Docker-in-Docker for act (GitHub Actions runner)\n- **solve.sh**: Reference solution script that applies dependency configuration patches\n- **test.sh**: Test execution using act to run CI/CD pipelines\n\n### Documentation\n- **README.md**: Complete documentation with prerequisites, usage examples, troubleshooting\n- **QUICKSTART.md**: Step-by-step getting started guide\n- **INTEGRATION.md**: Technical integration guide for Harbor framework\n- **dibench.yaml**: Adapter configuration and metadata\n- **parity_experiments.json**: Validation test cases\n\n## Technical Details:\n\n### Multi-Language Support\n- Python 3.10+ with pip\n- Node.js 18+ with npm\n- Rust (latest stable via rustup)\n- .NET 7.0 for C#\n- GitHub Actions execution via act\n\n### Adapter Features\n- Loads DI-Bench instances from JSONL dataset files\n- Converts repository instances to Harbor task format\n- Preserves metadata and environment specifications\n- Template-based task generation with placeholder replacement\n- Harbor-compatible evaluation (reward 0/1 based on test results)\n\n### File Structure\nCreated 13 files across 5 directories following Harbor adapter patterns from SWE-Bench, LiveCodeBench, and DevEval adapters.\n\n## Integration Ready\nThe adapter is ready for integration into Harbor's dataset registry and can process all DI-Bench instances across Python, Rust, C#, and JavaScript languages.\n\nLocation: /Users/sjarmak/harbor/adapters/dibench/","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-19T14:55:13.832117-05:00","updated_at":"2025-12-19T14:55:29.784104-05:00","closed_at":"2025-12-19T14:55:29.784104-05:00"}
{"id":"CodeContextBench-agv","title":"Investigate SWE-bench reward=0 despite correct fix and passing tests","description":"**ROOT CAUSE CONFIRMED - File Path Mismatch**:\n\nHarbor and SWE-bench test.sh use DIFFERENT log files\\!\n\n**Harbor (verifier.py lines 93-114)**:\n- Executes: bash test.sh | tee /logs/verifier/test-stdout.txt 2\u003e\u00261  \n- Expects: All output in /logs/verifier/test-stdout.txt\n- Reality: Gets only exec() stdout (incomplete, ~134 lines)\n\n**SWE-bench test.sh**:\n- Line 64: LOG_FILE=$(mktemp) → /tmp/tmpXXXXXX\n- Lines 66-67: exec \u003e \u003e(tee \"$LOG_FILE\") 2\u003e\u00261 → redirects to temp file\n- Lines 111-115: Parser adds START_TEST_OUTPUT/END_TEST_OUTPUT to $LOG_FILE\n- Line 150: uv run parser.py | tee -a \"$LOG_FILE\" → appends to temp file\n- Lines 155-160: Writes reward to /logs/verifier/reward.txt ✓\n- MISSING: Never copies $LOG_FILE to /logs/verifier/test-stdout.txt\\!\n\n**Result**:\n- /logs/verifier/test-stdout.txt (Harbor's file): 134 lines, no markers\n- /tmp/tmpXXXXXX (SWE-bench's file): Full output + markers (LOST)\n- Parser runs, sees no markers in test output → found=False → reward=0\n\n**FIX OPTIONS**:\n1. Modify test.sh to write $LOG_FILE to /logs/verifier/test-stdout.txt at end\n2. Modify Harbor verifier to download/read temp file instead\n3. Set LOG_FILE=/logs/verifier/test-stdout.txt in test.sh\n4. Harbor should download container's /logs/verifier BEFORE reading files\n\n**NEXT STEP**: Implement fix #1 (cleanest) - add to test.sh template","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-28T18:41:52.685748-05:00","updated_at":"2025-12-28T22:37:30.658831-05:00","closed_at":"2025-12-28T22:37:30.658831-05:00","close_reason":"Investigation complete. Root cause identified: SWE-bench test.sh uses complex stdout redirections (exec \u003e \u003e(tee LOG_FILE), then exec 1\u003e\u00263 to restore) that prevent parser output from reaching Harbor's test-stdout.txt. Created CodeContextBench-sx4 to track fix."}
{"id":"CodeContextBench-aj1","title":"Admin page (storage health and audit log)","description":"Dashboard admin functionality.\n\nFEATURES:\n1. Storage health\n   - Database size\n   - Jobs directory size\n   - Artifacts count\n   - Cache status\n\n2. Audit log\n   - Config changes (who, when, what)\n   - Run triggers\n   - Evaluation re-runs\n\n3. Settings\n   - Judge model selection\n   - Rate limits\n   - Default timeout values\n\n4. Cleanup utilities\n   - Clear old runs\n   - Prune artifacts\n   - Reset cache\n\nCreate dashboard/views/admin.py","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-27T11:52:22.716162-05:00","updated_at":"2025-12-27T11:52:22.716162-05:00"}
{"id":"CodeContextBench-ax7","title":"Test baseline and MCP agents on SWE-Bench (real benchmark)","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:32:28.834155-05:00","updated_at":"2025-12-19T13:36:19.582412-05:00","closed_at":"2025-12-19T13:36:19.582412-05:00"}
{"id":"CodeContextBench-az9","title":"Investigate sgt-003 MCP failure: Missing from comparison results","description":"MCP run for sgt-003 is completely absent from mcp-10task-20251219/ directory. Baseline completed this task in 280s with 214 steps. Investigate: timeout, API failure, crash during MCP init, or incomplete run. This is a 10% task failure rate for MCP agent.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T17:34:19.926778-05:00","updated_at":"2025-12-20T18:03:43.498341-05:00","closed_at":"2025-12-20T18:03:43.498341-05:00","dependencies":[{"issue_id":"CodeContextBench-az9","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:19.927478-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-b4m","title":"Validate 58% comprehension time hypothesis in benchmark data","description":"Test if agents spend majority of time on comprehension vs generation:\n\nHYPOTHESIS (from ENTERPRISE_CODEBASES.md):\n- Real developers: 58% on comprehension, 35% on navigation, ~7% writing\n- Do AI agents show similar patterns?\n- Does MCP shift time allocation?\n\nMETRICS TO ANALYZE:\n- Time reading files vs writing code\n- Search/navigation actions vs edit actions\n- Re-reading patterns (comprehension indicators)\n- Time to first code change (comprehension complete?)\n\nEXPECTED FINDINGS:\n- Baseline: More trial-and-error, less comprehension\n- MCP: More upfront search/reading, better first attempts\n- Success correlation with comprehension time\n\nANALYSIS:\n- Segment task timeline: comprehension | planning | implementation | validation\n- Compare successful vs failed tasks\n- Identify optimal comprehension:implementation ratio\n\nDELIVERABLE:\n- Report: 'Do AI Agents Mirror Human Time Allocation?'\n- Recommendations for agent design\n- Validation of enterprise metrics relevance\n\nDEPENDENCIES: Metrics infrastructure, completed experiments","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T18:16:28.480592-05:00","updated_at":"2025-12-22T15:30:43.19363-05:00","closed_at":"2025-12-22T15:30:43.19363-05:00","dependencies":[{"issue_id":"CodeContextBench-b4m","depends_on_id":"CodeContextBench-trg","type":"blocks","created_at":"2025-12-20T18:16:40.344755-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-bze","title":"Enhanced A/B comparison with paired task analysis","description":"Key for tool-stack comparison (baseline vs MCP vs DeepSearch).\n\nFEATURES:\n1. Paired comparison - same task_id across tool stacks\n   - Delta scores visualization (MCP - baseline)\n   - Side-by-side metrics table\n\n2. Filtering - faceted filters that persist across tabs\n   - Benchmark, agent, tool_stack, model, date range\n   - Use st.session_state for persistence\n\n3. Charts:\n   - Success rate bar chart by tool stack\n   - Token usage comparison (input/output/cached breakdown)\n   - Cost comparison with caching benefits noted\n   - Judge score radar (multi-dimensional quality)\n\n4. Export: CSV/JSON download of comparison data\n\nUpdate dashboard/views/comparison_enhanced.py or create new compare.py","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-27T11:51:23.806447-05:00","updated_at":"2025-12-27T12:05:52.976875-05:00","closed_at":"2025-12-27T12:05:52.976875-05:00"}
{"id":"CodeContextBench-c65","title":"Implement Trevor Nederlof's exact big code MCP tasks","description":"Phase 2: Implement Trevor Nederlof's exact big code MCP tasks ✅ COMPLETED\n\nImplemented 4 validated big code tasks with proper naming (big-code-{codebase}-{id}):\n\n✅ big-code-vsc-001: VS Code stale diagnostics after git branch switch\n   - Prompt: Understand diagnostics pipeline, fix stale errors on branch switch\n   - Repo: microsoft/vscode (1GB TypeScript)\n   - Why big code: Diagnostics distributed across modules (extension host, file watchers, problems panel)\n\n✅ big-code-servo-001: Servo scrollend DOM event implementation  \n   - Prompt: Add scrollend event with debouncing across all scroll handlers\n   - Repo: servo/servo (1.6GB Rust)\n   - Why big code: Scroll handling scattered across browser/compositor/DOM event systems\n\n✅ big-code-k8s-001: Kubernetes NoScheduleNoTraffic taint effect\n   - Prompt: Implement new taint effect, find all evaluation points\n   - Repo: kubernetes/kubernetes (1.4GB Go)\n   - Why big code: Taint effects evaluated in scheduler, admission controller, endpoint slices\n\n✅ big-code-trt-001: TensorRT-LLM W4A8_MXFP4_INT8 quantization mode\n   - Prompt: Add new quantization mode following W4A8_MXFP4_FP8 pattern\n   - Repo: NVIDIA/TensorRT-LLM (1.6GB Python/C++)\n   - Why big code: Spans Python/C++ boundary, kernel selection, build system, tests\n\nEach task includes:\n- Trevor's exact prompt from research (docs/TREVOR_RESEARCH_DEC2025.md)\n- CLAUDE.md with MCP search guidance\n- Minimal Docker environment (git init, no full clone)\n- time_limit_sec and difficulty for baseline vs MCP comparison\n\nFramework ready: Run with scripts/run_mcp_comparison.sh big_code_mcp big-code-{codebase}-{id}\n\nReference: docs/TREVOR_RESEARCH_DEC2025.md","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T07:42:21.563592-05:00","updated_at":"2025-12-20T07:50:30.382324-05:00","closed_at":"2025-12-20T07:50:30.382324-05:00"}
{"id":"CodeContextBench-c74","title":"Build/test integration benchmarking: Feedback loop metrics","description":"Measure build/test cycle overhead in tool workflows. Track: number of build attempts, test runs, time to first passing test. Simulate slow CI (like 12hr integration tests), flaky tests (40% false positives). Test tool's ability to minimize cycles through better comprehension. Reference Uber's CI scale (10k+ monthly changes), PayPal flaky test stats.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:45.365569-05:00","updated_at":"2025-12-20T22:13:06.104727-05:00","closed_at":"2025-12-20T22:13:06.104727-05:00"}
{"id":"CodeContextBench-ch8","title":"Create reference implementations for benchmark tasks","description":"Create reference implementations for benchmark tasks.\n\nPRIORITY INCREASED from P3 → P2. Needed to distinguish agent 'luck' from genuine understanding.\n\nSCOPE:\nFor each benchmark type, document ideal agent behavior:\n- What searches should be performed\n- What code should be examined\n- Minimal sufficient changes\n\nPURPOSE:\n- Identify when agents get lucky vs genuinely understand\n- Enable trace analysis comparing agent behavior to ideal\n- Provide 'gold standard' for evaluation\n\nOUTPUT:\n- Reference solutions for big_code_mcp tasks\n- Documented search/navigation patterns\n- Success criteria beyond pass/fail","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T13:49:29.143249-05:00","updated_at":"2025-12-24T13:25:36.00135-05:00","closed_at":"2025-12-24T13:25:36.00135-05:00"}
{"id":"CodeContextBench-cu8","title":"Design benchmark lifecycle pipeline","description":"Chain benchmark creation, adapter refresh, harbor validation, and manifest emission into a single script/pipeline so every suite has reproducible setup artifacts (MANIFEST.json with repo commits, env hashes, validation logs).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T13:28:48.789394-05:00","updated_at":"2025-12-25T12:50:46.389658-05:00","closed_at":"2025-12-25T12:50:46.389658-05:00"}
{"id":"CodeContextBench-cvk","title":"Create enterprise-scale test repository (1M+ LOC)","description":"Build large-scale test repository approaching enterprise size:\n\nCOMPOSITION:\n- Combine 15-20 substantial OSS projects\n- Target: 1M-5M LOC total\n- Mix: Python (40%), TypeScript (30%), Go (20%), other (10%)\n\nPROJECTS TO INCLUDE:\n- Web frameworks (Django, FastAPI, Express)\n- Data tools (pandas, numpy, scipy alternatives)\n- CLI tools (rich, click, cobra)\n- Infrastructure (Docker SDK, Kubernetes clients)\n- Testing frameworks\n\nSTRUCTURE:\n- Option A: True monorepo (all in one)\n- Option B: Multi-repo with shared libraries\n- Option C: Hybrid (like Uber's multi-monorepo)\n\nTASKS TO CREATE:\n1. Scale testing: Search performance at 1M+ LOC\n2. Cross-project dependencies\n3. Wide-impact refactors\n4. Performance at enterprise scale\n\nREFERENCE: Google 2B LOC, Stripe 20M LOC - this is 'medium enterprise'\n\nVALIDATION: Compare metrics at 10k, 100k, 1M, 5M LOC scales","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-20T18:15:23.893729-05:00","updated_at":"2025-12-24T13:27:02.927752-05:00","closed_at":"2025-12-24T13:27:02.927752-05:00"}
{"id":"CodeContextBench-cy6","title":"Run Harbor benchmarks on 10figure + github_mined tasks","description":"Execute Phase 2b Harbor benchmarks on github_mined (50 tasks) + 10figure (4 tasks). Pilot: 10 tasks both agents to validate infrastructure \u0026 calibrate timeouts. Full: 50+4 tasks both agents. Capture manifests, NeMo traces, tool usage. Success: \u003e90% tasks complete. Agents: claude-baseline (no search) vs claude-mcp (with Sourcegraph Deep Search). Expected: baseline 30-40%, MCP 40-55%, validates hypothesis. Ready for execution.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:40:44.9698-05:00","updated_at":"2025-12-17T20:26:35.898354-05:00","closed_at":"2025-12-17T20:26:35.898354-05:00","dependencies":[{"issue_id":"CodeContextBench-cy6","depends_on_id":"CodeContextBench-wkb","type":"discovered-from","created_at":"2025-12-17T19:40:44.97196-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-d5w","title":"Clean up history/ directory - move artifacts to proper locations","description":"History directory contains files that should be elsewhere:\n\nARTIFACTS (should be in artifacts/):\n- llm_judge_results.json\n- dependeval_repos_for_indexing.txt\n\nSCRIPTS (should be in scripts/):\n- check_status.sh\n\nALSO REVIEW:\n- archive/ subdirectory (32 items) - assess if needed or can be deleted\n- Multiple dated planning files - keep but organize by phase\n\nACTION: Move files to proper locations, clean up archive/, ensure history/ only has planning docs.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T07:32:57.113396-05:00","updated_at":"2025-12-21T07:48:21.443477-05:00","closed_at":"2025-12-21T07:48:21.443477-05:00"}
{"id":"CodeContextBench-dge","title":"RESOLVED: Original 9.9x speedup was false - real data shows 1.1x, MCP uses MORE tokens","description":"Real original data (baseline-10task-20251219 \u0026 mcp-10task-20251219): Baseline avg 139.7s vs MCP 124.8s = 1.1x speedup. Baseline 34.8M tokens vs MCP 40.5M tokens = MCP is more expensive. sgt-003 missing from MCP. The 'comparison-20251219-clean' used for original report had API key failures masking real results.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-19T17:17:35.174134-05:00","updated_at":"2025-12-19T17:26:30.658829-05:00","closed_at":"2025-12-19T17:26:30.658829-05:00","dependencies":[{"issue_id":"CodeContextBench-dge","depends_on_id":"CodeContextBench-704","type":"discovered-from","created_at":"2025-12-19T17:17:35.175617-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-dgt","title":"Fix big_code_mcp Docker infrastructure (repos not cloning)","description":"big_code_mcp benchmark Docker containers are not properly cloning the target repositories.\n\nSYMPTOMS:\n- RewardFileNotFoundError on all big_code_mcp runs\n- Container shows 22-byte package.json instead of real VS Code (2GB+)\n- git clone in Dockerfile appears to fail silently or use stale cache\n\nROOT CAUSE INVESTIGATION NEEDED:\n1. Harbor may be using cached images from before Dockerfile fixes\n2. git clone --depth 1 may be failing on large repos\n3. Harbor may be overriding workspace content after Docker build\n\nAFFECTED TASKS:\n- big-code-vsc-001 (VS Code)\n- big-code-k8s-001 (Kubernetes)\n- big-code-servo-001 (Servo)\n- big-code-trt-001 (TensorRT-LLM)\n\nSOLUTION OPTIONS:\n1. Force rebuild: harbor run --force-build\n2. Clear Docker cache: docker system prune -a\n3. Use pre-built base images with repos\n4. Mount repos from host instead of cloning in container\n\nVALIDATION:\n- Run: harbor run --path benchmarks/big_code_mcp/big-code-vsc-001 --force-build\n- Check: docker exec container_id ls -la /workspace (should show real VS Code files)","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-22T15:23:50.348647-05:00","updated_at":"2025-12-22T17:03:52.236473-05:00","closed_at":"2025-12-22T17:03:52.236473-05:00"}
{"id":"CodeContextBench-dkf","title":"Re-run Big Code MCP benchmark with proper repo setup","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:10:47.108096-05:00","updated_at":"2025-12-22T15:01:15.016542-05:00","closed_at":"2025-12-22T15:01:15.016542-05:00"}
{"id":"CodeContextBench-dv9","title":"Design and run onboarding simulation experiments","description":"Create benchmark simulating new developer onboarding scenarios:\n\nONBOARDING SCENARIOS (from ENTERPRISE_CODEBASES.md):\n1. Cold start: Unfamiliar codebase, no prior context\n2. Documentation gap: Poor/missing docs (like Google study)\n3. First contribution: Make small fix in unknown subsystem\n4. Codebase exploration: Answer questions about architecture\n\nTASKS:\n- 'Find where feature X is implemented'\n- 'Explain what module Y does'\n- 'Fix simple bug in unfamiliar code'\n- 'Add small feature following existing patterns'\n\nMETRICS:\n- Time to locate relevant code\n- Documentation consultation frequency\n- Code search patterns\n- Comprehension quality (via explanation tasks)\n\nCOMPARISON:\n- Baseline vs MCP on cold-start comprehension\n- Which tool helps 'learn' codebase faster?\n- Correlation between exploration and task success\n\nTARGET: Validate if MCP reduces months-long onboarding\n\nREFERENCE: Google's 3-6 week remote onboarding delay","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-20T18:15:23.393469-05:00","updated_at":"2025-12-24T13:27:13.165123-05:00","closed_at":"2025-12-24T13:27:13.165123-05:00"}
{"id":"CodeContextBench-e69","title":"Agent and tool stack registry UI","description":"Dashboard page for managing agents and tool stacks.\n\nAGENT CONFIG:\n- Name, display name, import path\n- Model selection\n- Prompt template (editable)\n- Max steps, timeouts\n- Version history\n\nTOOL STACKS:\n- baseline: no MCP\n- sourcegraph: full Sourcegraph MCP toolkit\n- deepsearch: Deep Search MCP only\n- Custom stacks\n\nEach stack defines:\n- tools_enabled list\n- mcp_config JSON\n- Tool permissions\n\nStore in agent_versions and tool_stacks tables.\nCreate dashboard/views/agents.py","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-27T11:52:17.432652-05:00","updated_at":"2025-12-27T11:52:17.432652-05:00"}
{"id":"CodeContextBench-eaf","title":"Analyze existing results through enterprise metrics lens","description":"Re-analyze Phase 3 results using enterprise-informed metrics:\n\nEXISTING DATA TO MINE:\n- Phase 3 agent trajectories (50 tasks)\n- Conversation logs and tool calls\n- Code changes and test results\n\nNEW ANALYSIS ANGLES:\n1. Code search patterns:\n   - How many searches per task?\n   - Search success rate\n   - Query refinement patterns\n2. Navigation efficiency:\n   - Files opened vs files changed\n   - Re-reads of same files\n   - Exploration path coherence\n3. Comprehension indicators:\n   - Time before first code change\n   - Questions asked about codebase\n   - Documentation references\n\nCOMPARE:\n- Baseline vs MCP patterns\n- Success vs failure task patterns\n- Task type correlations\n\nOUTPUT:\n- Updated Phase 3 report with enterprise metrics\n- Identify clear differentiators\n- Highlight where MCP shows \u003e2x advantage\n\nREFERENCE: CodeContextBench-13j process quality metrics design","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T18:15:23.637419-05:00","updated_at":"2025-12-20T21:13:56.364343-05:00","closed_at":"2025-12-20T21:13:56.364343-05:00"}
{"id":"CodeContextBench-ehx","title":"Validate RepoQA requires actual tool usage (not pattern matching)","description":"Verify RepoQA semantic navigation genuinely requires tool-based understanding. Run agent trajectories through task instances: measure tool calls made, relevance of retrieved code, whether baseline could memorize answers. Document findings.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:49:28.530035-05:00","updated_at":"2025-12-20T22:17:43.758294-05:00","closed_at":"2025-12-20T22:17:43.758294-05:00"}
{"id":"CodeContextBench-eil","title":"Mine tasks from 6 additional OSS repos","description":"Mine GitHub tasks from firefox, pytorch, vscode, ffmpeg, tensorrt_llm, servo. Generate Harbor task dirs from results. Target 100+ total tasks across all repos for diverse benchmark set.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:32:21.283592-05:00","updated_at":"2025-12-17T19:40:55.159051-05:00","closed_at":"2025-12-17T19:40:55.159051-05:00"}
{"id":"CodeContextBench-ekr","title":"Build benchmark manifest viewer page","description":"Create Streamlit page to display benchmark manifests with dataset info, validation logs, environment fingerprints, and artifact paths.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:45.246126-05:00","updated_at":"2025-12-25T12:42:58.13458-05:00","closed_at":"2025-12-25T12:42:58.13458-05:00"}
{"id":"CodeContextBench-epv","title":"Implement DeathStarBench microservice integration","description":"Integrate DeathStarBench for multi-repo microservice scenarios:\n\nDEATHSTARBENCH COMPONENTS:\n- Social network application (multiple services)\n- Media service (video processing pipeline)\n- Hotel reservation (booking system)\n- E-commerce application\n\nTASKS TO CREATE:\n1. Cross-service bug fixes (trace through service mesh)\n2. API contract updates (breaking changes across services)\n3. Performance optimization (N+1 query problems)\n4. Service dependency analysis\n\nCHALLENGES TO TEST:\n- Multi-repo navigation\n- Service interface understanding\n- Distributed tracing comprehension\n- Microservice-specific patterns\n\nEVALUATION:\n- Success on cross-service changes\n- Completeness (all affected services found)\n- Understanding of service boundaries\n\nREFERENCE: ENTERPRISE_CODEBASES.md synthetic environments section","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-20T18:15:22.879284-05:00","updated_at":"2025-12-24T13:27:33.640238-05:00","closed_at":"2025-12-24T13:27:33.640238-05:00"}
{"id":"CodeContextBench-ews","title":"Create ClaudeCodeAgent (baseline) for Harbor","description":"Implement claude_code_agent.py extending BasePatchAgent. Use Harbor's claude-code CLI. No Sourcegraph tools (baseline condition).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.340214-05:00","updated_at":"2025-12-17T15:56:23.711557-05:00","closed_at":"2025-12-17T15:56:23.711557-05:00","dependencies":[{"issue_id":"CodeContextBench-ews","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.340656-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-fft","title":"Implement benchmark profile config and matrix runner","description":"Introduce a configs/benchmark_profiles.yaml registry describing quick/full task slices, agent matrices (including custom harnesses \u0026 prompt/tool variants), and build a runner that consumes it to launch Harbor jobs with consistent experiment manifests.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-24T13:29:03.057037-05:00","updated_at":"2025-12-25T12:50:46.39028-05:00","closed_at":"2025-12-25T12:50:46.39028-05:00"}
{"id":"CodeContextBench-fgm","title":"Add Deep Search analytics dashboard","description":"Create page to analyze MCP Deep Search usage patterns, query types, retrieval success rates from experiment data.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:38:06.495367-05:00","updated_at":"2025-12-25T12:49:19.386361-05:00","closed_at":"2025-12-25T12:49:19.386361-05:00"}
{"id":"CodeContextBench-gh5","title":"Mine GitHub PRs and regenerate tasks with real commit SHAs","description":"Execute mining pipeline with real commit extraction (CodeContextBench-k70) and regenerate all task definitions with proper pre_fix_rev/ground_truth_rev values. Update task.toml files and Dockerfiles to checkout correct commits. Steps: (1) Set GITHUB_TOKEN, (2) Run mine_tasks.py with multiple repos, (3) Run regenerate_tasks.py to update existing tasks, (4) Validate Dockerfiles checkout pre-fix commits, (5) Spot-check 2-3 tasks manually. Success: All tasks have real commit SHAs and Dockerfiles checkout pre-fix state.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T11:04:07.622307-05:00","updated_at":"2025-12-19T11:14:19.621707-05:00","closed_at":"2025-12-19T11:14:19.621707-05:00"}
{"id":"CodeContextBench-gn9","title":"Create Harbor adapter for DependEval benchmark","description":"","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-19T15:06:39.601134-05:00","updated_at":"2025-12-19T17:53:52.626189-05:00","closed_at":"2025-12-19T17:53:52.626189-05:00"}
{"id":"CodeContextBench-hv7","title":"Build agent comparison view with charts","description":"Create comparison view with plotly charts for retrieval quality, code quality, rewards, and tool usage across agents.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:55.863135-05:00","updated_at":"2025-12-25T12:47:20.045862-05:00","closed_at":"2025-12-25T12:47:20.045862-05:00"}
{"id":"CodeContextBench-iq3","title":"Recommendation Engine - Config optimization suggestions","description":"Analyze failure patterns to suggest agent config improvements. Generate prompt/tool permission diffs. Track experiment progress and link recommendations to outcomes. Files: src/recommend/config_optimizer.py, experiment_tracker.py","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T09:58:16.050358-05:00","updated_at":"2026-01-02T09:58:16.050358-05:00","labels":["epic:CodeContextBench-7t0"]}
{"id":"CodeContextBench-jdm","title":"Scale testing benchmark: Small → Enterprise codebase sizes","description":"Design benchmark that tests tools at multiple codebase scales: small (10k-100k LOC), medium (100k-1M LOC), large (1M-10M LOC), enterprise (10M+ LOC). Simulate scale using combinations of open-source projects. Test code search, navigation, cross-module changes. Reference: Google 2B LOC, Stripe 20M LOC, Uber 100M+ LOC. Key metrics: search latency, comprehension time, multi-file edit correctness.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:44.417793-05:00","updated_at":"2025-12-24T13:25:15.525879-05:00","closed_at":"2025-12-24T13:25:15.525879-05:00"}
{"id":"CodeContextBench-jzv","title":"Document new evaluation pipeline and agent extensibility","description":"Update docs/ARCHITECTURE.md, docs/DEVELOPMENT.md, docs/EXPERIMENT_MANAGEMENT.md to cover the lifecycle pipeline, profile configs, Streamlit dashboard, and guidance for plugging in new agent harnesses/prompt variants.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-24T13:29:45.898481-05:00","updated_at":"2025-12-24T13:29:45.898481-05:00"}
{"id":"CodeContextBench-k4l","title":"IR-SDLC Benchmark Integration","description":"Generate Harbor-compatible task directories from IR-SDLC-Factory benchmarks. Implement dual evaluation measuring both IR metrics (retrieval quality) and patch correctness. Push generated benchmarks to VM via sync layer.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T09:58:10.935299-05:00","updated_at":"2026-01-02T09:58:10.935299-05:00","labels":["epic:CodeContextBench-7t0"]}
{"id":"CodeContextBench-k5s","title":"Establish baseline metrics: What does good tool usage look like?","description":"Establish baseline metrics: What does good tool usage look like?\n\nPRIORITY INCREASED from P3 → P2. This is foundational for interpreting all experiment results.\n\nSCOPE:\n- Define metrics: deepsearch relevance, tokens-per-task, retrieval latency impact\n- Run successful trajectories to extract patterns\n- Create reference for evaluating future agent implementations\n\nOUTPUT:\n- Documented baseline metrics in docs/\n- Example good/bad tool usage patterns\n- Thresholds for 'efficient' vs 'wasteful' tool usage\n\nBLOCKS: All MCP improvement work - need to know what 'good' looks like before optimizing.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T13:49:29.295903-05:00","updated_at":"2025-12-24T13:25:25.769635-05:00","closed_at":"2025-12-24T13:25:25.769635-05:00"}
{"id":"CodeContextBench-k70","title":"Populate pre_fix_rev and ground_truth_rev in task definitions","description":"CRITICAL: Task definitions (task.toml files) have placeholder values for pre_fix_rev and ground_truth_rev. Currently all tasks use generic 'HEAD~1' instead of actual PR commit info. This blocks proper benchmark validation because agent changes can't be compared against known ground truth.\n\nROOT CAUSE: src/task_mining/mine_tasks.py line 84 uses hardcoded 'HEAD~1' placeholder. The GitHub mining analysis doesn't extract actual parent commit before PR merge.\n\nREQUIRED FIX:\n1. Analyze mine_tasks.py to understand what PR metadata is available (GitHubPullRequest object)\n2. Extract merge_commit_sha and parent commit SHA from PR\n3. For each PR: pre_fix_rev = parent commit before merge, ground_truth_rev = merge commit SHA\n4. Update mining script to compute real values instead of placeholders\n5. Regenerate task definitions or update existing ones with correct commits\n6. Validate: Each task.toml should have actual git commits, not 'HEAD~1'\n\nVALIDATION: After fix, tasks should have commits like: pre_fix_rev=abc123def456, ground_truth_rev=def456ghi789 (from actual GitHub PR data)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:26:50.659314-05:00","updated_at":"2025-12-19T10:29:06.790608-05:00","closed_at":"2025-12-19T10:29:06.790608-05:00"}
{"id":"CodeContextBench-ky2","title":"Context switching cost simulation in benchmarks","description":"Design benchmark simulating interruptions and task switching (23min recovery time). Test: partial task completion → context switch → resume. Measure: ability to recover context, redundant work after switch, notes/documentation quality. Simulate microservice 'context-switching hell' with multi-tool workflows (code + logs + docs).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:45.600765-05:00","updated_at":"2025-12-20T22:12:31.479224-05:00","closed_at":"2025-12-20T22:12:31.479224-05:00"}
{"id":"CodeContextBench-l52","title":"Task Dockerfiles missing repository clones - must checkout pre_fix_rev and run real tests","description":"Task Dockerfiles are incomplete stubs without repository clones. Blocks all task validation. Related to CodeContextBench-7wq. Must add: (1) git clone of target repo, (2) git checkout pre_fix_rev, (3) build setup, (4) real test commands. Discovered from baseline test execution: workspace is empty, test can't run, no reward.txt generated.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-19T11:21:09.345723-05:00","updated_at":"2025-12-19T11:23:38.118901-05:00","closed_at":"2025-12-19T11:23:38.118901-05:00","dependencies":[{"issue_id":"CodeContextBench-l52","depends_on_id":"CodeContextBench-gh5","type":"discovered-from","created_at":"2025-12-19T11:21:09.346474-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-lxl","title":"Experiment: Scale impact on tool effectiveness (10k→1M LOC)","description":"Run controlled experiment testing tool performance at different scales:\n\nEXPERIMENT DESIGN:\n- Same task type across different repo sizes\n- Sizes: 10k, 50k, 100k, 500k, 1M LOC\n- Both agents: baseline and MCP\n- Measure degradation curves\n\nHYPOTHESIS:\n- Baseline: Performance degrades faster with scale\n- MCP: More stable across scales\n- Crossover point: where MCP becomes essential (not just helpful)\n\nTASKS:\n- 'Find function implementing X'\n- 'Fix bug in module Y'\n- 'Update API usage across codebase'\n- 'Refactor pattern Z'\n\nMETRICS:\n- Search time and success rate\n- Navigation efficiency\n- Correctness at each scale\n- Token usage / cost\n\nREFERENCE POINTS:\n- Google: 2B LOC, thousands of daily searches\n- Stripe: 20M LOC monorepo\n- Small projects: 10k-100k LOC\n\nDELIVERABLE:\n- Scale performance curves\n- Crossover analysis\n- ROI model: at what scale does MCP pay for itself?\n\nDEPENDENCIES: Enterprise-scale test repository (1M+ LOC)","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T18:16:28.727478-05:00","updated_at":"2025-12-24T13:26:52.687027-05:00","closed_at":"2025-12-24T13:26:52.687027-05:00","dependencies":[{"issue_id":"CodeContextBench-lxl","depends_on_id":"CodeContextBench-cvk","type":"blocks","created_at":"2025-12-20T18:16:40.551643-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-m13","title":"Create metrics extraction from Harbor results","description":"Extract tool usage stats, token counts, success/failure rates, and timing info from Harbor trajectory and result files.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:12.70317-05:00","updated_at":"2025-12-25T12:42:58.133648-05:00","closed_at":"2025-12-25T12:42:58.133648-05:00"}
{"id":"CodeContextBench-m6c","title":"Add missing database tables for evaluation system","description":"Add tables to support checklist evaluation and grounding:\n\n1. ref_bundles - Reference excerpt packs for grounded evaluation\n   (bundle_id, name, sources JSON, excerpts JSON, created_at)\n\n2. checklists - Versioned checklist definitions\n   (checklist_id, ref_bundle_id FK, yaml_content, item_count, created_at)\n\n3. checklist_evaluations - Per-task checklist scoring\n   (run_id, task_name, checklist_id, covered_items JSON, coverage_score, accuracy_score, weighted_score)\n\n4. grounding_validations - Anchor validation results\n   (run_id, task_name, anchors_total, anchors_valid, missing_areas JSON, invalid_anchors JSON)\n\n5. tool_stacks - Tool stack configurations for A/B comparison\n   (stack_id, name, tools_enabled JSON, mcp_config JSON)\n\nUpdate database.py init_database() function.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-27T11:50:47.486143-05:00","updated_at":"2025-12-27T12:03:48.718443-05:00","closed_at":"2025-12-27T12:03:48.718443-05:00"}
{"id":"CodeContextBench-m7j","title":"Capture full multi-turn agent conversations, not just final summary","description":"Current traces only capture final result. Need to capture ALL turns:\n- Each turn = request + response pair\n- For 44-turn conversation: save all 44 interactions\n- Show agent's reasoning evolution (why did it ask tool X, then tool Y, then make change Z?)\n- Identify where MCP (Deep Search) was actually helpful\n- Track when agent gets stuck vs makes progress\n\nEnable via:\n1. Claude API streaming with events\n2. Parse and capture each turn separately\n3. Group into conversation tree/narrative\n4. Analyze decision points where Deep Search changed reasoning","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T08:50:08.075036-05:00","updated_at":"2025-12-22T15:01:23.953363-05:00","closed_at":"2025-12-22T15:01:23.953363-05:00"}
{"id":"CodeContextBench-mk9","title":"Create ClaudeCodeSourcegraphMCPAgent for Harbor","description":"Implement claude_code_sg_mcp_agent.py with Sourcegraph MCP tools enabled. Requires SRC_ACCESS_TOKEN. Treatment condition for A/B testing.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.420936-05:00","updated_at":"2025-12-17T15:56:26.950123-05:00","closed_at":"2025-12-17T15:56:26.950123-05:00","dependencies":[{"issue_id":"CodeContextBench-mk9","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.421346-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-mpg","title":"Develop Streamlit dashboard for benchmarks and agents","description":"Streamlit UI that surfaces benchmark manifests, run histories, resource metrics, Deep Search usage, judge assessments, and hooks to trigger quick/full runs or benchmark validation.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-24T13:29:31.394376-05:00","updated_at":"2025-12-25T12:49:33.247906-05:00","closed_at":"2025-12-25T12:49:33.247906-05:00","dependencies":[{"issue_id":"CodeContextBench-mpg","depends_on_id":"CodeContextBench-xe2","type":"blocks","created_at":"2025-12-25T12:38:45.463556-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-mpg","depends_on_id":"CodeContextBench-ekr","type":"blocks","created_at":"2025-12-25T12:38:50.603077-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-mpg","depends_on_id":"CodeContextBench-1rp","type":"blocks","created_at":"2025-12-25T12:38:55.740828-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-mpg","depends_on_id":"CodeContextBench-hv7","type":"blocks","created_at":"2025-12-25T12:39:00.876811-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-mpg","depends_on_id":"CodeContextBench-nwf","type":"blocks","created_at":"2025-12-25T12:39:06.026225-05:00","created_by":"sjarmak"},{"issue_id":"CodeContextBench-mpg","depends_on_id":"CodeContextBench-fgm","type":"blocks","created_at":"2025-12-25T12:39:11.17564-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-mqz","title":"Fix task environment checkouts to pre-fix commits for proper benchmark validation","description":"CRITICAL: All benchmark tasks currently clone code with fixes already applied. For sgt-001, the Dockerfile clones PyTorch HEAD which includes commit 9d0d198cb50 (thread safety fix). When agents run and try to implement the fix, 'git diff HEAD' shows empty because HEAD already has the changes. This makes all tests fail with 0.0 reward.\n\nROOT CAUSE: Each task's Dockerfile must checkout code to the commit state BEFORE the fix was merged, not after.\n\nREQUIRED FIX: For each task:\n1. Identify the merged PR commit (e.g., 9d0d198cb50 for sgt-001)\n2. Find the commit immediately BEFORE it was merged\n3. Update Dockerfile to checkout that pre-fix commit\n4. Agents will then implement the fix from a clean baseline\n5. 'git diff HEAD' will show their actual changes\n\nEVIDENCE: MCP agent made 2 lines of code changes (test output showed changes detected), proving autonomous env vars are working. But test failed because it can't find NCCLUtils.cpp modifications in git diff against HEAD which already has them.\n\nVALIDATION: After fixing environments, re-run Phase 4 tests. Both agents should show code changes and test results matching the fix implementation.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:07:17.933235-05:00","updated_at":"2025-12-19T10:46:53.204148-05:00","closed_at":"2025-12-19T10:46:53.204148-05:00"}
{"id":"CodeContextBench-mw8","title":"Implement tool_profiles.py for MCP tool configuration","description":"Create tool_profiles.py to manage MCP tool definitions, capabilities, and configuration. Support tool registration, validation, and runtime configuration. Integrate with agents for tool availability at execution time.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:49.18927-05:00","updated_at":"2025-12-20T22:12:31.114315-05:00","closed_at":"2025-12-20T22:12:31.114315-05:00"}
{"id":"CodeContextBench-n0t","title":"Index 150 DependEval repos into Sourcegraph","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T12:44:45.213428-05:00","updated_at":"2025-12-20T12:52:21.361946-05:00","closed_at":"2025-12-20T12:52:21.361946-05:00"}
{"id":"CodeContextBench-nar","title":"Implement build/test feedback loop instrumentation","description":"Add instrumentation to track build/test cycles in benchmark runs:\n\nMETRICS TO CAPTURE:\n- Number of build attempts before success\n- Number of test runs (full suite + partial)\n- Time in build/test feedback loop\n- Test failures: real vs flaky\n- Build errors: syntax vs logic\n\nIMPLEMENTATION:\n- Hook into Harbor's build/test execution\n- Timestamp each build/test invocation\n- Capture stdout/stderr for error analysis\n- Track code changes between attempts\n\nANALYSIS TARGETS:\n- How many cycles does each agent need?\n- Do tools reduce trial-and-error?\n- Correlation: better comprehension = fewer cycles?\n- Compare to enterprise baselines (PayPal 40% flaky)\n\nSIMULATE (optional):\n- Slow CI (add artificial delays)\n- Flaky tests (random failures)\n- Complex build requirements\n\nREFERENCE: ENTERPRISE_CODEBASES.md build/test complexity, 68% burnout from slow feedback","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T18:15:24.144694-05:00","updated_at":"2025-12-24T13:24:24.301742-05:00","closed_at":"2025-12-24T13:24:24.301742-05:00"}
{"id":"CodeContextBench-npk","title":"Archive outdated benchmarks (github_mined_pilot, repoqa_sr_qa_tasks)","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:46:39.241818-05:00","updated_at":"2025-12-21T15:10:25.357067-05:00","closed_at":"2025-12-21T15:10:25.357067-05:00"}
{"id":"CodeContextBench-nqh","title":"Dashboard Refresh - Observability and analysis views","description":"Refactor dashboard for observability-only focus (no evaluation running). New views: Ingestion status, Experiment browser, Agent comparison, LLM judge results, Config editor. Remove evaluation runner views. Update navigation.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T09:58:05.819109-05:00","updated_at":"2026-01-02T09:58:05.819109-05:00","labels":["epic:CodeContextBench-7t0"]}
{"id":"CodeContextBench-nwf","title":"Build run trigger interface","description":"Create interface to trigger benchmark lifecycle, profile runs, or postprocess evaluations with parameter forms.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:38:01.179295-05:00","updated_at":"2025-12-25T12:49:19.385067-05:00","closed_at":"2025-12-25T12:49:19.385067-05:00"}
{"id":"CodeContextBench-onl","title":"Create evaluation_report.json schema and writer","description":"Define JSON schema for evaluation reports that includes manifest metadata, metrics, tool summaries, judge assessments, and environment info. Implement writer in src/benchmark/.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:02.078837-05:00","updated_at":"2025-12-25T12:41:11.223864-05:00","closed_at":"2025-12-25T12:41:11.223864-05:00"}
{"id":"CodeContextBench-ou2","title":"Onboarding simulation benchmark: New developer tasks","description":"Design tasks simulating new developer onboarding (takes months at Google scale). Test: unfamiliar codebase navigation, finding experts/docs, first contribution quality. Measure: time to locate relevant code, documentation consultation frequency, mentor interaction needs. Reference: Google's 3-6 week remote onboarding delay, top hindrances (new tech, poor docs, finding expertise).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:45.845938-05:00","updated_at":"2025-12-20T22:12:31.298046-05:00","closed_at":"2025-12-20T22:12:31.298046-05:00"}
{"id":"CodeContextBench-q2z","title":"Enhanced comparison table with filtering and metrics","description":"Add comprehensive filtering and metrics to comparison table.\n\nImplementation:\n- Filter by: benchmark, agent configuration, MCP type, date range\n- Metrics: pass rate, avg tokens, avg time, total cost, judge scores\n- Visualizations: bar charts for pass rates, token usage trends, cost comparisons\n- Export comparison results (CSV/JSON)\n\nAcceptance criteria:\n- Can filter comparison by multiple criteria\n- All key metrics displayed\n- Visual charts for key comparisons\n- Can export filtered results","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-25T19:13:47.218817-05:00","updated_at":"2025-12-25T19:13:47.218817-05:00","dependencies":[{"issue_id":"CodeContextBench-q2z","depends_on_id":"CodeContextBench-69v","type":"blocks","created_at":"2025-12-25T19:13:47.220856-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-q89","title":"Port GitHub task mining infrastructure from sg_benchmark","description":"Copy task_mining/ with GitHub API query builder. Enable generating real-world tasks from OSS repos (Firefox, Kubernetes, etc.).","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T15:27:23.079179-05:00","updated_at":"2025-12-17T19:26:01.542374-05:00","closed_at":"2025-12-17T19:26:01.542374-05:00","dependencies":[{"issue_id":"CodeContextBench-q89","depends_on_id":"CodeContextBench-6vn","type":"blocks","created_at":"2025-12-17T15:27:23.079635-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-qbg","title":"Dashboard \u0026 Pipeline UX Overhaul","description":"Comprehensive improvement to pipeline flow and dashboard UX for running, observing, measuring, comparing, tracking, and analyzing evaluation runs.\n\nTARGET BENCHMARKS:\n- SWE-bench Verified (pre-installed)\n- SWE-bench Pro\n- Big Code MCP\n- Kubernetes Docs\n- RepoQA\n- Custom Synthetic\n\nTARGET AGENTS:\n- Baseline Claude Code\n- Claude Code + Sourcegraph MCP\n- Claude Code + Deep Search MCP\n\nKEY DELIVERABLES:\n1. Tab-based navigation (Run | Results | Compare | Benchmarks | Eval Config | Agents | Admin)\n2. Checklist-based evaluation for doc tasks\n3. Reference bundle/excerpt pack system for grounded judging\n4. Enhanced A/B comparison with paired analysis\n5. Repo grounding metrics to expose MCP advantages\n6. Pass@k curves and cost-quality scatter plots","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-27T11:50:00.62588-05:00","updated_at":"2025-12-27T11:50:00.62588-05:00"}
{"id":"CodeContextBench-qg9","title":"Expand SWE-Bench tasks with enterprise scenarios","description":"Expand existing benchmark tasks with enterprise scenarios.\n\nNOTE: Priority reduced from P1 → P2. The immediate focus should be validating existing benchmark results (3js) and running comparison (trg) rather than expanding the task set.\n\nORIGINAL GOAL: Create 20-30 tasks reflecting enterprise patterns from SWE-Bench:\n- Wide-impact changes (10+ files)\n- Cross-service coordination\n- Monorepo patterns\n- Performance optimization\n- Onboarding simulation\n\nAPPROACH:\n- Select from SWE-Bench verified dataset\n- Filter for multi-file changes\n- Ensure deterministic test validation\n- Target repos with good test coverage\n\nDEFER UNTIL: CodeContextBench-3js and CodeContextBench-trg complete.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T18:15:22.09943-05:00","updated_at":"2025-12-24T13:24:34.549793-05:00","closed_at":"2025-12-24T13:24:34.549793-05:00"}
{"id":"CodeContextBench-qp1","title":"Monorepo vs Multi-repo benchmark scenarios","description":"Design benchmarks testing both monorepo patterns (like Google/Stripe) and multi-repo microservice patterns (like pre-2017 Uber). Test cross-service changes, dependency management, code search across repos. Simulate wide-impact commits (affecting 100+ modules). Use DeathStarBench or d'Aragona microservice dataset. Measure: blast radius handling, cross-repo refactoring, version skew detection.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:44.654392-05:00","updated_at":"2025-12-24T13:25:05.282064-05:00","closed_at":"2025-12-24T13:25:05.282064-05:00"}
{"id":"CodeContextBench-qtl","title":"Clean up Phase references in docs and beads","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:46:39.062552-05:00","updated_at":"2025-12-21T15:10:25.401413-05:00","closed_at":"2025-12-21T15:10:25.401413-05:00"}
{"id":"CodeContextBench-qxh","title":"Port infrastructure docs: PODMAN.md, docker-wrapper.sh, harbor-config.yaml","description":"Copy working Podman setup from sourcegraph-benchmarks. Include docker wrapper script and Harbor configuration.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:10.050801-05:00","updated_at":"2025-12-17T17:54:10.832954-05:00","closed_at":"2025-12-17T17:54:10.832954-05:00","dependencies":[{"issue_id":"CodeContextBench-qxh","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:10.051232-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-r9s","title":"Move status/report docs from docs/ to history/ (PHASE3_*, MINING_*, TREVOR_*)","description":"Status/report docs in docs/ should be in history/ per project conventions:\n\nFILES TO MOVE:\n- docs/PHASE3_SUMMARY.md (9.1 KB) - Phase 3 status report\n- docs/PHASE3_COMPREHENSIVE_REPORT.md (23.7 KB) - Phase 3 detailed report  \n- docs/MINING_EXECUTION_REPORT.md (8.3 KB) - One-time mining results\n- docs/TREVOR_RESEARCH_DEC2025.md (9.7 KB) - Session research notes\n\nRATIONALE: These are point-in-time status/analysis docs, not permanent reference documentation. Per AGENTS.md guidelines, status docs belong in history/.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T07:32:56.92817-05:00","updated_at":"2025-12-21T07:47:41.503225-05:00","closed_at":"2025-12-21T07:47:41.503225-05:00"}
{"id":"CodeContextBench-rqg","title":"Integrate NeMo-Agent-Toolkit for structured execution tracing","description":"NeMo-Agent-Toolkit provides structured instrumentation (per-call tracing, token counts, latency, success/failure). Refactor observability modules to consume NeMo traces instead of regex-parsing logs. Extract from NeMo: failed_tool_calls, token_usage, per_tool_latency, operation timeline.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T16:59:09.347198-05:00","updated_at":"2025-12-17T17:42:54.46463-05:00","closed_at":"2025-12-17T17:42:54.46463-05:00","dependencies":[{"issue_id":"CodeContextBench-rqg","depends_on_id":"CodeContextBench-4re","type":"discovered-from","created_at":"2025-12-17T16:59:09.347861-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-rvk","title":"Document github_mined benchmark limitations and scope","description":"Add README to benchmarks/github_mined/ explaining: sample size (25 tasks), single repo (PyTorch), task types, what it validates, known limitations. Clarify what this benchmark does/doesn't prove. Reference: history/benchmark-design-review-20251220.md","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:49:28.379858-05:00","updated_at":"2025-12-20T13:52:13.685288-05:00","closed_at":"2025-12-20T13:52:13.685288-05:00"}
{"id":"CodeContextBench-s0c","title":"Redesign big_code_mcp: Pre-clone repos for both baseline and MCP agents","description":"Deep Search found that Phase 3 comparison was invalid: baseline agents saw empty directory stubs while MCP agents could use Sourcegraph to clone repos. Redesign so both agents have identical pre-cloned repository access. This isolates comparison to search strategy effectiveness rather than file access capability. See history/benchmark-design-review-20251220.md","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:49:28.22345-05:00","updated_at":"2025-12-20T13:52:28.821594-05:00","closed_at":"2025-12-20T13:52:28.821594-05:00"}
{"id":"CodeContextBench-s70","title":"Create unified adapter framework","description":"Standardize benchmark adapters for consistency.\n\nCREATE src/benchmark/adapters/:\n1. base.py - BaseAdapter abstract class:\n   - NAME: str\n   - SUPPORTED_LANGUAGES: List[str]\n   - EVALUATION_TYPE: EvaluationType (binary, continuous, checklist)\n   - generate_tasks() -\u003e List[str]\n   - get_verifier() -\u003e BaseVerifier\n\n2. registry.py - Adapter auto-discovery and registration\n\n3. Migrate existing adapters:\n   - repoqa.py (from benchmarks/repoqa/)\n   - dibench.py (from benchmarks/dibench/)\n   - swebench.py (from benchmarks/swebench_pro/)\n\nBENEFITS:\n- Consistent task generation CLI\n- Unified verifier interface\n- Standardized reward format (0.0-1.0)\n- Plugin-style registration","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-27T11:52:28.009744-05:00","updated_at":"2025-12-27T11:52:28.009744-05:00"}
{"id":"CodeContextBench-sbh","title":"Create comparative visualization dashboard for enterprise metrics","description":"Build dashboard visualizing enterprise-informed benchmark metrics:\n\nVISUALIZATIONS:\n1. Time allocation breakdown (58% comprehension, 35% navigation)\n   - Show baseline vs MCP comparison\n   - Per-task and aggregate views\n2. Code search patterns\n   - Frequency heatmaps\n   - Success rate trends\n   - Query refinement flows\n3. Navigation efficiency\n   - Files read vs files changed scatter\n   - Exploration path graphs\n4. Build/test cycles\n   - Cycles to success distribution\n   - Time in feedback loop\n5. Process quality scores\n   - Multi-metric radar charts\n   - Baseline vs MCP side-by-side\n\nTECHNOLOGY:\n- Python: matplotlib/seaborn for static\n- Optional: Plotly/Dash for interactive\n- Export: PNG for reports, HTML for exploration\n\nOUTPUT:\n- Auto-generated after each benchmark run\n- Comparison across benchmark suites\n- Export for presentations/papers\n\nREFERENCE: docs/ROADMAP_ENTERPRISE_BENCHMARKS.md KPI section","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T18:15:24.402151-05:00","updated_at":"2025-12-24T13:24:14.084004-05:00","closed_at":"2025-12-24T13:24:14.084004-05:00","dependencies":[{"issue_id":"CodeContextBench-sbh","depends_on_id":"CodeContextBench-trg","type":"blocks","created_at":"2025-12-22T15:02:55.797025-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-swl","title":"Phase 1: Create CodeContextBench directory skeleton","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:40.440777-05:00","updated_at":"2025-12-17T15:31:50.10868-05:00","closed_at":"2025-12-17T15:31:50.10868-05:00"}
{"id":"CodeContextBench-sx4","title":"Fix SWE-bench verifier output capture issue","description":"SWE-bench test.sh has complex stdout redirections that prevent parser output from reaching Harbor's test-stdout.txt. Need to either: (1) Fix harbor-datasets test.sh generation, or (2) Create custom verifier for SWE-bench that properly captures all output including parser results.  Current issue: pytest output captured but parser evaluation (START_TEST_OUTPUT markers, FAIL_TO_PASS evaluation) never reaches Harbor, causing all rewards=0 even for correct solutions.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-28T22:34:19.035023-05:00","updated_at":"2025-12-29T08:58:16.06697-05:00","closed_at":"2025-12-29T08:58:16.06697-05:00","close_reason":"Implemented SWEBenchVerifier in Harbor fork and switched to Daytona cloud sandboxes. Solution: Created SWEBenchVerifier class to capture parser output correctly, modified trial.py for auto-detection, switched to Daytona environment to avoid ARM Mac QEMU segfaults. Verified with reward=1.0 (69s vs 6+ min). Committed to sjarmak/harbor (77d6a92). Updated documentation in CLAUDE.md."}
{"id":"CodeContextBench-trg","title":"Run baseline vs MCP comparison with enterprise metrics","description":"Execute benchmark comparison capturing enterprise productivity metrics:\n\nEXPERIMENT SETUP:\n- Use existing tasks: big_code, RepoQA, DependEval (NOT SWE-Bench)\n- Both agents: baseline (no MCP) and big_code_mcp\n- Equal file access (pre-cloned repos)\n\nMETRICS TO COMPARE:\n1. Outcome: Correctness, test pass rate\n2. Process (NEW):\n   - Code search frequency and patterns\n   - Files read before understanding\n   - Navigation path efficiency\n   - Build/test cycles to success\n   - Time to comprehension\n\nANALYSIS:\n- Where does MCP show 2x+ improvement?\n- Which enterprise patterns benefit most?\n- Tool usage correlation with success\n- Identify failure modes for both approaches\n\nDELIVERABLE: Comparative report with enterprise lens\n\nDEPENDENCIES: Metrics infrastructure (zez - DONE)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T18:15:22.362919-05:00","updated_at":"2025-12-22T15:26:00.4065-05:00","closed_at":"2025-12-22T15:26:00.4065-05:00","dependencies":[{"issue_id":"CodeContextBench-trg","depends_on_id":"CodeContextBench-3js","type":"blocks","created_at":"2025-12-22T15:02:55.573066-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-tsd","title":"Cross-service change benchmark: Multi-module edits","description":"Design tasks requiring changes across multiple services/modules (like Uber's commits affecting 100-1000+ services). Test: API contract changes propagating through call chain, shared library updates, cross-cutting refactors. Measure: completeness (all affected modules found), correctness (changes are compatible), efficiency (minimal exploration overhead).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:45.134149-05:00","updated_at":"2025-12-24T13:24:44.800003-05:00","closed_at":"2025-12-24T13:24:44.800003-05:00"}
{"id":"CodeContextBench-tz5","title":"Cost-quality scatter plot visualization","description":"Add scatter plot to Compare tab.\n\nX-AXIS: Total cost (USD)\nY-AXIS: Weighted score (0-1)\n\nFEATURES:\n- Point per task/run\n- Color by tool stack\n- Size by token count\n- Hover shows task_id, agent, metrics\n\nShows cost-quality tradeoffs between agents/tool stacks.\nUse Plotly scatter with tool_stack as color dimension.\n\nIntegrate into Compare tab alongside pass@k curves.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-27T11:52:53.400976-05:00","updated_at":"2025-12-27T11:52:53.400976-05:00","dependencies":[{"issue_id":"CodeContextBench-tz5","depends_on_id":"CodeContextBench-bze","type":"blocks","created_at":"2025-12-27T11:57:18.993295-05:00","created_by":"sjarmak"}]}
{"id":"CodeContextBench-u8x","title":"Create postprocess script CLI","description":"Build scripts/postprocess_experiment.py that orchestrates manifest loading, metrics extraction, judge evaluation, and report generation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:23.336332-05:00","updated_at":"2025-12-25T12:47:20.045524-05:00","closed_at":"2025-12-25T12:47:20.045524-05:00"}
{"id":"CodeContextBench-u9l","title":"Standardize benchmark format for 3-agent testing","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:46:39.436102-05:00","updated_at":"2025-12-21T15:10:25.443294-05:00","closed_at":"2025-12-21T15:10:25.443294-05:00"}
{"id":"CodeContextBench-ubt","title":"Create REPORT.md generator","description":"Generate human-readable markdown report from evaluation_report.json with tables, summaries, and judge assessments.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:18.017982-05:00","updated_at":"2025-12-25T12:47:20.044549-05:00","closed_at":"2025-12-25T12:47:20.044549-05:00"}
{"id":"CodeContextBench-ucr","title":"Profile runner blocks on subprocess.run causing KeyboardInterrupt","description":"Profile runner in src/benchmark/profile_runner.py line 405 uses subprocess.run which blocks indefinitely. When user interrupts, gets KeyboardInterrupt in _invoke_harbor method.\n\nError location: profile_runner.py:405 in _invoke_harbor\nIssue: subprocess.run blocks the process, preventing background execution\nImpact: Cannot run profiles in background from dashboard\n\nRelated: Dashboard needs profile runner to execute without blocking so runs can be monitored from Current Runs tab.\n\nLikely fix: Change subprocess.run to subprocess.Popen similar to evaluation_runner.py fix (commit 34a7a378)","status":"open","priority":1,"issue_type":"bug","created_at":"2025-12-28T10:12:51.29074-05:00","updated_at":"2025-12-28T10:12:51.29074-05:00"}
{"id":"CodeContextBench-uza","title":"Verify SWE-bench test.sh patch fixes reward=0 issue","description":"Re-run astropy__astropy-12907 evaluation to verify the patch fixes the reward=0 issue.\n\nWHAT WAS DONE:\n- Patched all 500 SWE-bench Verified tasks in Harbor cache (~/.cache/harbor/tasks/)\n- Added: cp \"$LOG_FILE\" /logs/verifier/test-stdout.txt to test.sh files\n- This ensures Harbor sees complete test output including SWE-bench parser section\n\nEXPECTED OUTCOME:\n- test-stdout.txt should contain START_TEST_OUTPUT/END_TEST_OUTPUT markers\n- SWE-bench parser should successfully evaluate test results\n- Reward should be 1.0 (not 0.0) for correctly solved task\n\nTEST COMMAND:\nharbor run --dataset swebench-verified@1.0 --task-ids astropy__astropy-12907 --agent mini_swe_agent_mcp:MiniSweAgentBaseline --model anthropic/claude-haiku-4-5-20251001 -n 1\n\nVERIFICATION STEPS:\n1. Check jobs/\u003crun-id\u003e/astropy__astropy-12907.../verifier/test-stdout.txt for markers\n2. Check jobs/\u003crun-id\u003e/astropy__astropy-12907.../verifier/reward.txt (should be 1)\n3. Check result.json verifier_result.rewards.reward (should be 1.0)\n\nNOTE: Harbor cache is at ~/.cache/harbor/tasks/ - patches persist until cache is cleared","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-28T19:56:33.842432-05:00","updated_at":"2025-12-29T08:58:16.068498-05:00","closed_at":"2025-12-29T08:58:16.068498-05:00","close_reason":"Implemented SWEBenchVerifier in Harbor fork and switched to Daytona cloud sandboxes. Solution: Created SWEBenchVerifier class to capture parser output correctly, modified trial.py for auto-detection, switched to Daytona environment to avoid ARM Mac QEMU segfaults. Verified with reward=1.0 (69s vs 6+ min). Committed to sjarmak/harbor (77d6a92). Updated documentation in CLAUDE.md."}
{"id":"CodeContextBench-uzn","title":"Port 10Figure task generator and Harbor integration","description":"Copy gen_harbor_tasks.py, test.sh.j2 template, and Harbor dataset configs from harbor-10figure-dataset. Create benchmarks/10figure/ structure.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.302054-05:00","updated_at":"2025-12-17T19:26:01.643071-05:00","closed_at":"2025-12-17T19:26:01.643071-05:00","dependencies":[{"issue_id":"CodeContextBench-uzn","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.302463-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-v9j","title":"Enhance big_code_mcp task instructions to require MCP tool usage","description":"Current instructions let agents work without Sourcegraph tools. Redesign to REQUIRE deepsearch for understanding architecture before changes. Makes MCP value visible. Document expected tool usage patterns.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T13:49:28.991995-05:00","updated_at":"2025-12-22T15:01:23.635549-05:00","closed_at":"2025-12-22T15:01:23.635549-05:00"}
{"id":"CodeContextBench-var","title":"Run baseline vs MCP comparison on DependEval subset","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T12:44:49.538232-05:00","updated_at":"2025-12-20T12:54:34.136229-05:00","closed_at":"2025-12-20T12:54:34.136229-05:00"}
{"id":"CodeContextBench-von","title":"Analyze benchmark results \u0026 generate comparative report","description":"Execute Phase 2c: Aggregate benchmark results \u0026 test hypothesis per MINING_PLAN.md Phase 2c. Hypothesis: Sourcegraph code search improves agent success on multi-file tasks. Aggregate metrics (success rate, efficiency, cost) across task categories, difficulty, language. Stratified analysis revealing which task types benefit most from search. Generate HTML/JSON comparative report. Validate H1: +MCP success \u003e baseline (+10-15% expected).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:47.139209-05:00","updated_at":"2025-12-22T15:01:01.77895-05:00","closed_at":"2025-12-22T15:01:01.77895-05:00","dependencies":[{"issue_id":"CodeContextBench-von","depends_on_id":"CodeContextBench-cy6","type":"discovered-from","created_at":"2025-12-17T19:40:47.140584-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-vqr","title":"Experiment: Task type performance matrix (7 enterprise scenarios)","description":"Comprehensive experiment across 7 enterprise task types:\n\nTASK TYPES (from ENTERPRISE_CODEBASES.md and CodeContextBench-2wz):\n1. Bug fixing across multiple services\n2. Feature implementation with cross-cutting changes\n3. Refactoring and tech debt reduction\n4. Performance optimization\n5. Code review and comprehension\n6. Documentation and knowledge sharing\n7. Onboarding and learning new subsystems\n\nEXPERIMENT:\n- Use existing tasks: big_code, RepoQA, DependEval, github_mined\n- Both agents on all tasks\n- Full metrics capture\n\nANALYSIS:\n- Performance matrix: task type × agent type\n- Where does MCP excel? (hypothesis: comprehension, cross-file)\n- Where is baseline sufficient? (hypothesis: simple, isolated tasks)\n\nDELIVERABLE:\n- Heat map: performance by task type\n- 'Sweet spot' analysis for each tool\n- Task type recommendations for tool selection","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T18:16:29.479379-05:00","updated_at":"2025-12-24T13:26:21.995939-05:00","closed_at":"2025-12-24T13:26:21.995939-05:00"}
{"id":"CodeContextBench-wkb","title":"Mine 6 additional OSS repos \u0026 generate 100+ Harbor tasks","description":"Execute Phase 2a: Mine 6 repos (firefox, pytorch, vscode, ffmpeg, tensorrt_llm, servo) per MINING_PLAN.md. Generate 50-75 high-quality Harbor tasks. See history/MINING_PLAN.md for master strategy (MINING_STRATEGY.md for detailed pipeline, RESEARCH_ALIGNMENT.md for paper alignment). Requirements: multi-file (≥2 files), deterministic verification, real GitHub work. Success: ≥80% validation pass, 5+ language coverage, balanced difficulty distribution.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:40:42.708461-05:00","updated_at":"2025-12-17T20:02:18.30501-05:00","closed_at":"2025-12-17T20:02:18.30501-05:00"}
{"id":"CodeContextBench-xcz","title":"Analysis Layer - Comparison, IR metrics, LLM-as-judge","description":"Implement comparator for baseline vs MCP delta analysis. Calculate IR metrics (precision/recall/MRR) from tool usage. Integrate LLM-as-judge from ir_sdlc. Detect failure patterns. Files: src/analysis/comparator.py, ir_metrics.py, llm_judge.py, failure_patterns.py","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-02T09:58:00.693105-05:00","updated_at":"2026-01-02T09:58:00.693105-05:00","labels":["epic:CodeContextBench-7t0"]}
{"id":"CodeContextBench-xe2","title":"Create Streamlit app structure and navigation","description":"Set up basic Streamlit app with multi-page layout, sidebar navigation, and session state management. Create app.py entrypoint.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T12:37:39.945979-05:00","updated_at":"2025-12-25T12:41:16.515526-05:00","closed_at":"2025-12-25T12:41:16.515526-05:00"}
{"id":"CodeContextBench-xiz","title":"Developer productivity metrics: Comprehension \u0026 navigation","description":"Design metrics capturing real developer workflows: time to comprehension (58% of dev time), navigation efficiency (35% of time), search success rate. Measure: time to locate relevant code, number of files read before understanding, search query refinement count. Compare to industry baselines (Google's internal metrics, Stripe surveys). Test onboarding scenarios (new dev ramp-up time).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:44.907266-05:00","updated_at":"2025-12-24T13:24:55.046394-05:00","closed_at":"2025-12-24T13:24:55.046394-05:00"}
{"id":"CodeContextBench-yao","title":"Validate RepoQA adapter: run 2-task baseline vs MCP comparison","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T11:00:47.513831-05:00","updated_at":"2025-12-20T11:02:02.416848-05:00","closed_at":"2025-12-20T11:02:02.416848-05:00"}
{"id":"CodeContextBench-yie","title":"Remove COMPARISON_DELIVERABLES.md from root","description":"Move COMPARISON_DELIVERABLES.md out of repository root per AGENTS.md root directory rules.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T17:16:42.205576-05:00","updated_at":"2025-12-22T17:16:55.937327-05:00","closed_at":"2025-12-22T17:16:55.937327-05:00"}
{"id":"CodeContextBench-zez","title":"Implement enterprise metrics collection infrastructure","description":"Build infrastructure to capture enterprise-informed metrics during benchmark runs:\n\nMETRICS TO CAPTURE:\n- Code search patterns: frequency, success rate, query refinement\n- Navigation efficiency: files read vs files needed\n- Comprehension indicators: time spent reading, re-reads\n- Tool usage patterns: MCP deep search frequency, baseline context utilization\n- Build/test cycles: attempts to success, time in feedback loop\n\nIMPLEMENTATION:\n- Extend Harbor agent logging to capture these metrics\n- Create metrics collector module\n- Design JSON schema for metric storage\n- Add timestamps for time-based analysis\n\nOUTPUT:\n- metrics.json per task run\n- Aggregated metrics across benchmark suite\n- Visualization-ready data format\n\nREFERENCE: ENTERPRISE_CODEBASES.md sections on developer time allocation","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T18:15:21.835139-05:00","updated_at":"2025-12-20T21:54:10.629665-05:00","closed_at":"2025-12-20T21:54:10.629665-05:00"}
{"id":"CodeContextBench-zyq","title":"Create monorepo simulation benchmark from OSS projects","description":"Build monorepo benchmark by combining open-source projects:\n\nAPPROACH (from ENTERPRISE_CODEBASES.md recommendations):\n- Combine 10-20 top GitHub projects into single repo\n- Simulate Google/Stripe monorepo pattern\n- Total target: 1M+ LOC combined\n\nPROJECT SELECTION:\n- Popular projects with good test coverage\n- Mix of languages (Python, TypeScript, Go)\n- Complementary domains (web, CLI, data processing)\n\nTASKS TO CREATE:\n1. Cross-project dependency tasks (like shared library updates)\n2. Wide-impact refactors (affecting multiple projects)\n3. Code search across entire monorepo\n4. Consistent API pattern enforcement\n\nEVALUATION:\n- Can tools handle monorepo scale?\n- Search effectiveness across project boundaries\n- Multi-project change consistency\n\nREFERENCE: Stripe 20M LOC monorepo, Uber monorepo patterns","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-20T18:15:22.6254-05:00","updated_at":"2025-12-24T13:27:43.892948-05:00","closed_at":"2025-12-24T13:27:43.892948-05:00"}
