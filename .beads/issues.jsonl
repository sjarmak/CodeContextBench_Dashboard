{"id":"CodeContextBench-09h","title":"Fix Harbor framework installation for reproducible benchmarking","description":"Harbor CLI 0.3.0 (pip package) is broken: typer incompatibility, unmaintained. Blocks reproducibility of benchmark results. Solution: Install official Harbor framework (harborai) from harborframework.com instead. This will enable: (1) real task execution with reproducible metrics, (2) shareable setup for others to replicate, (3) publication-ready results. Current direct_benchmark.py uses synthetic mocks. Plan: Install harborai package, validate agent integration, re-run 10-task pilot with real Harbor, document setup for reproducibility. See .beads/ccb-harbor-fix.md for implementation plan (4 hours, 1 session).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T20:42:34.874834-05:00","updated_at":"2025-12-19T12:27:59.111446-05:00","closed_at":"2025-12-19T12:27:59.111446-05:00"}
{"id":"CodeContextBench-0f3","title":"Complete DEVELOPMENT.md \u0026 documentation suite","description":"Update docs to reflect mining strategy \u0026 benchmark execution per MINING_PLAN.md. README.md: quick-start for running benchmarks with github_mined tasks. DEVELOPMENT.md: detailed mining pipeline, agent setup, benchmark execution. TROUBLESHOOTING.md: common issues from mining + Harbor execution. Reference MINING_PLAN.md as canonical source.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:51.499071-05:00","updated_at":"2025-12-17T19:45:53.231529-05:00"}
{"id":"CodeContextBench-0ji","title":"Replace DI-Bench test.sh with Python-based dependency validation (no Docker-in-Docker)","description":"","status":"in_progress","priority":2,"issue_type":"task","created_at":"2025-12-20T10:21:47.710133-05:00","updated_at":"2025-12-20T10:23:01.271059-05:00"}
{"id":"CodeContextBench-1md","title":"Optimize MCP agent: Increase Deep Search usage and reduce token overhead","description":"MCP agent underutilizes Deep Search (only 11 calls in 1248s). Despite having MCP available, agent still uses 36% fewer tool calls but INCREASES token usage (1.16x). Analysis: (1) Review system prompt - encourage Deep Search usage; (2) Optimize context window sizes; (3) Reduce over-reasoning on complex tasks (sgt-009 was 3x slower with MCP). Goal: Achieve speedup without token overhead.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-19T17:34:30.159726-05:00","updated_at":"2025-12-19T17:34:33.279607-05:00","dependencies":[{"issue_id":"CodeContextBench-1md","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:30.160185-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-1x7","title":"Single-task direct comparison: claude-code vs claude-code-mcp with real test validation and streaming output","description":"Run ONE task with both agents. Requirements:\n1. Use Claude streaming JSON output (not --output-format json flag, actual streaming API)\n2. System prompt: explicitly state non-interactive, MUST complete task, no placeholders\n3. Capture full multi-turn conversation (all 44 turns if applicable)\n4. Real test validation (task has actual test command, not empty make test)\n5. Show actual code changes (git diff must have content)\n6. Capture Deep Search queries and responses (for MCP agent)\n7. Output: complete trace JSON with reasoning, queries, results\n\nCompare:\n- Baseline: claude-code (no MCP)\n- MCP: claude-code-sourcegraph-mcp (with Deep Search)\n\nGoal: Clear evidence of whether MCP actually helps or if Phase 3 results were measurement error.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T08:50:01.112183-05:00","updated_at":"2025-12-18T19:03:46.434571-05:00","closed_at":"2025-12-18T19:03:46.434571-05:00"}
{"id":"CodeContextBench-2il","title":"Re-run 50-task benchmark with corrected comparison script","description":"Use the fixed run_10task_comparison.sh with timestamped directories and validation script. Expand to 50-task benchmark to get statistically significant results. Use validate_comparison_results.py to verify data integrity before analysis. Real 10-task data showed only 1.05x speedup with 1.16x higher token cost - need larger sample to confirm pattern.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T17:34:24.856229-05:00","updated_at":"2025-12-19T17:34:28.149379-05:00","dependencies":[{"issue_id":"CodeContextBench-2il","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:24.856716-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-34b","title":"Run full 10-task MCP pilot and compare with baseline (80% success)","description":"DATA FIXED: All 50 main set tasks now properly configured with real commits (pre_fix_rev and ground_truth_rev from mining results). Test scripts corrected to compare against pre_fix_rev instead of HEAD. Ready for 10-task baseline vs MCP comparison on properly configured mined tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T07:20:56.902137-05:00","updated_at":"2025-12-19T14:39:21.007706-05:00","closed_at":"2025-12-19T14:39:21.007706-05:00"}
{"id":"CodeContextBench-3js","title":"Phase 3 Results Invalid: Tests are fake (make test undefined), code_changes empty, no Deep Search visible","description":"","status":"open","priority":1,"issue_type":"bug","created_at":"2025-12-18T08:49:56.738041-05:00","updated_at":"2025-12-18T08:49:56.738041-05:00"}
{"id":"CodeContextBench-4m5","title":"Run 10-task baseline vs MCP comparison on SWE-Bench (real tasks, proven infrastructure)","description":"Run baseline (Claude Code) and MCP (Claude+Sourcegraph) agents on 10 SWE-Bench tasks. SWE-Bench is fully configured in Harbor at /Users/sjarmak/harbor/adapters/swebench/. This validates MCP improves agent performance on real software engineering benchmarks before committing to custom mining infrastructure. Compare metrics: task completion rate, tokens used, execution time.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T14:16:44.126037-05:00","updated_at":"2025-12-19T14:16:46.747106-05:00"}
{"id":"CodeContextBench-4re","title":"Implement simple observability: manifest_writer.py and metrics_collector.py","description":"Replace NeMo with lightweight JSON manifests. Write run_manifest.json with harness, tool_profile, result, retrieval_metrics. Parse Harbor logs for tool usage.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:09.976846-05:00","updated_at":"2025-12-17T16:52:42.199699-05:00","closed_at":"2025-12-17T16:52:42.199699-05:00","dependencies":[{"issue_id":"CodeContextBench-4re","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:09.977265-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-6f2","title":"Apply autonomous environment variables to baseline Claude Code agent","description":"DISCOVERY: MCP agent successfully made code changes (2 lines detected) despite failing the test due to environment setup. This proves the autonomous environment variables (FORCE_AUTO_BACKGROUND_TASKS=1, ENABLE_BACKGROUND_TASKS=1) injected in create_run_agent_commands() are working correctly.\n\nOBSERVATION: Baseline agent made 0 changes, while MCP agent made changes. The only difference is:\n- MCP: Uses ClaudeCodeSourcegraphMCPAgent which injects autonomous env vars\n- Baseline: Uses built-in harbor claude-code agent without these env vars\n\nSOLUTION: Create a baseline-compatible agent (or extend built-in) that also injects the autonomous env vars without the MCP overhead. This allows fair comparison:\n- BaselineClaudeCodeAgent: Injects autonomous vars, no MCP\n- ClaudeCodeSourcegraphMCPAgent: Injects autonomous vars + MCP tools\n\nIMPLEMENTATION:\n1. Create agents/claude_baseline_agent.py extending ClaudeCode\n2. Inject FORCE_AUTO_BACKGROUND_TASKS=1 and ENABLE_BACKGROUND_TASKS=1 in create_run_agent_commands()\n3. Run baseline tests using: --agent-import-path agents.claude_baseline_agent:BaselineClaudeCodeAgent\n4. This enables fair Phase 4 comparison: Baseline vs MCP on equal footing\n\nThis is critical for valid benchmarking - both agents must have access to autonomous operation mode.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:07:29.215662-05:00","updated_at":"2025-12-19T10:08:07.827608-05:00","closed_at":"2025-12-19T10:08:07.827608-05:00"}
{"id":"CodeContextBench-6vn","title":"Port task_schema.py from sg_benchmark","description":"Copy src/task_schema.py with JSON schema validation and TaskSpecification dataclass. This becomes the canonical validator for all tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.133496-05:00","updated_at":"2025-12-17T16:14:40.13573-05:00","closed_at":"2025-12-17T16:14:40.13573-05:00","dependencies":[{"issue_id":"CodeContextBench-6vn","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.13406-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-704","title":"Analyze baseline vs MCP results, validate hypothesis, generate report","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T07:20:59.005639-05:00","updated_at":"2025-12-19T16:27:04.987168-05:00","closed_at":"2025-12-19T16:27:04.987168-05:00","dependencies":[{"issue_id":"CodeContextBench-704","depends_on_id":"CodeContextBench-34b","type":"discovered-from","created_at":"2025-12-18T07:20:59.008337-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-7qs","title":"Update ARCHITECTURE.md and AGENTS.md for unified CodeContextBench","description":"Consolidate docs from sg_benchmark and sourcegraph-benchmarks. Make Claude-first, MCP-first. Keep Amp as optional/legacy.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:23.002458-05:00","updated_at":"2025-12-17T17:55:26.998927-05:00","closed_at":"2025-12-17T17:55:26.998927-05:00"}
{"id":"CodeContextBench-7sp","title":"Add thread safety when calling ncclCommGetAsyncError","description":"","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-19T08:19:33.714087-05:00","updated_at":"2025-12-19T08:22:17.725184-05:00","closed_at":"2025-12-19T08:22:17.725184-05:00"}
{"id":"CodeContextBench-7tn","title":"Validate Harbor + Daytona + Claude Code + Sourcegraph MCP integration","description":"Validate Harbor + Daytona + Claude Code integration. Confirmed baseline works with 6M+ cached tokens. MCP pattern: use --agent claude-code with task Dockerfile calling sourcegraph_mcp_setup.sh.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-18T11:46:16.014003-05:00","updated_at":"2025-12-18T11:49:20.557685-05:00","closed_at":"2025-12-18T11:49:20.557685-05:00"}
{"id":"CodeContextBench-7wq","title":"Fix task Dockerfiles to have real test commands (not empty make test)","description":"Current tasks have 'make test' with no target defined. Need to:\n1. Audit each task: does it have actual test validation?\n2. Update task Dockerfiles to run real tests\n3. Examples: pytest, cargo test, npm test, make check\n4. Verify tests actually fail if code is wrong\n5. Re-run Phase 3 with real validation\n\nWithout this, we can't know if 100% success is real or measurement error.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-18T08:50:04.268838-05:00","updated_at":"2025-12-18T08:50:04.268838-05:00"}
{"id":"CodeContextBench-7xz","title":"Port BasePatchAgent from sourcegraph-benchmarks to agents/base.py","description":"Copy BasePatchAgent from amp_agent.py, generalize for any CLI agent (remove Amp-specific logic). Keep Harbor integration and repo discovery.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.257834-05:00","updated_at":"2025-12-17T15:52:31.770754-05:00","closed_at":"2025-12-17T15:52:31.770754-05:00","dependencies":[{"issue_id":"CodeContextBench-7xz","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.258373-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-81d","title":"Port Harbor runners and benchmark scripts","description":"Migrate run-full-benchmark.sh → harbor_benchmark.sh, compare-harbor-results.py → compare_results.py. Generalize to accept --agent and --benchmark flags.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.224614-05:00","updated_at":"2025-12-17T16:17:07.325137-05:00","closed_at":"2025-12-17T16:17:07.325137-05:00","dependencies":[{"issue_id":"CodeContextBench-81d","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.225009-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-820","title":"Implement tool_profiles.py for MCP tool configuration","description":"Define none, search_only, code_intel, deep_search profiles. Configure which MCP tools are exposed per profile.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T15:27:23.153115-05:00","updated_at":"2025-12-17T18:36:10.118045-05:00","closed_at":"2025-12-17T18:36:10.118045-05:00","dependencies":[{"issue_id":"CodeContextBench-820","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:23.153528-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-848","title":"Run 4-task 10Figure smoke test: Claude baseline vs Claude+MCP","description":"End-to-end validation: run 4 tasks (cross_file, refactor, api_upgrade, bug_localization) with both agent conditions. Compare results and validate hypothesis.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:22.922545-05:00","updated_at":"2025-12-17T16:32:16.244379-05:00","closed_at":"2025-12-17T16:32:16.244379-05:00","dependencies":[{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-7xz","type":"blocks","created_at":"2025-12-17T15:27:22.922992-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-ews","type":"blocks","created_at":"2025-12-17T15:27:22.92344-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-mk9","type":"blocks","created_at":"2025-12-17T15:27:22.923923-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-81d","type":"blocks","created_at":"2025-12-17T15:27:22.924285-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-uzn","type":"blocks","created_at":"2025-12-17T15:27:22.92461-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-9sn","title":"Set up adapters for DI-Bench, RepoQA, DependEval + baseline/MCP comparison pipeline","description":"Set up Harbor adapters for three core benchmarks: DI-Bench (dependency reasoning), RepoQA (semantic navigation), DependEval (scale+hierarchy). Study /Users/sjarmak/harbor source to understand adapter architecture. Create comparison pipeline: Claude baseline vs Claude+Sourcegraph MCP. Metrics to capture: code quality/accuracy, token use/cost, execution time, dependency reasoning quality. Goal: Simple direct comparison showing MCP value.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T13:36:22.114-05:00","updated_at":"2025-12-19T13:36:29.115723-05:00"}
{"id":"CodeContextBench-a2g","title":"Create infrastructure/datasets.yaml for 10Figure corpus","description":"Define external dataset contract: harbor-10figure:base image, TEN_FIGURE_CODEBASES_PATH env var. Document in docs/10FIGURE.md.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:09.901-05:00","updated_at":"2025-12-17T16:44:07.714566-05:00","closed_at":"2025-12-17T16:44:07.714566-05:00","dependencies":[{"issue_id":"CodeContextBench-a2g","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:09.901469-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-aeg","title":"Create Harbor adapter for DI-Bench","description":"Created a complete Harbor adapter for Microsoft's DI-Bench (Dependency Inference Benchmark).\n\n## What was accomplished:\n\n### Core Adapter Implementation\n- **adapter.py** (319 lines): Main adapter with DIBenchInstance data model, DIBenchLoader for JSONL datasets, and DIBenchAdapter for task conversion\n- **run_adapter.py** (147 lines): CLI tool with filtering by language, instance ID, and limits\n- **__init__.py**: Package initialization\n\n### Template System\n- **instruction.md**: Task instructions template with environment specs and output format requirements\n- **task.toml**: Harbor configuration with metadata, timeouts, and resource limits\n- **Dockerfile**: Multi-language environment supporting Python, Rust, C#, JavaScript, Docker-in-Docker for act (GitHub Actions runner)\n- **solve.sh**: Reference solution script that applies dependency configuration patches\n- **test.sh**: Test execution using act to run CI/CD pipelines\n\n### Documentation\n- **README.md**: Complete documentation with prerequisites, usage examples, troubleshooting\n- **QUICKSTART.md**: Step-by-step getting started guide\n- **INTEGRATION.md**: Technical integration guide for Harbor framework\n- **dibench.yaml**: Adapter configuration and metadata\n- **parity_experiments.json**: Validation test cases\n\n## Technical Details:\n\n### Multi-Language Support\n- Python 3.10+ with pip\n- Node.js 18+ with npm\n- Rust (latest stable via rustup)\n- .NET 7.0 for C#\n- GitHub Actions execution via act\n\n### Adapter Features\n- Loads DI-Bench instances from JSONL dataset files\n- Converts repository instances to Harbor task format\n- Preserves metadata and environment specifications\n- Template-based task generation with placeholder replacement\n- Harbor-compatible evaluation (reward 0/1 based on test results)\n\n### File Structure\nCreated 13 files across 5 directories following Harbor adapter patterns from SWE-Bench, LiveCodeBench, and DevEval adapters.\n\n## Integration Ready\nThe adapter is ready for integration into Harbor's dataset registry and can process all DI-Bench instances across Python, Rust, C#, and JavaScript languages.\n\nLocation: /Users/sjarmak/harbor/adapters/dibench/","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-19T14:55:13.832117-05:00","updated_at":"2025-12-19T14:55:29.784104-05:00","closed_at":"2025-12-19T14:55:29.784104-05:00"}
{"id":"CodeContextBench-ax7","title":"Test baseline and MCP agents on SWE-Bench (real benchmark)","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:32:28.834155-05:00","updated_at":"2025-12-19T13:36:19.582412-05:00","closed_at":"2025-12-19T13:36:19.582412-05:00"}
{"id":"CodeContextBench-az9","title":"Investigate sgt-003 MCP failure: Missing from comparison results","description":"MCP run for sgt-003 is completely absent from mcp-10task-20251219/ directory. Baseline completed this task in 280s with 214 steps. Investigate: timeout, API failure, crash during MCP init, or incomplete run. This is a 10% task failure rate for MCP agent.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T17:34:19.926778-05:00","updated_at":"2025-12-19T17:34:22.739783-05:00","dependencies":[{"issue_id":"CodeContextBench-az9","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:19.927478-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-c65","title":"Implement Trevor Nederlof's exact big code MCP tasks","description":"Phase 2: Implement Trevor Nederlof's exact big code MCP tasks ✅ COMPLETED\n\nImplemented 4 validated big code tasks with proper naming (big-code-{codebase}-{id}):\n\n✅ big-code-vsc-001: VS Code stale diagnostics after git branch switch\n   - Prompt: Understand diagnostics pipeline, fix stale errors on branch switch\n   - Repo: microsoft/vscode (1GB TypeScript)\n   - Why big code: Diagnostics distributed across modules (extension host, file watchers, problems panel)\n\n✅ big-code-servo-001: Servo scrollend DOM event implementation  \n   - Prompt: Add scrollend event with debouncing across all scroll handlers\n   - Repo: servo/servo (1.6GB Rust)\n   - Why big code: Scroll handling scattered across browser/compositor/DOM event systems\n\n✅ big-code-k8s-001: Kubernetes NoScheduleNoTraffic taint effect\n   - Prompt: Implement new taint effect, find all evaluation points\n   - Repo: kubernetes/kubernetes (1.4GB Go)\n   - Why big code: Taint effects evaluated in scheduler, admission controller, endpoint slices\n\n✅ big-code-trt-001: TensorRT-LLM W4A8_MXFP4_INT8 quantization mode\n   - Prompt: Add new quantization mode following W4A8_MXFP4_FP8 pattern\n   - Repo: NVIDIA/TensorRT-LLM (1.6GB Python/C++)\n   - Why big code: Spans Python/C++ boundary, kernel selection, build system, tests\n\nEach task includes:\n- Trevor's exact prompt from research (docs/TREVOR_RESEARCH_DEC2025.md)\n- CLAUDE.md with MCP search guidance\n- Minimal Docker environment (git init, no full clone)\n- time_limit_sec and difficulty for baseline vs MCP comparison\n\nFramework ready: Run with scripts/run_mcp_comparison.sh big_code_mcp big-code-{codebase}-{id}\n\nReference: docs/TREVOR_RESEARCH_DEC2025.md","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T07:42:21.563592-05:00","updated_at":"2025-12-20T07:50:30.382324-05:00","closed_at":"2025-12-20T07:50:30.382324-05:00"}
{"id":"CodeContextBench-cy6","title":"Run Harbor benchmarks on 10figure + github_mined tasks","description":"Execute Phase 2b Harbor benchmarks on github_mined (50 tasks) + 10figure (4 tasks). Pilot: 10 tasks both agents to validate infrastructure \u0026 calibrate timeouts. Full: 50+4 tasks both agents. Capture manifests, NeMo traces, tool usage. Success: \u003e90% tasks complete. Agents: claude-baseline (no search) vs claude-mcp (with Sourcegraph Deep Search). Expected: baseline 30-40%, MCP 40-55%, validates hypothesis. Ready for execution.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:40:44.9698-05:00","updated_at":"2025-12-17T20:26:35.898354-05:00","closed_at":"2025-12-17T20:26:35.898354-05:00","dependencies":[{"issue_id":"CodeContextBench-cy6","depends_on_id":"CodeContextBench-wkb","type":"discovered-from","created_at":"2025-12-17T19:40:44.97196-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-dge","title":"RESOLVED: Original 9.9x speedup was false - real data shows 1.1x, MCP uses MORE tokens","description":"Real original data (baseline-10task-20251219 \u0026 mcp-10task-20251219): Baseline avg 139.7s vs MCP 124.8s = 1.1x speedup. Baseline 34.8M tokens vs MCP 40.5M tokens = MCP is more expensive. sgt-003 missing from MCP. The 'comparison-20251219-clean' used for original report had API key failures masking real results.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-19T17:17:35.174134-05:00","updated_at":"2025-12-19T17:26:30.658829-05:00","closed_at":"2025-12-19T17:26:30.658829-05:00","dependencies":[{"issue_id":"CodeContextBench-dge","depends_on_id":"CodeContextBench-704","type":"discovered-from","created_at":"2025-12-19T17:17:35.175617-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-eil","title":"Mine tasks from 6 additional OSS repos","description":"Mine GitHub tasks from firefox, pytorch, vscode, ffmpeg, tensorrt_llm, servo. Generate Harbor task dirs from results. Target 100+ total tasks across all repos for diverse benchmark set.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:32:21.283592-05:00","updated_at":"2025-12-17T19:40:55.159051-05:00","closed_at":"2025-12-17T19:40:55.159051-05:00"}
{"id":"CodeContextBench-ews","title":"Create ClaudeCodeAgent (baseline) for Harbor","description":"Implement claude_code_agent.py extending BasePatchAgent. Use Harbor's claude-code CLI. No Sourcegraph tools (baseline condition).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.340214-05:00","updated_at":"2025-12-17T15:56:23.711557-05:00","closed_at":"2025-12-17T15:56:23.711557-05:00","dependencies":[{"issue_id":"CodeContextBench-ews","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.340656-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-gh5","title":"Mine GitHub PRs and regenerate tasks with real commit SHAs","description":"Execute mining pipeline with real commit extraction (CodeContextBench-k70) and regenerate all task definitions with proper pre_fix_rev/ground_truth_rev values. Update task.toml files and Dockerfiles to checkout correct commits. Steps: (1) Set GITHUB_TOKEN, (2) Run mine_tasks.py with multiple repos, (3) Run regenerate_tasks.py to update existing tasks, (4) Validate Dockerfiles checkout pre-fix commits, (5) Spot-check 2-3 tasks manually. Success: All tasks have real commit SHAs and Dockerfiles checkout pre-fix state.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T11:04:07.622307-05:00","updated_at":"2025-12-19T11:14:19.621707-05:00","closed_at":"2025-12-19T11:14:19.621707-05:00"}
{"id":"CodeContextBench-gn9","title":"Create Harbor adapter for DependEval benchmark","description":"","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-19T15:06:39.601134-05:00","updated_at":"2025-12-19T17:53:52.626189-05:00","closed_at":"2025-12-19T17:53:52.626189-05:00"}
{"id":"CodeContextBench-k70","title":"Populate pre_fix_rev and ground_truth_rev in task definitions","description":"CRITICAL: Task definitions (task.toml files) have placeholder values for pre_fix_rev and ground_truth_rev. Currently all tasks use generic 'HEAD~1' instead of actual PR commit info. This blocks proper benchmark validation because agent changes can't be compared against known ground truth.\n\nROOT CAUSE: src/task_mining/mine_tasks.py line 84 uses hardcoded 'HEAD~1' placeholder. The GitHub mining analysis doesn't extract actual parent commit before PR merge.\n\nREQUIRED FIX:\n1. Analyze mine_tasks.py to understand what PR metadata is available (GitHubPullRequest object)\n2. Extract merge_commit_sha and parent commit SHA from PR\n3. For each PR: pre_fix_rev = parent commit before merge, ground_truth_rev = merge commit SHA\n4. Update mining script to compute real values instead of placeholders\n5. Regenerate task definitions or update existing ones with correct commits\n6. Validate: Each task.toml should have actual git commits, not 'HEAD~1'\n\nVALIDATION: After fix, tasks should have commits like: pre_fix_rev=abc123def456, ground_truth_rev=def456ghi789 (from actual GitHub PR data)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:26:50.659314-05:00","updated_at":"2025-12-19T10:29:06.790608-05:00","closed_at":"2025-12-19T10:29:06.790608-05:00"}
{"id":"CodeContextBench-l52","title":"Task Dockerfiles missing repository clones - must checkout pre_fix_rev and run real tests","description":"Task Dockerfiles are incomplete stubs without repository clones. Blocks all task validation. Related to CodeContextBench-7wq. Must add: (1) git clone of target repo, (2) git checkout pre_fix_rev, (3) build setup, (4) real test commands. Discovered from baseline test execution: workspace is empty, test can't run, no reward.txt generated.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-19T11:21:09.345723-05:00","updated_at":"2025-12-19T11:23:38.118901-05:00","closed_at":"2025-12-19T11:23:38.118901-05:00","dependencies":[{"issue_id":"CodeContextBench-l52","depends_on_id":"CodeContextBench-gh5","type":"discovered-from","created_at":"2025-12-19T11:21:09.346474-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-m7j","title":"Capture full multi-turn agent conversations, not just final summary","description":"Current traces only capture final result. Need to capture ALL turns:\n- Each turn = request + response pair\n- For 44-turn conversation: save all 44 interactions\n- Show agent's reasoning evolution (why did it ask tool X, then tool Y, then make change Z?)\n- Identify where MCP (Deep Search) was actually helpful\n- Track when agent gets stuck vs makes progress\n\nEnable via:\n1. Claude API streaming with events\n2. Parse and capture each turn separately\n3. Group into conversation tree/narrative\n4. Analyze decision points where Deep Search changed reasoning","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-18T08:50:08.075036-05:00","updated_at":"2025-12-18T08:50:08.075036-05:00"}
{"id":"CodeContextBench-mk9","title":"Create ClaudeCodeSourcegraphMCPAgent for Harbor","description":"Implement claude_code_sg_mcp_agent.py with Sourcegraph MCP tools enabled. Requires SRC_ACCESS_TOKEN. Treatment condition for A/B testing.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.420936-05:00","updated_at":"2025-12-17T15:56:26.950123-05:00","closed_at":"2025-12-17T15:56:26.950123-05:00","dependencies":[{"issue_id":"CodeContextBench-mk9","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.421346-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-mqz","title":"Fix task environment checkouts to pre-fix commits for proper benchmark validation","description":"CRITICAL: All benchmark tasks currently clone code with fixes already applied. For sgt-001, the Dockerfile clones PyTorch HEAD which includes commit 9d0d198cb50 (thread safety fix). When agents run and try to implement the fix, 'git diff HEAD' shows empty because HEAD already has the changes. This makes all tests fail with 0.0 reward.\n\nROOT CAUSE: Each task's Dockerfile must checkout code to the commit state BEFORE the fix was merged, not after.\n\nREQUIRED FIX: For each task:\n1. Identify the merged PR commit (e.g., 9d0d198cb50 for sgt-001)\n2. Find the commit immediately BEFORE it was merged\n3. Update Dockerfile to checkout that pre-fix commit\n4. Agents will then implement the fix from a clean baseline\n5. 'git diff HEAD' will show their actual changes\n\nEVIDENCE: MCP agent made 2 lines of code changes (test output showed changes detected), proving autonomous env vars are working. But test failed because it can't find NCCLUtils.cpp modifications in git diff against HEAD which already has them.\n\nVALIDATION: After fixing environments, re-run Phase 4 tests. Both agents should show code changes and test results matching the fix implementation.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:07:17.933235-05:00","updated_at":"2025-12-19T10:46:53.204148-05:00","closed_at":"2025-12-19T10:46:53.204148-05:00"}
{"id":"CodeContextBench-mw8","title":"Implement tool_profiles.py for MCP tool configuration","description":"Create tool_profiles.py to manage MCP tool definitions, capabilities, and configuration. Support tool registration, validation, and runtime configuration. Integrate with agents for tool availability at execution time.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:49.18927-05:00","updated_at":"2025-12-17T19:40:49.18927-05:00"}
{"id":"CodeContextBench-q89","title":"Port GitHub task mining infrastructure from sg_benchmark","description":"Copy task_mining/ with GitHub API query builder. Enable generating real-world tasks from OSS repos (Firefox, Kubernetes, etc.).","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T15:27:23.079179-05:00","updated_at":"2025-12-17T19:26:01.542374-05:00","closed_at":"2025-12-17T19:26:01.542374-05:00","dependencies":[{"issue_id":"CodeContextBench-q89","depends_on_id":"CodeContextBench-6vn","type":"blocks","created_at":"2025-12-17T15:27:23.079635-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-qxh","title":"Port infrastructure docs: PODMAN.md, docker-wrapper.sh, harbor-config.yaml","description":"Copy working Podman setup from sourcegraph-benchmarks. Include docker wrapper script and Harbor configuration.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:10.050801-05:00","updated_at":"2025-12-17T17:54:10.832954-05:00","closed_at":"2025-12-17T17:54:10.832954-05:00","dependencies":[{"issue_id":"CodeContextBench-qxh","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:10.051232-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-rqg","title":"Integrate NeMo-Agent-Toolkit for structured execution tracing","description":"NeMo-Agent-Toolkit provides structured instrumentation (per-call tracing, token counts, latency, success/failure). Refactor observability modules to consume NeMo traces instead of regex-parsing logs. Extract from NeMo: failed_tool_calls, token_usage, per_tool_latency, operation timeline.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T16:59:09.347198-05:00","updated_at":"2025-12-17T17:42:54.46463-05:00","closed_at":"2025-12-17T17:42:54.46463-05:00","dependencies":[{"issue_id":"CodeContextBench-rqg","depends_on_id":"CodeContextBench-4re","type":"discovered-from","created_at":"2025-12-17T16:59:09.347861-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-swl","title":"Phase 1: Create CodeContextBench directory skeleton","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:40.440777-05:00","updated_at":"2025-12-17T15:31:50.10868-05:00","closed_at":"2025-12-17T15:31:50.10868-05:00"}
{"id":"CodeContextBench-uzn","title":"Port 10Figure task generator and Harbor integration","description":"Copy gen_harbor_tasks.py, test.sh.j2 template, and Harbor dataset configs from harbor-10figure-dataset. Create benchmarks/10figure/ structure.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.302054-05:00","updated_at":"2025-12-17T19:26:01.643071-05:00","closed_at":"2025-12-17T19:26:01.643071-05:00","dependencies":[{"issue_id":"CodeContextBench-uzn","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.302463-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-von","title":"Analyze benchmark results \u0026 generate comparative report","description":"Execute Phase 2c: Aggregate benchmark results \u0026 test hypothesis per MINING_PLAN.md Phase 2c. Hypothesis: Sourcegraph code search improves agent success on multi-file tasks. Aggregate metrics (success rate, efficiency, cost) across task categories, difficulty, language. Stratified analysis revealing which task types benefit most from search. Generate HTML/JSON comparative report. Validate H1: +MCP success \u003e baseline (+10-15% expected).","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:47.139209-05:00","updated_at":"2025-12-17T19:45:49.021383-05:00","dependencies":[{"issue_id":"CodeContextBench-von","depends_on_id":"CodeContextBench-cy6","type":"discovered-from","created_at":"2025-12-17T19:40:47.140584-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-wkb","title":"Mine 6 additional OSS repos \u0026 generate 100+ Harbor tasks","description":"Execute Phase 2a: Mine 6 repos (firefox, pytorch, vscode, ffmpeg, tensorrt_llm, servo) per MINING_PLAN.md. Generate 50-75 high-quality Harbor tasks. See history/MINING_PLAN.md for master strategy (MINING_STRATEGY.md for detailed pipeline, RESEARCH_ALIGNMENT.md for paper alignment). Requirements: multi-file (≥2 files), deterministic verification, real GitHub work. Success: ≥80% validation pass, 5+ language coverage, balanced difficulty distribution.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:40:42.708461-05:00","updated_at":"2025-12-17T20:02:18.30501-05:00","closed_at":"2025-12-17T20:02:18.30501-05:00"}
