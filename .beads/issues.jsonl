{"id":"CodeContextBench-09h","title":"Fix Harbor framework installation for reproducible benchmarking","description":"Harbor CLI 0.3.0 (pip package) is broken: typer incompatibility, unmaintained. Blocks reproducibility of benchmark results. Solution: Install official Harbor framework (harborai) from harborframework.com instead. This will enable: (1) real task execution with reproducible metrics, (2) shareable setup for others to replicate, (3) publication-ready results. Current direct_benchmark.py uses synthetic mocks. Plan: Install harborai package, validate agent integration, re-run 10-task pilot with real Harbor, document setup for reproducibility. See .beads/ccb-harbor-fix.md for implementation plan (4 hours, 1 session).","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-17T20:42:34.874834-05:00","updated_at":"2025-12-17T20:42:39.075701-05:00"}
{"id":"CodeContextBench-0f3","title":"Complete DEVELOPMENT.md \u0026 documentation suite","description":"Update docs to reflect mining strategy \u0026 benchmark execution per MINING_PLAN.md. README.md: quick-start for running benchmarks with github_mined tasks. DEVELOPMENT.md: detailed mining pipeline, agent setup, benchmark execution. TROUBLESHOOTING.md: common issues from mining + Harbor execution. Reference MINING_PLAN.md as canonical source.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:51.499071-05:00","updated_at":"2025-12-17T19:45:53.231529-05:00"}
{"id":"CodeContextBench-1x7","title":"Single-task direct comparison: claude-code vs claude-code-mcp with real test validation and streaming output","description":"Run ONE task with both agents. Requirements:\n1. Use Claude streaming JSON output (not --output-format json flag, actual streaming API)\n2. System prompt: explicitly state non-interactive, MUST complete task, no placeholders\n3. Capture full multi-turn conversation (all 44 turns if applicable)\n4. Real test validation (task has actual test command, not empty make test)\n5. Show actual code changes (git diff must have content)\n6. Capture Deep Search queries and responses (for MCP agent)\n7. Output: complete trace JSON with reasoning, queries, results\n\nCompare:\n- Baseline: claude-code (no MCP)\n- MCP: claude-code-sourcegraph-mcp (with Deep Search)\n\nGoal: Clear evidence of whether MCP actually helps or if Phase 3 results were measurement error.","status":"in_progress","priority":1,"issue_type":"task","created_at":"2025-12-18T08:50:01.112183-05:00","updated_at":"2025-12-18T08:50:11.985558-05:00"}
{"id":"CodeContextBench-34b","title":"Run full 10-task MCP pilot and compare with baseline (80% success)","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T07:20:56.902137-05:00","updated_at":"2025-12-18T08:33:01.346167-05:00","closed_at":"2025-12-18T08:33:01.346205-05:00"}
{"id":"CodeContextBench-3js","title":"Phase 3 Results Invalid: Tests are fake (make test undefined), code_changes empty, no Deep Search visible","description":"","status":"open","priority":1,"issue_type":"bug","created_at":"2025-12-18T08:49:56.738041-05:00","updated_at":"2025-12-18T08:49:56.738041-05:00"}
{"id":"CodeContextBench-4re","title":"Implement simple observability: manifest_writer.py and metrics_collector.py","description":"Replace NeMo with lightweight JSON manifests. Write run_manifest.json with harness, tool_profile, result, retrieval_metrics. Parse Harbor logs for tool usage.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:09.976846-05:00","updated_at":"2025-12-17T16:52:42.199699-05:00","closed_at":"2025-12-17T16:52:42.199699-05:00","dependencies":[{"issue_id":"CodeContextBench-4re","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:09.977265-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-6vn","title":"Port task_schema.py from sg_benchmark","description":"Copy src/task_schema.py with JSON schema validation and TaskSpecification dataclass. This becomes the canonical validator for all tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.133496-05:00","updated_at":"2025-12-17T16:14:40.13573-05:00","closed_at":"2025-12-17T16:14:40.13573-05:00","dependencies":[{"issue_id":"CodeContextBench-6vn","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.13406-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-704","title":"Analyze baseline vs MCP results, validate hypothesis, generate report","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-18T07:20:59.005639-05:00","updated_at":"2025-12-18T07:20:59.005639-05:00","dependencies":[{"issue_id":"CodeContextBench-704","depends_on_id":"CodeContextBench-34b","type":"discovered-from","created_at":"2025-12-18T07:20:59.008337-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-7qs","title":"Update ARCHITECTURE.md and AGENTS.md for unified CodeContextBench","description":"Consolidate docs from sg_benchmark and sourcegraph-benchmarks. Make Claude-first, MCP-first. Keep Amp as optional/legacy.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:23.002458-05:00","updated_at":"2025-12-17T17:55:26.998927-05:00","closed_at":"2025-12-17T17:55:26.998927-05:00"}
{"id":"CodeContextBench-7wq","title":"Fix task Dockerfiles to have real test commands (not empty make test)","description":"Current tasks have 'make test' with no target defined. Need to:\n1. Audit each task: does it have actual test validation?\n2. Update task Dockerfiles to run real tests\n3. Examples: pytest, cargo test, npm test, make check\n4. Verify tests actually fail if code is wrong\n5. Re-run Phase 3 with real validation\n\nWithout this, we can't know if 100% success is real or measurement error.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-18T08:50:04.268838-05:00","updated_at":"2025-12-18T08:50:04.268838-05:00"}
{"id":"CodeContextBench-7xz","title":"Port BasePatchAgent from sourcegraph-benchmarks to agents/base.py","description":"Copy BasePatchAgent from amp_agent.py, generalize for any CLI agent (remove Amp-specific logic). Keep Harbor integration and repo discovery.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.257834-05:00","updated_at":"2025-12-17T15:52:31.770754-05:00","closed_at":"2025-12-17T15:52:31.770754-05:00","dependencies":[{"issue_id":"CodeContextBench-7xz","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.258373-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-81d","title":"Port Harbor runners and benchmark scripts","description":"Migrate run-full-benchmark.sh → harbor_benchmark.sh, compare-harbor-results.py → compare_results.py. Generalize to accept --agent and --benchmark flags.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.224614-05:00","updated_at":"2025-12-17T16:17:07.325137-05:00","closed_at":"2025-12-17T16:17:07.325137-05:00","dependencies":[{"issue_id":"CodeContextBench-81d","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.225009-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-820","title":"Implement tool_profiles.py for MCP tool configuration","description":"Define none, search_only, code_intel, deep_search profiles. Configure which MCP tools are exposed per profile.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T15:27:23.153115-05:00","updated_at":"2025-12-17T18:36:10.118045-05:00","closed_at":"2025-12-17T18:36:10.118045-05:00","dependencies":[{"issue_id":"CodeContextBench-820","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:23.153528-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-848","title":"Run 4-task 10Figure smoke test: Claude baseline vs Claude+MCP","description":"End-to-end validation: run 4 tasks (cross_file, refactor, api_upgrade, bug_localization) with both agent conditions. Compare results and validate hypothesis.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:22.922545-05:00","updated_at":"2025-12-17T16:32:16.244379-05:00","closed_at":"2025-12-17T16:32:16.244379-05:00","dependencies":[{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-7xz","type":"blocks","created_at":"2025-12-17T15:27:22.922992-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-ews","type":"blocks","created_at":"2025-12-17T15:27:22.92344-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-mk9","type":"blocks","created_at":"2025-12-17T15:27:22.923923-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-81d","type":"blocks","created_at":"2025-12-17T15:27:22.924285-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-uzn","type":"blocks","created_at":"2025-12-17T15:27:22.92461-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-a2g","title":"Create infrastructure/datasets.yaml for 10Figure corpus","description":"Define external dataset contract: harbor-10figure:base image, TEN_FIGURE_CODEBASES_PATH env var. Document in docs/10FIGURE.md.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:09.901-05:00","updated_at":"2025-12-17T16:44:07.714566-05:00","closed_at":"2025-12-17T16:44:07.714566-05:00","dependencies":[{"issue_id":"CodeContextBench-a2g","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:09.901469-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-cy6","title":"Run Harbor benchmarks on 10figure + github_mined tasks","description":"Execute Phase 2b Harbor benchmarks on github_mined (50 tasks) + 10figure (4 tasks). Pilot: 10 tasks both agents to validate infrastructure \u0026 calibrate timeouts. Full: 50+4 tasks both agents. Capture manifests, NeMo traces, tool usage. Success: \u003e90% tasks complete. Agents: claude-baseline (no search) vs claude-mcp (with Sourcegraph Deep Search). Expected: baseline 30-40%, MCP 40-55%, validates hypothesis. Ready for execution.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:40:44.9698-05:00","updated_at":"2025-12-17T20:26:35.898354-05:00","closed_at":"2025-12-17T20:26:35.898354-05:00","dependencies":[{"issue_id":"CodeContextBench-cy6","depends_on_id":"CodeContextBench-wkb","type":"discovered-from","created_at":"2025-12-17T19:40:44.97196-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-eil","title":"Mine tasks from 6 additional OSS repos","description":"Mine GitHub tasks from firefox, pytorch, vscode, ffmpeg, tensorrt_llm, servo. Generate Harbor task dirs from results. Target 100+ total tasks across all repos for diverse benchmark set.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:32:21.283592-05:00","updated_at":"2025-12-17T19:40:55.159051-05:00","closed_at":"2025-12-17T19:40:55.159051-05:00"}
{"id":"CodeContextBench-ews","title":"Create ClaudeCodeAgent (baseline) for Harbor","description":"Implement claude_code_agent.py extending BasePatchAgent. Use Harbor's claude-code CLI. No Sourcegraph tools (baseline condition).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.340214-05:00","updated_at":"2025-12-17T15:56:23.711557-05:00","closed_at":"2025-12-17T15:56:23.711557-05:00","dependencies":[{"issue_id":"CodeContextBench-ews","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.340656-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-m7j","title":"Capture full multi-turn agent conversations, not just final summary","description":"Current traces only capture final result. Need to capture ALL turns:\n- Each turn = request + response pair\n- For 44-turn conversation: save all 44 interactions\n- Show agent's reasoning evolution (why did it ask tool X, then tool Y, then make change Z?)\n- Identify where MCP (Deep Search) was actually helpful\n- Track when agent gets stuck vs makes progress\n\nEnable via:\n1. Claude API streaming with events\n2. Parse and capture each turn separately\n3. Group into conversation tree/narrative\n4. Analyze decision points where Deep Search changed reasoning","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-18T08:50:08.075036-05:00","updated_at":"2025-12-18T08:50:08.075036-05:00"}
{"id":"CodeContextBench-mk9","title":"Create ClaudeCodeSourcegraphMCPAgent for Harbor","description":"Implement claude_code_sg_mcp_agent.py with Sourcegraph MCP tools enabled. Requires SRC_ACCESS_TOKEN. Treatment condition for A/B testing.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.420936-05:00","updated_at":"2025-12-17T15:56:26.950123-05:00","closed_at":"2025-12-17T15:56:26.950123-05:00","dependencies":[{"issue_id":"CodeContextBench-mk9","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.421346-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-mw8","title":"Implement tool_profiles.py for MCP tool configuration","description":"Create tool_profiles.py to manage MCP tool definitions, capabilities, and configuration. Support tool registration, validation, and runtime configuration. Integrate with agents for tool availability at execution time.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:49.18927-05:00","updated_at":"2025-12-17T19:40:49.18927-05:00"}
{"id":"CodeContextBench-q89","title":"Port GitHub task mining infrastructure from sg_benchmark","description":"Copy task_mining/ with GitHub API query builder. Enable generating real-world tasks from OSS repos (Firefox, Kubernetes, etc.).","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T15:27:23.079179-05:00","updated_at":"2025-12-17T19:26:01.542374-05:00","closed_at":"2025-12-17T19:26:01.542374-05:00","dependencies":[{"issue_id":"CodeContextBench-q89","depends_on_id":"CodeContextBench-6vn","type":"blocks","created_at":"2025-12-17T15:27:23.079635-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-qxh","title":"Port infrastructure docs: PODMAN.md, docker-wrapper.sh, harbor-config.yaml","description":"Copy working Podman setup from sourcegraph-benchmarks. Include docker wrapper script and Harbor configuration.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:10.050801-05:00","updated_at":"2025-12-17T17:54:10.832954-05:00","closed_at":"2025-12-17T17:54:10.832954-05:00","dependencies":[{"issue_id":"CodeContextBench-qxh","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:10.051232-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-rqg","title":"Integrate NeMo-Agent-Toolkit for structured execution tracing","description":"NeMo-Agent-Toolkit provides structured instrumentation (per-call tracing, token counts, latency, success/failure). Refactor observability modules to consume NeMo traces instead of regex-parsing logs. Extract from NeMo: failed_tool_calls, token_usage, per_tool_latency, operation timeline.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T16:59:09.347198-05:00","updated_at":"2025-12-17T17:42:54.46463-05:00","closed_at":"2025-12-17T17:42:54.46463-05:00","dependencies":[{"issue_id":"CodeContextBench-rqg","depends_on_id":"CodeContextBench-4re","type":"discovered-from","created_at":"2025-12-17T16:59:09.347861-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-swl","title":"Phase 1: Create CodeContextBench directory skeleton","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:40.440777-05:00","updated_at":"2025-12-17T15:31:50.10868-05:00","closed_at":"2025-12-17T15:31:50.10868-05:00"}
{"id":"CodeContextBench-uzn","title":"Port 10Figure task generator and Harbor integration","description":"Copy gen_harbor_tasks.py, test.sh.j2 template, and Harbor dataset configs from harbor-10figure-dataset. Create benchmarks/10figure/ structure.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.302054-05:00","updated_at":"2025-12-17T19:26:01.643071-05:00","closed_at":"2025-12-17T19:26:01.643071-05:00","dependencies":[{"issue_id":"CodeContextBench-uzn","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.302463-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-von","title":"Analyze benchmark results \u0026 generate comparative report","description":"Execute Phase 2c: Aggregate benchmark results \u0026 test hypothesis per MINING_PLAN.md Phase 2c. Hypothesis: Sourcegraph code search improves agent success on multi-file tasks. Aggregate metrics (success rate, efficiency, cost) across task categories, difficulty, language. Stratified analysis revealing which task types benefit most from search. Generate HTML/JSON comparative report. Validate H1: +MCP success \u003e baseline (+10-15% expected).","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:47.139209-05:00","updated_at":"2025-12-17T19:45:49.021383-05:00","dependencies":[{"issue_id":"CodeContextBench-von","depends_on_id":"CodeContextBench-cy6","type":"discovered-from","created_at":"2025-12-17T19:40:47.140584-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-wkb","title":"Mine 6 additional OSS repos \u0026 generate 100+ Harbor tasks","description":"Execute Phase 2a: Mine 6 repos (firefox, pytorch, vscode, ffmpeg, tensorrt_llm, servo) per MINING_PLAN.md. Generate 50-75 high-quality Harbor tasks. See history/MINING_PLAN.md for master strategy (MINING_STRATEGY.md for detailed pipeline, RESEARCH_ALIGNMENT.md for paper alignment). Requirements: multi-file (≥2 files), deterministic verification, real GitHub work. Success: ≥80% validation pass, 5+ language coverage, balanced difficulty distribution.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:40:42.708461-05:00","updated_at":"2025-12-17T20:02:18.30501-05:00","closed_at":"2025-12-17T20:02:18.30501-05:00"}
