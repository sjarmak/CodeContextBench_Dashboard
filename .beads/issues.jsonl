{"id":"CodeContextBench-09h","title":"Fix Harbor framework installation for reproducible benchmarking","description":"Harbor CLI 0.3.0 (pip package) is broken: typer incompatibility, unmaintained. Blocks reproducibility of benchmark results. Solution: Install official Harbor framework (harborai) from harborframework.com instead. This will enable: (1) real task execution with reproducible metrics, (2) shareable setup for others to replicate, (3) publication-ready results. Current direct_benchmark.py uses synthetic mocks. Plan: Install harborai package, validate agent integration, re-run 10-task pilot with real Harbor, document setup for reproducibility. See .beads/ccb-harbor-fix.md for implementation plan (4 hours, 1 session).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T20:42:34.874834-05:00","updated_at":"2025-12-19T12:27:59.111446-05:00","closed_at":"2025-12-19T12:27:59.111446-05:00"}
{"id":"CodeContextBench-0f3","title":"Complete DEVELOPMENT.md \u0026 documentation suite","description":"Update documentation suite to reflect mining strategy, benchmark execution, and enterprise codebase insights.\n\nFILES TO UPDATE/CREATE:\n- README.md: quick-start for running benchmarks with github_mined tasks\n- DEVELOPMENT.md: detailed mining pipeline, agent setup, benchmark execution\n- TROUBLESHOOTING.md: common issues from mining + Harbor execution\n- docs/BENCHMARK_DESIGN_GUIDE.md: integrate ENTERPRISE_CODEBASES.md insights\n- Reference MINING_PLAN.md as canonical source\n\nENTERPRISE INSIGHTS TO INTEGRATE:\n- Scale considerations (small to enterprise: 10k â†’ 2B LOC)\n- Developer productivity metrics (58% comprehension time, 35% navigation)\n- Monorepo vs multi-repo patterns\n- Real-world reference metrics (Google, Stripe, Uber, NVIDIA)\n- Context switching costs (23min recovery)\n- Onboarding challenges (months to productivity at scale)\n\nNEW FILE CREATED:\n- docs/ENTERPRISE_CODEBASES.md: comprehensive enterprise codebase characteristics","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:51.499071-05:00","updated_at":"2025-12-20T17:56:25.073962-05:00"}
{"id":"CodeContextBench-0ji","title":"Replace DI-Bench test.sh with Python-based dependency validation (no Docker-in-Docker)","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T10:21:47.710133-05:00","updated_at":"2025-12-20T10:25:20.192592-05:00","closed_at":"2025-12-20T10:25:20.192592-05:00"}
{"id":"CodeContextBench-13j","title":"Design new benchmark: Process quality metrics (tool usage patterns)","description":"Create benchmark evaluating HOW agents work, capturing process quality beyond binary pass/fail.\n\nMETRICS TO CAPTURE:\n- MCP: Deep search patterns, retrieval efficiency, token usage\n- Baseline: Context utilization, exploration patterns\n- Tool effectiveness: Search query refinement, navigation paths\n\nDEVELOPER TIME ALLOCATION (Real-world data):\n- 58% on code reading/comprehension\n- 35% on navigation/search\n- 19% on external documentation\n- 23min to regain focus after interrupt\n\nEVALUATE:\n- Code search frequency and success rate\n- Time to locate relevant code\n- Number of files read before understanding\n- Search query evolution (refinement patterns)\n- Context switch recovery efficiency\n\nReference: Google's internal code search metrics (thousands of daily queries), Stripe productivity surveys, context switching research.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T13:49:28.836717-05:00","updated_at":"2025-12-20T17:56:10.594607-05:00"}
{"id":"CodeContextBench-1md","title":"Optimize MCP agent: Increase Deep Search usage and reduce token overhead","description":"MCP agent underutilizes Deep Search (only 11 calls in 1248s). Despite having MCP available, agent still uses 36% fewer tool calls but INCREASES token usage (1.16x). Analysis: (1) Review system prompt - encourage Deep Search usage; (2) Optimize context window sizes; (3) Reduce over-reasoning on complex tasks (sgt-009 was 3x slower with MCP). Goal: Achieve speedup without token overhead.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-19T17:34:30.159726-05:00","updated_at":"2025-12-19T17:34:33.279607-05:00","dependencies":[{"issue_id":"CodeContextBench-1md","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:30.160185-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-1tn","title":"Create RepoQA adapter in ~/harbor/adapters/repoqa/","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T10:47:15.807183-05:00","updated_at":"2025-12-20T10:53:41.015287-05:00","closed_at":"2025-12-20T10:53:41.015287-05:00","dependencies":[{"issue_id":"CodeContextBench-1tn","depends_on_id":"CodeContextBench-9sn","type":"discovered-from","created_at":"2025-12-20T10:47:15.807916-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-1v2","title":"Experiment: Wide-impact commit simulation (100+ files)","description":"Test agent performance on wide-impact changes like Uber's 100+ service commits:\n\nEXPERIMENT DESIGN:\n- Create tasks requiring changes to 10, 50, 100+ files\n- Examples: Library version upgrade, API signature change, config migration\n- Test completeness and consistency\n\nREFERENCE (ENTERPRISE_CODEBASES.md):\n- Uber: 1.4% of commits touch 100+ services\n- 0.3% touch 1000+ services\n- Common in monorepo patterns\n\nTASKS TO CREATE:\n1. Upgrade shared library across 50 consumers\n2. Rename function called in 100+ places\n3. Config format migration (100+ config files)\n4. Deprecate old API (update 75+ call sites)\n\nMETRICS:\n- Completeness: % of required changes found\n- Correctness: Are changes compatible?\n- Consistency: Are patterns uniform?\n- Efficiency: How many search iterations?\n\nEVALUATION:\n- Search for old patterns: should return zero\n- All tests pass after changes\n- No regressions introduced\n\nHYPOTHESIS:\n- MCP: Systematic search finds all occurrences\n- Baseline: Misses scattered references\n\nDELIVERABLE:\n- Wide-impact task dataset\n- Completeness analysis\n- Best practices for systematic changes","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T18:16:28.971447-05:00","updated_at":"2025-12-20T18:16:28.971447-05:00","dependencies":[{"issue_id":"CodeContextBench-1v2","depends_on_id":"CodeContextBench-zyq","type":"blocks","created_at":"2025-12-20T18:16:40.769746-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-1wi","title":"Update benchmark design docs with enterprise codebase insights","description":"Update docs/BENCHMARK_DESIGN_GUIDE.md and related docs to reference ENTERPRISE_CODEBASES.md. Integrate insights: 58% time on comprehension, 35% on navigation, 23min context switch cost, monorepo vs multi-repo patterns, scale considerations (Google 2B LOC â†’ small projects). Add recommendations section citing real-world metrics.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T17:55:46.096266-05:00","updated_at":"2025-12-20T17:55:46.096266-05:00"}
{"id":"CodeContextBench-1x7","title":"Single-task direct comparison: claude-code vs claude-code-mcp with real test validation and streaming output","description":"Run ONE task with both agents. Requirements:\n1. Use Claude streaming JSON output (not --output-format json flag, actual streaming API)\n2. System prompt: explicitly state non-interactive, MUST complete task, no placeholders\n3. Capture full multi-turn conversation (all 44 turns if applicable)\n4. Real test validation (task has actual test command, not empty make test)\n5. Show actual code changes (git diff must have content)\n6. Capture Deep Search queries and responses (for MCP agent)\n7. Output: complete trace JSON with reasoning, queries, results\n\nCompare:\n- Baseline: claude-code (no MCP)\n- MCP: claude-code-sourcegraph-mcp (with Deep Search)\n\nGoal: Clear evidence of whether MCP actually helps or if Phase 3 results were measurement error.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T08:50:01.112183-05:00","updated_at":"2025-12-18T19:03:46.434571-05:00","closed_at":"2025-12-18T19:03:46.434571-05:00"}
{"id":"CodeContextBench-2bq","title":"Experiment: Documentation gap impact on task success","description":"Test how missing/poor documentation affects agent performance:\n\nMOTIVATION (from ENTERPRISE_CODEBASES.md):\n- Google onboarding study: poor docs = top hindrance\n- Real codebases: docs often outdated or missing\n- Agents rely heavily on documentation\n\nEXPERIMENT DESIGN:\n- Same codebase, three documentation conditions:\n  1. Excellent: Complete, up-to-date docs\n  2. Poor: Outdated, incomplete docs\n  3. None: No documentation\n\nTASKS:\n- Onboarding tasks (understand unfamiliar code)\n- Feature implementation (following existing patterns)\n- Bug fixing (requires understanding intent)\n\nHYPOTHESIS:\n- MCP: Can compensate for poor docs via code search\n- Baseline: Heavily dependent on documentation quality\n- Success gap widens with doc quality\n\nMETRICS:\n- Task success rate by doc condition\n- Documentation consultation frequency\n- Code search patterns\n- Time to comprehension\n\nIMPLEMENTATION:\n- Select 3 repos with varying doc quality\n- OR: artificially degrade docs in test repo\n- Run same tasks across conditions\n\nDELIVERABLE:\n- Documentation impact analysis\n- MCP value prop: 'Code as documentation'\n- Recommendations for doc-poor codebases\n\nREFERENCE: 19% of developer time on external documentation","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T18:16:29.229505-05:00","updated_at":"2025-12-20T18:16:29.229505-05:00"}
{"id":"CodeContextBench-2cv","title":"Industry validation: Partner with company for benchmark validation","description":"Establish partnership with tech company for benchmark validation (without exposing proprietary code).\n\nPOTENTIAL PARTNERS:\n- Companies with open engineering blogs (Stripe, Uber, Netflix)\n- Companies investing in AI tooling (Stripe: 65-70% using AI assistants)\n- Academic partnership programs (Meta, Google, Microsoft)\n\nVALIDATION APPROACH:\n1. Company runs benchmark internally on proprietary codebase\n2. Share aggregated metrics only (no code exposure)\n3. Validate against their internal productivity metrics\n4. Compare tool performance to their internal tools\n\nREFERENCE METRICS:\n- Build time improvements\n- Developer productivity survey results\n- Onboarding time reduction\n- Code search success rates\n\nOUTCOME:\n- Industry-validated benchmark\n- Real-world performance baselines\n- Credibility for research publication\n- Feedback loop for benchmark improvement","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-20T17:56:48.985828-05:00","updated_at":"2025-12-20T17:56:48.985828-05:00","dependencies":[{"issue_id":"CodeContextBench-2cv","depends_on_id":"CodeContextBench-jdm","type":"blocks","created_at":"2025-12-20T17:58:21.046861-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-2cv","depends_on_id":"CodeContextBench-qp1","type":"blocks","created_at":"2025-12-20T17:58:21.577651-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-2cv","depends_on_id":"CodeContextBench-xiz","type":"blocks","created_at":"2025-12-20T17:58:21.777855-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-2il","title":"Re-run 50-task benchmark with corrected comparison script","description":"Use the fixed run_10task_comparison.sh with timestamped directories and validation script. Expand to 50-task benchmark to get statistically significant results. Use validate_comparison_results.py to verify data integrity before analysis. Real 10-task data showed only 1.05x speedup with 1.16x higher token cost - need larger sample to confirm pattern.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T17:34:24.856229-05:00","updated_at":"2025-12-19T17:34:28.149379-05:00","dependencies":[{"issue_id":"CodeContextBench-2il","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:24.856716-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-2pw","title":"Comparative analysis: CodeContextBench vs AgentCompany/EnterpriseBench","description":"Document how CodeContextBench compares to similar benchmarks:\n\nBENCHMARKS TO COMPARE:\n1. AgentCompany: Simulates small software company with interrelated tasks\n2. EnterpriseBench: Synthesizes enterprise data (repos, tickets, wikis, org charts)\n3. SWE-Bench: GitHub issue resolution\n4. DevBench: Software engineering task completion\n\nANALYSIS DIMENSIONS:\n- Task diversity and realism\n- Scale (codebase size, task complexity)\n- Process quality metrics vs outcome metrics\n- Tool usage evaluation depth\n- Enterprise workflow fidelity\n\nOUTCOME:\n- Positioning document: What makes CodeContextBench unique?\n- Gap analysis: What we cover that others don't\n- Integration opportunities: Where we can complement existing benchmarks\n- Research paper material","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-20T17:56:48.777626-05:00","updated_at":"2025-12-20T17:56:48.777626-05:00","dependencies":[{"issue_id":"CodeContextBench-2pw","depends_on_id":"CodeContextBench-1wi","type":"blocks","created_at":"2025-12-20T17:58:21.978005-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-2s1","title":"Integrate existing enterprise-scale benchmarks: DeathStarBench, SWE-Bench","description":"Integrate or adapt existing benchmarks for CodeContextBench:\n\nBENCHMARKS TO INTEGRATE:\n1. DeathStarBench: Realistic microservice applications (e-commerce, media) with service meshes\n2. d'Aragona dataset: 378 open-source microservice projects with labeled architectures\n3. SWE-Bench: GitHub issues and bug reports from large projects\n4. DevBench: Software engineering tasks on real-world code\n\nAPPROACH:\n- Use DeathStarBench for multi-service scenarios\n- Mine d'Aragona projects for microservice dependency patterns\n- Adapt SWE-Bench tasks for tool comparison (baseline vs MCP)\n- Create hybrid scenarios combining multiple benchmarks\n\nOUTCOME:\n- Validated against existing research standards\n- Comparable to academic benchmarks\n- Real architectural complexity from open-source","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-20T17:56:48.568885-05:00","updated_at":"2025-12-20T17:56:48.568885-05:00"}
{"id":"CodeContextBench-2wz","title":"Design new benchmark: Diverse task types validation","description":"Create benchmark mixing diverse task types to validate genuine codebase understanding vs task-type expertise.\n\nTASK TYPES (from enterprise workflows):\n1. Bug fixing across multiple services\n2. Feature implementation with cross-cutting changes\n3. Refactoring and tech debt reduction\n4. Performance optimization\n5. Code review and comprehension\n6. Documentation and knowledge sharing\n7. Onboarding and learning new subsystems\n\nREQUIREMENTS:\n- Equal file access (both agents pre-cloned repos)\n- Mix of monorepo and multi-repo scenarios\n- Tasks requiring cross-service coordination\n- Wide-impact changes (affecting 100+ modules, like Uber commits)\n\nEVALUATION CRITERIA:\n- Correctness across task types\n- Exploration efficiency by task type\n- Tool usage patterns (search, navigation, build/test)\n- Time to comprehension\n\nReference: Uber's wide-impact commits (1.4% touch 100+ services), enterprise task diversity research.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T13:49:28.678841-05:00","updated_at":"2025-12-20T17:56:16.088712-05:00"}
{"id":"CodeContextBench-34b","title":"Run full 10-task MCP pilot and compare with baseline (80% success)","description":"DATA FIXED: All 50 main set tasks now properly configured with real commits (pre_fix_rev and ground_truth_rev from mining results). Test scripts corrected to compare against pre_fix_rev instead of HEAD. Ready for 10-task baseline vs MCP comparison on properly configured mined tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T07:20:56.902137-05:00","updated_at":"2025-12-19T14:39:21.007706-05:00","closed_at":"2025-12-19T14:39:21.007706-05:00"}
{"id":"CodeContextBench-3js","title":"Phase 3 Results Invalid: Tests are fake (make test undefined), code_changes empty, no Deep Search visible","description":"","status":"open","priority":1,"issue_type":"bug","created_at":"2025-12-18T08:49:56.738041-05:00","updated_at":"2025-12-18T08:49:56.738041-05:00"}
{"id":"CodeContextBench-4m5","title":"Run 10-task baseline vs MCP comparison on SWE-Bench (real tasks, proven infrastructure)","description":"Run baseline (Claude Code) and MCP (Claude+Sourcegraph) agents on 10 SWE-Bench tasks. SWE-Bench is fully configured in Harbor at /Users/sjarmak/harbor/adapters/swebench/. This validates MCP improves agent performance on real software engineering benchmarks before committing to custom mining infrastructure. Compare metrics: task completion rate, tokens used, execution time.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T14:16:44.126037-05:00","updated_at":"2025-12-20T18:02:16.739686-05:00","closed_at":"2025-12-20T18:02:16.739686-05:00"}
{"id":"CodeContextBench-4nq","title":"Build context switching simulation framework","description":"Create framework simulating developer interruptions and context switches:\n\nSIMULATION APPROACH:\n- Agent starts task A (partial completion)\n- Forced context switch: present unrelated task B\n- Agent completes task B\n- Agent returns to complete task A\n\nMETRICS (23min recovery reference):\n- Time to resume task A effectively\n- Redundant work (re-reading files)\n- Quality of notes/documentation left\n- Success rate on task A after switch\n\nSCENARIOS:\n1. Simple switch: similar tasks, same codebase\n2. Hard switch: different codebases, different domains\n3. Multi-switch: A â†’ B â†’ C â†’ back to A\n4. Interrupted during critical reasoning\n\nHYPOTHESIS:\n- MCP tools help recover context faster (via search)\n- Baseline relies more on conversation history\n- Documentation quality matters more for baseline\n\nREFERENCE: ENTERPRISE_CODEBASES.md - 23min focus recovery time","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T18:15:23.140435-05:00","updated_at":"2025-12-20T18:15:23.140435-05:00","dependencies":[{"issue_id":"CodeContextBench-4nq","depends_on_id":"CodeContextBench-zez","type":"blocks","created_at":"2025-12-20T18:15:36.059969-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-4re","title":"Implement simple observability: manifest_writer.py and metrics_collector.py","description":"Replace NeMo with lightweight JSON manifests. Write run_manifest.json with harness, tool_profile, result, retrieval_metrics. Parse Harbor logs for tool usage.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:09.976846-05:00","updated_at":"2025-12-17T16:52:42.199699-05:00","closed_at":"2025-12-17T16:52:42.199699-05:00","dependencies":[{"issue_id":"CodeContextBench-4re","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:09.977265-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-4vb","title":"Write enterprise benchmarking research paper","description":"Prepare research paper on enterprise-informed AI agent benchmarking:\n\nPAPER STRUCTURE:\n1. Introduction\n   - Gap: existing benchmarks miss enterprise realities\n   - Contribution: enterprise-informed metrics and tasks\n2. Background\n   - Enterprise codebase characteristics (ENTERPRISE_CODEBASES.md)\n   - Developer productivity challenges\n   - Prior benchmarks (SWE-Bench, AgentCompany, etc.)\n3. Methodology\n   - Task design principles\n   - Metric selection rationale\n   - Evaluation approach\n4. Benchmarks\n   - Scale testing (10kâ†’1M LOC)\n   - Monorepo vs multi-repo\n   - Process quality metrics\n   - Context switching, onboarding, wide-impact\n5. Experiments\n   - Baseline vs MCP results\n   - Enterprise metric findings\n   - Task type performance matrix\n6. Analysis\n   - Where tools matter most\n   - Scale crossover points\n   - ROI modeling\n7. Discussion\n   - Implications for tool design\n   - Industry validation\n   - Limitations and future work\n8. Conclusion\n   - Enterprise benchmarking best practices\n   - Open datasets and reproducibility\n\nTARGET VENUES:\n- ICSE (International Conference on Software Engineering)\n- FSE (Foundations of Software Engineering)\n- ASE (Automated Software Engineering)\n- EMSE (Empirical Software Engineering and Measurement)\n\nDEPENDENCIES: All experiments complete, results analyzed\n\nDELIVERABLE:\n- Full research paper draft\n- Supplementary materials\n- Dataset release plan","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-20T18:16:29.732799-05:00","updated_at":"2025-12-20T18:16:29.732799-05:00","dependencies":[{"issue_id":"CodeContextBench-4vb","depends_on_id":"CodeContextBench-b4m","type":"blocks","created_at":"2025-12-20T18:16:41.204248-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-4vb","depends_on_id":"CodeContextBench-lxl","type":"blocks","created_at":"2025-12-20T18:16:41.417165-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-4vb","depends_on_id":"CodeContextBench-1v2","type":"blocks","created_at":"2025-12-20T18:16:41.62989-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-4vb","depends_on_id":"CodeContextBench-vqr","type":"blocks","created_at":"2025-12-20T18:16:41.84664-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-6f2","title":"Apply autonomous environment variables to baseline Claude Code agent","description":"DISCOVERY: MCP agent successfully made code changes (2 lines detected) despite failing the test due to environment setup. This proves the autonomous environment variables (FORCE_AUTO_BACKGROUND_TASKS=1, ENABLE_BACKGROUND_TASKS=1) injected in create_run_agent_commands() are working correctly.\n\nOBSERVATION: Baseline agent made 0 changes, while MCP agent made changes. The only difference is:\n- MCP: Uses ClaudeCodeSourcegraphMCPAgent which injects autonomous env vars\n- Baseline: Uses built-in harbor claude-code agent without these env vars\n\nSOLUTION: Create a baseline-compatible agent (or extend built-in) that also injects the autonomous env vars without the MCP overhead. This allows fair comparison:\n- BaselineClaudeCodeAgent: Injects autonomous vars, no MCP\n- ClaudeCodeSourcegraphMCPAgent: Injects autonomous vars + MCP tools\n\nIMPLEMENTATION:\n1. Create agents/claude_baseline_agent.py extending ClaudeCode\n2. Inject FORCE_AUTO_BACKGROUND_TASKS=1 and ENABLE_BACKGROUND_TASKS=1 in create_run_agent_commands()\n3. Run baseline tests using: --agent-import-path agents.claude_baseline_agent:BaselineClaudeCodeAgent\n4. This enables fair Phase 4 comparison: Baseline vs MCP on equal footing\n\nThis is critical for valid benchmarking - both agents must have access to autonomous operation mode.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:07:29.215662-05:00","updated_at":"2025-12-19T10:08:07.827608-05:00","closed_at":"2025-12-19T10:08:07.827608-05:00"}
{"id":"CodeContextBench-6vn","title":"Port task_schema.py from sg_benchmark","description":"Copy src/task_schema.py with JSON schema validation and TaskSpecification dataclass. This becomes the canonical validator for all tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.133496-05:00","updated_at":"2025-12-17T16:14:40.13573-05:00","closed_at":"2025-12-17T16:14:40.13573-05:00","dependencies":[{"issue_id":"CodeContextBench-6vn","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.13406-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-6wi","title":"Redesign big_code_mcp: Use pre-cloned repos for equal file access","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:49:09.389094-05:00","updated_at":"2025-12-20T21:12:04.243406-05:00","closed_at":"2025-12-20T21:12:04.243406-05:00"}
{"id":"CodeContextBench-704","title":"Analyze baseline vs MCP results, validate hypothesis, generate report","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T07:20:59.005639-05:00","updated_at":"2025-12-19T16:27:04.987168-05:00","closed_at":"2025-12-19T16:27:04.987168-05:00","dependencies":[{"issue_id":"CodeContextBench-704","depends_on_id":"CodeContextBench-34b","type":"discovered-from","created_at":"2025-12-18T07:20:59.008337-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-78b","title":"Phase 3: Set up DependEval and DI-Bench adapters for complete benchmark suite","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T13:42:49.582397-05:00","updated_at":"2025-12-20T13:42:49.582397-05:00","dependencies":[{"issue_id":"CodeContextBench-78b","depends_on_id":"CodeContextBench-9sn","type":"discovered-from","created_at":"2025-12-20T13:42:49.582819-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-7qs","title":"Update ARCHITECTURE.md and AGENTS.md for unified CodeContextBench","description":"Consolidate docs from sg_benchmark and sourcegraph-benchmarks. Make Claude-first, MCP-first. Keep Amp as optional/legacy.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:23.002458-05:00","updated_at":"2025-12-17T17:55:26.998927-05:00","closed_at":"2025-12-17T17:55:26.998927-05:00"}
{"id":"CodeContextBench-7sp","title":"Add thread safety when calling ncclCommGetAsyncError","description":"","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-19T08:19:33.714087-05:00","updated_at":"2025-12-19T08:22:17.725184-05:00","closed_at":"2025-12-19T08:22:17.725184-05:00"}
{"id":"CodeContextBench-7tn","title":"Validate Harbor + Daytona + Claude Code + Sourcegraph MCP integration","description":"Validate Harbor + Daytona + Claude Code integration. Confirmed baseline works with 6M+ cached tokens. MCP pattern: use --agent claude-code with task Dockerfile calling sourcegraph_mcp_setup.sh.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-18T11:46:16.014003-05:00","updated_at":"2025-12-18T11:49:20.557685-05:00","closed_at":"2025-12-18T11:49:20.557685-05:00"}
{"id":"CodeContextBench-7wq","title":"Fix task Dockerfiles to have real test commands (not empty make test)","description":"Current tasks have 'make test' with no target defined. Need to:\n1. Audit each task: does it have actual test validation?\n2. Update task Dockerfiles to run real tests\n3. Examples: pytest, cargo test, npm test, make check\n4. Verify tests actually fail if code is wrong\n5. Re-run Phase 3 with real validation\n\nWithout this, we can't know if 100% success is real or measurement error.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-18T08:50:04.268838-05:00","updated_at":"2025-12-18T08:50:04.268838-05:00"}
{"id":"CodeContextBench-7xz","title":"Port BasePatchAgent from sourcegraph-benchmarks to agents/base.py","description":"Copy BasePatchAgent from amp_agent.py, generalize for any CLI agent (remove Amp-specific logic). Keep Harbor integration and repo discovery.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.257834-05:00","updated_at":"2025-12-17T15:52:31.770754-05:00","closed_at":"2025-12-17T15:52:31.770754-05:00","dependencies":[{"issue_id":"CodeContextBench-7xz","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.258373-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-81d","title":"Port Harbor runners and benchmark scripts","description":"Migrate run-full-benchmark.sh â†’ harbor_benchmark.sh, compare-harbor-results.py â†’ compare_results.py. Generalize to accept --agent and --benchmark flags.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.224614-05:00","updated_at":"2025-12-17T16:17:07.325137-05:00","closed_at":"2025-12-17T16:17:07.325137-05:00","dependencies":[{"issue_id":"CodeContextBench-81d","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.225009-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-820","title":"Implement tool_profiles.py for MCP tool configuration","description":"Define none, search_only, code_intel, deep_search profiles. Configure which MCP tools are exposed per profile.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T15:27:23.153115-05:00","updated_at":"2025-12-17T18:36:10.118045-05:00","closed_at":"2025-12-17T18:36:10.118045-05:00","dependencies":[{"issue_id":"CodeContextBench-820","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:23.153528-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-848","title":"Run 4-task 10Figure smoke test: Claude baseline vs Claude+MCP","description":"End-to-end validation: run 4 tasks (cross_file, refactor, api_upgrade, bug_localization) with both agent conditions. Compare results and validate hypothesis.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:22.922545-05:00","updated_at":"2025-12-17T16:32:16.244379-05:00","closed_at":"2025-12-17T16:32:16.244379-05:00","dependencies":[{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-7xz","type":"blocks","created_at":"2025-12-17T15:27:22.922992-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-ews","type":"blocks","created_at":"2025-12-17T15:27:22.92344-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-mk9","type":"blocks","created_at":"2025-12-17T15:27:22.923923-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-81d","type":"blocks","created_at":"2025-12-17T15:27:22.924285-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-848","depends_on_id":"CodeContextBench-uzn","type":"blocks","created_at":"2025-12-17T15:27:22.92461-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-8dx","title":"Phase 2 Complete: Fix RepoQA verifier test.sh path issues and verify end-to-end scoring","description":"Phase 2 Complete: Fixed RepoQA verifier test.sh path issues\n\nCOMPLETED:\n- Fixed test.sh to look for /app/solution.json (shared mount between agent \u0026 verifier)\n- Updated instruction_sr-qa.md to write to /app/ instead of /logs/verifier/\n- Added debug output to test.sh for diagnosing missing files\n- Verified verifier logic works: scores 1.0 for correct answers, 0.0 for wrong\n- Updated AGENTS.md with container architecture documentation\n\nVERIFIED WORKING:\n- Ground truth loading: /tests/ground_truth.json found correctly\n- Verifier test.sh creates reward.json successfully\n- Harbor collects verifier output correctly\n- Verifier scoring logic produces correct scores (tested with manual solutions)\n\nREMAINING BLOCKER:\n- Docker container runs out of disk space during repo clone in Dockerfile\n- This prevents agent execution, but paths are now fixed for when it works\n\nStatus: Ready for next phase once Docker disk space resolved.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:42:46.97736-05:00","updated_at":"2025-12-20T18:34:33.950881-05:00","closed_at":"2025-12-20T18:34:33.950881-05:00","dependencies":[{"issue_id":"CodeContextBench-8dx","depends_on_id":"CodeContextBench-9sn","type":"discovered-from","created_at":"2025-12-20T13:42:46.978211-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-8tz","title":"Generate DependEval benchmark tasks for 150 indexed repos (DR, RC, ME)","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T12:44:47.332064-05:00","updated_at":"2025-12-20T12:53:51.335443-05:00","closed_at":"2025-12-20T12:53:51.335443-05:00"}
{"id":"CodeContextBench-9sn","title":"Set up adapters for DI-Bench, RepoQA, DependEval + baseline/MCP comparison pipeline","description":"Set up Harbor adapters for three core benchmarks: DI-Bench (dependency reasoning), RepoQA (semantic navigation), DependEval (scale+hierarchy). Study /Users/sjarmak/harbor source to understand adapter architecture. Create comparison pipeline: Claude baseline vs Claude+Sourcegraph MCP. Metrics to capture: code quality/accuracy, token use/cost, execution time, dependency reasoning quality. Goal: Simple direct comparison showing MCP value.","status":"in_progress","priority":1,"issue_type":"task","created_at":"2025-12-19T13:36:22.114-05:00","updated_at":"2025-12-20T12:40:06.999051-05:00","comments":[{"id":1,"issue_id":"CodeContextBench-9sn","author":"sjarmak","text":"REPOQA ADAPTER COMPLETE (Phase 1):\n\nâœ… COMPLETED:\n- commit_validator.py: Validates full SHA-1 hashes before task generation\n- benchmarks/repoqa_instances_validated.jsonl: 5 verified instances from requests repo\n- benchmarks/repoqa_validated_tasks/: 3 test SR-QA tasks generated\n- benchmarks/repoqa_repos_for_indexing.txt: 28 repos for Sourcegraph indexing\n- AGENTS.md: RepoQA benchmark execution guide (+71 lines)\n- ~/harbor/adapters/repoqa/run_adapter.py: Integration with validation\n\nâœ… VERIFIED:\n- 5/5 commits validated successfully\n- 3/3 test tasks generated without errors\n- All Dockerfiles contain valid git checkout commands\n- Validator integrates with adapter CLI\n\nðŸ“‹ COMMITS:\n- 58b5d85a: Add commit hash validation to RepoQA adapter\n- 19a98748: Add RepoQA repos for Sourcegraph indexing\n\nðŸš€ NEXT (Phase 2):\n- Run baseline vs MCP comparison on repoqa_validated_tasks\n- Validate end-to-end pipeline: bash scripts/run_mcp_comparison.sh repoqa_validated_tasks requests-001-sr-qa\n- Collect metrics: token usage, accuracy, reasoning quality\n- Once validated, expand dataset with instances from numpy, pandas, tensorflow, django, flask\n\nðŸ“Š PROGRESS: 1/3 adapters complete (RepoQA). DI-Bench and DependEval still needed.","created_at":"2025-12-20T17:40:13Z"}]}
{"id":"CodeContextBench-a2g","title":"Create infrastructure/datasets.yaml for 10Figure corpus","description":"Define external dataset contract: harbor-10figure:base image, TEN_FIGURE_CODEBASES_PATH env var. Document in docs/10FIGURE.md.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:09.901-05:00","updated_at":"2025-12-17T16:44:07.714566-05:00","closed_at":"2025-12-17T16:44:07.714566-05:00","dependencies":[{"issue_id":"CodeContextBench-a2g","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:09.901469-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-aeg","title":"Create Harbor adapter for DI-Bench","description":"Created a complete Harbor adapter for Microsoft's DI-Bench (Dependency Inference Benchmark).\n\n## What was accomplished:\n\n### Core Adapter Implementation\n- **adapter.py** (319 lines): Main adapter with DIBenchInstance data model, DIBenchLoader for JSONL datasets, and DIBenchAdapter for task conversion\n- **run_adapter.py** (147 lines): CLI tool with filtering by language, instance ID, and limits\n- **__init__.py**: Package initialization\n\n### Template System\n- **instruction.md**: Task instructions template with environment specs and output format requirements\n- **task.toml**: Harbor configuration with metadata, timeouts, and resource limits\n- **Dockerfile**: Multi-language environment supporting Python, Rust, C#, JavaScript, Docker-in-Docker for act (GitHub Actions runner)\n- **solve.sh**: Reference solution script that applies dependency configuration patches\n- **test.sh**: Test execution using act to run CI/CD pipelines\n\n### Documentation\n- **README.md**: Complete documentation with prerequisites, usage examples, troubleshooting\n- **QUICKSTART.md**: Step-by-step getting started guide\n- **INTEGRATION.md**: Technical integration guide for Harbor framework\n- **dibench.yaml**: Adapter configuration and metadata\n- **parity_experiments.json**: Validation test cases\n\n## Technical Details:\n\n### Multi-Language Support\n- Python 3.10+ with pip\n- Node.js 18+ with npm\n- Rust (latest stable via rustup)\n- .NET 7.0 for C#\n- GitHub Actions execution via act\n\n### Adapter Features\n- Loads DI-Bench instances from JSONL dataset files\n- Converts repository instances to Harbor task format\n- Preserves metadata and environment specifications\n- Template-based task generation with placeholder replacement\n- Harbor-compatible evaluation (reward 0/1 based on test results)\n\n### File Structure\nCreated 13 files across 5 directories following Harbor adapter patterns from SWE-Bench, LiveCodeBench, and DevEval adapters.\n\n## Integration Ready\nThe adapter is ready for integration into Harbor's dataset registry and can process all DI-Bench instances across Python, Rust, C#, and JavaScript languages.\n\nLocation: /Users/sjarmak/harbor/adapters/dibench/","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-19T14:55:13.832117-05:00","updated_at":"2025-12-19T14:55:29.784104-05:00","closed_at":"2025-12-19T14:55:29.784104-05:00"}
{"id":"CodeContextBench-ax7","title":"Test baseline and MCP agents on SWE-Bench (real benchmark)","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:32:28.834155-05:00","updated_at":"2025-12-19T13:36:19.582412-05:00","closed_at":"2025-12-19T13:36:19.582412-05:00"}
{"id":"CodeContextBench-az9","title":"Investigate sgt-003 MCP failure: Missing from comparison results","description":"MCP run for sgt-003 is completely absent from mcp-10task-20251219/ directory. Baseline completed this task in 280s with 214 steps. Investigate: timeout, API failure, crash during MCP init, or incomplete run. This is a 10% task failure rate for MCP agent.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T17:34:19.926778-05:00","updated_at":"2025-12-20T18:03:43.498341-05:00","closed_at":"2025-12-20T18:03:43.498341-05:00","dependencies":[{"issue_id":"CodeContextBench-az9","depends_on_id":"CodeContextBench-dge","type":"discovered-from","created_at":"2025-12-19T17:34:19.927478-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-b4m","title":"Validate 58% comprehension time hypothesis in benchmark data","description":"Test if agents spend majority of time on comprehension vs generation:\n\nHYPOTHESIS (from ENTERPRISE_CODEBASES.md):\n- Real developers: 58% on comprehension, 35% on navigation, ~7% writing\n- Do AI agents show similar patterns?\n- Does MCP shift time allocation?\n\nMETRICS TO ANALYZE:\n- Time reading files vs writing code\n- Search/navigation actions vs edit actions\n- Re-reading patterns (comprehension indicators)\n- Time to first code change (comprehension complete?)\n\nEXPECTED FINDINGS:\n- Baseline: More trial-and-error, less comprehension\n- MCP: More upfront search/reading, better first attempts\n- Success correlation with comprehension time\n\nANALYSIS:\n- Segment task timeline: comprehension | planning | implementation | validation\n- Compare successful vs failed tasks\n- Identify optimal comprehension:implementation ratio\n\nDELIVERABLE:\n- Report: 'Do AI Agents Mirror Human Time Allocation?'\n- Recommendations for agent design\n- Validation of enterprise metrics relevance\n\nDEPENDENCIES: Metrics infrastructure, completed experiments","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-20T18:16:28.480592-05:00","updated_at":"2025-12-20T18:16:28.480592-05:00","dependencies":[{"issue_id":"CodeContextBench-b4m","depends_on_id":"CodeContextBench-trg","type":"blocks","created_at":"2025-12-20T18:16:40.344755-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-c65","title":"Implement Trevor Nederlof's exact big code MCP tasks","description":"Phase 2: Implement Trevor Nederlof's exact big code MCP tasks âœ… COMPLETED\n\nImplemented 4 validated big code tasks with proper naming (big-code-{codebase}-{id}):\n\nâœ… big-code-vsc-001: VS Code stale diagnostics after git branch switch\n   - Prompt: Understand diagnostics pipeline, fix stale errors on branch switch\n   - Repo: microsoft/vscode (1GB TypeScript)\n   - Why big code: Diagnostics distributed across modules (extension host, file watchers, problems panel)\n\nâœ… big-code-servo-001: Servo scrollend DOM event implementation  \n   - Prompt: Add scrollend event with debouncing across all scroll handlers\n   - Repo: servo/servo (1.6GB Rust)\n   - Why big code: Scroll handling scattered across browser/compositor/DOM event systems\n\nâœ… big-code-k8s-001: Kubernetes NoScheduleNoTraffic taint effect\n   - Prompt: Implement new taint effect, find all evaluation points\n   - Repo: kubernetes/kubernetes (1.4GB Go)\n   - Why big code: Taint effects evaluated in scheduler, admission controller, endpoint slices\n\nâœ… big-code-trt-001: TensorRT-LLM W4A8_MXFP4_INT8 quantization mode\n   - Prompt: Add new quantization mode following W4A8_MXFP4_FP8 pattern\n   - Repo: NVIDIA/TensorRT-LLM (1.6GB Python/C++)\n   - Why big code: Spans Python/C++ boundary, kernel selection, build system, tests\n\nEach task includes:\n- Trevor's exact prompt from research (docs/TREVOR_RESEARCH_DEC2025.md)\n- CLAUDE.md with MCP search guidance\n- Minimal Docker environment (git init, no full clone)\n- time_limit_sec and difficulty for baseline vs MCP comparison\n\nFramework ready: Run with scripts/run_mcp_comparison.sh big_code_mcp big-code-{codebase}-{id}\n\nReference: docs/TREVOR_RESEARCH_DEC2025.md","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T07:42:21.563592-05:00","updated_at":"2025-12-20T07:50:30.382324-05:00","closed_at":"2025-12-20T07:50:30.382324-05:00"}
{"id":"CodeContextBench-c74","title":"Build/test integration benchmarking: Feedback loop metrics","description":"Measure build/test cycle overhead in tool workflows. Track: number of build attempts, test runs, time to first passing test. Simulate slow CI (like 12hr integration tests), flaky tests (40% false positives). Test tool's ability to minimize cycles through better comprehension. Reference Uber's CI scale (10k+ monthly changes), PayPal flaky test stats.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:45.365569-05:00","updated_at":"2025-12-20T17:55:45.365569-05:00"}
{"id":"CodeContextBench-ch8","title":"Create reference implementations for benchmark tasks","description":"For each benchmark type, document ideal agent behavior: what searches, what code examined, minimal sufficient changes. Identify when agents get lucky vs understand genuinely. Future: use for trace analysis.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-20T13:49:29.143249-05:00","updated_at":"2025-12-20T13:49:29.143249-05:00"}
{"id":"CodeContextBench-cvk","title":"Create enterprise-scale test repository (1M+ LOC)","description":"Build large-scale test repository approaching enterprise size:\n\nCOMPOSITION:\n- Combine 15-20 substantial OSS projects\n- Target: 1M-5M LOC total\n- Mix: Python (40%), TypeScript (30%), Go (20%), other (10%)\n\nPROJECTS TO INCLUDE:\n- Web frameworks (Django, FastAPI, Express)\n- Data tools (pandas, numpy, scipy alternatives)\n- CLI tools (rich, click, cobra)\n- Infrastructure (Docker SDK, Kubernetes clients)\n- Testing frameworks\n\nSTRUCTURE:\n- Option A: True monorepo (all in one)\n- Option B: Multi-repo with shared libraries\n- Option C: Hybrid (like Uber's multi-monorepo)\n\nTASKS TO CREATE:\n1. Scale testing: Search performance at 1M+ LOC\n2. Cross-project dependencies\n3. Wide-impact refactors\n4. Performance at enterprise scale\n\nREFERENCE: Google 2B LOC, Stripe 20M LOC - this is 'medium enterprise'\n\nVALIDATION: Compare metrics at 10k, 100k, 1M, 5M LOC scales","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T18:15:23.893729-05:00","updated_at":"2025-12-20T18:15:23.893729-05:00"}
{"id":"CodeContextBench-cy6","title":"Run Harbor benchmarks on 10figure + github_mined tasks","description":"Execute Phase 2b Harbor benchmarks on github_mined (50 tasks) + 10figure (4 tasks). Pilot: 10 tasks both agents to validate infrastructure \u0026 calibrate timeouts. Full: 50+4 tasks both agents. Capture manifests, NeMo traces, tool usage. Success: \u003e90% tasks complete. Agents: claude-baseline (no search) vs claude-mcp (with Sourcegraph Deep Search). Expected: baseline 30-40%, MCP 40-55%, validates hypothesis. Ready for execution.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:40:44.9698-05:00","updated_at":"2025-12-17T20:26:35.898354-05:00","closed_at":"2025-12-17T20:26:35.898354-05:00","dependencies":[{"issue_id":"CodeContextBench-cy6","depends_on_id":"CodeContextBench-wkb","type":"discovered-from","created_at":"2025-12-17T19:40:44.97196-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-dge","title":"RESOLVED: Original 9.9x speedup was false - real data shows 1.1x, MCP uses MORE tokens","description":"Real original data (baseline-10task-20251219 \u0026 mcp-10task-20251219): Baseline avg 139.7s vs MCP 124.8s = 1.1x speedup. Baseline 34.8M tokens vs MCP 40.5M tokens = MCP is more expensive. sgt-003 missing from MCP. The 'comparison-20251219-clean' used for original report had API key failures masking real results.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-19T17:17:35.174134-05:00","updated_at":"2025-12-19T17:26:30.658829-05:00","closed_at":"2025-12-19T17:26:30.658829-05:00","dependencies":[{"issue_id":"CodeContextBench-dge","depends_on_id":"CodeContextBench-704","type":"discovered-from","created_at":"2025-12-19T17:17:35.175617-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-dkf","title":"Phase 3 Rerun: Big Code MCP with Proper Repo Setup","description":"","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-20T13:10:47.108096-05:00","updated_at":"2025-12-20T13:10:47.108096-05:00"}
{"id":"CodeContextBench-dv9","title":"Design and run onboarding simulation experiments","description":"Create benchmark simulating new developer onboarding scenarios:\n\nONBOARDING SCENARIOS (from ENTERPRISE_CODEBASES.md):\n1. Cold start: Unfamiliar codebase, no prior context\n2. Documentation gap: Poor/missing docs (like Google study)\n3. First contribution: Make small fix in unknown subsystem\n4. Codebase exploration: Answer questions about architecture\n\nTASKS:\n- 'Find where feature X is implemented'\n- 'Explain what module Y does'\n- 'Fix simple bug in unfamiliar code'\n- 'Add small feature following existing patterns'\n\nMETRICS:\n- Time to locate relevant code\n- Documentation consultation frequency\n- Code search patterns\n- Comprehension quality (via explanation tasks)\n\nCOMPARISON:\n- Baseline vs MCP on cold-start comprehension\n- Which tool helps 'learn' codebase faster?\n- Correlation between exploration and task success\n\nTARGET: Validate if MCP reduces months-long onboarding\n\nREFERENCE: Google's 3-6 week remote onboarding delay","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T18:15:23.393469-05:00","updated_at":"2025-12-20T18:15:23.393469-05:00","dependencies":[{"issue_id":"CodeContextBench-dv9","depends_on_id":"CodeContextBench-zez","type":"blocks","created_at":"2025-12-20T18:15:36.272155-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-eaf","title":"Analyze existing results through enterprise metrics lens","description":"Re-analyze Phase 3 results using enterprise-informed metrics:\n\nEXISTING DATA TO MINE:\n- Phase 3 agent trajectories (50 tasks)\n- Conversation logs and tool calls\n- Code changes and test results\n\nNEW ANALYSIS ANGLES:\n1. Code search patterns:\n   - How many searches per task?\n   - Search success rate\n   - Query refinement patterns\n2. Navigation efficiency:\n   - Files opened vs files changed\n   - Re-reads of same files\n   - Exploration path coherence\n3. Comprehension indicators:\n   - Time before first code change\n   - Questions asked about codebase\n   - Documentation references\n\nCOMPARE:\n- Baseline vs MCP patterns\n- Success vs failure task patterns\n- Task type correlations\n\nOUTPUT:\n- Updated Phase 3 report with enterprise metrics\n- Identify clear differentiators\n- Highlight where MCP shows \u003e2x advantage\n\nREFERENCE: CodeContextBench-13j process quality metrics design","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T18:15:23.637419-05:00","updated_at":"2025-12-20T21:13:56.364343-05:00","closed_at":"2025-12-20T21:13:56.364343-05:00"}
{"id":"CodeContextBench-ehx","title":"Validate RepoQA requires actual tool usage (not pattern matching)","description":"Verify RepoQA semantic navigation genuinely requires tool-based understanding. Run agent trajectories through task instances: measure tool calls made, relevance of retrieved code, whether baseline could memorize answers. Document findings.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-20T13:49:28.530035-05:00","updated_at":"2025-12-20T13:49:28.530035-05:00"}
{"id":"CodeContextBench-eil","title":"Mine tasks from 6 additional OSS repos","description":"Mine GitHub tasks from firefox, pytorch, vscode, ffmpeg, tensorrt_llm, servo. Generate Harbor task dirs from results. Target 100+ total tasks across all repos for diverse benchmark set.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:32:21.283592-05:00","updated_at":"2025-12-17T19:40:55.159051-05:00","closed_at":"2025-12-17T19:40:55.159051-05:00"}
{"id":"CodeContextBench-epv","title":"Implement DeathStarBench microservice integration","description":"Integrate DeathStarBench for multi-repo microservice scenarios:\n\nDEATHSTARBENCH COMPONENTS:\n- Social network application (multiple services)\n- Media service (video processing pipeline)\n- Hotel reservation (booking system)\n- E-commerce application\n\nTASKS TO CREATE:\n1. Cross-service bug fixes (trace through service mesh)\n2. API contract updates (breaking changes across services)\n3. Performance optimization (N+1 query problems)\n4. Service dependency analysis\n\nCHALLENGES TO TEST:\n- Multi-repo navigation\n- Service interface understanding\n- Distributed tracing comprehension\n- Microservice-specific patterns\n\nEVALUATION:\n- Success on cross-service changes\n- Completeness (all affected services found)\n- Understanding of service boundaries\n\nREFERENCE: ENTERPRISE_CODEBASES.md synthetic environments section","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T18:15:22.879284-05:00","updated_at":"2025-12-20T18:15:22.879284-05:00","dependencies":[{"issue_id":"CodeContextBench-epv","depends_on_id":"CodeContextBench-zez","type":"blocks","created_at":"2025-12-20T18:15:35.837892-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-ews","title":"Create ClaudeCodeAgent (baseline) for Harbor","description":"Implement claude_code_agent.py extending BasePatchAgent. Use Harbor's claude-code CLI. No Sourcegraph tools (baseline condition).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.340214-05:00","updated_at":"2025-12-17T15:56:23.711557-05:00","closed_at":"2025-12-17T15:56:23.711557-05:00","dependencies":[{"issue_id":"CodeContextBench-ews","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.340656-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-gh5","title":"Mine GitHub PRs and regenerate tasks with real commit SHAs","description":"Execute mining pipeline with real commit extraction (CodeContextBench-k70) and regenerate all task definitions with proper pre_fix_rev/ground_truth_rev values. Update task.toml files and Dockerfiles to checkout correct commits. Steps: (1) Set GITHUB_TOKEN, (2) Run mine_tasks.py with multiple repos, (3) Run regenerate_tasks.py to update existing tasks, (4) Validate Dockerfiles checkout pre-fix commits, (5) Spot-check 2-3 tasks manually. Success: All tasks have real commit SHAs and Dockerfiles checkout pre-fix state.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T11:04:07.622307-05:00","updated_at":"2025-12-19T11:14:19.621707-05:00","closed_at":"2025-12-19T11:14:19.621707-05:00"}
{"id":"CodeContextBench-gn9","title":"Create Harbor adapter for DependEval benchmark","description":"","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-19T15:06:39.601134-05:00","updated_at":"2025-12-19T17:53:52.626189-05:00","closed_at":"2025-12-19T17:53:52.626189-05:00"}
{"id":"CodeContextBench-jdm","title":"Scale testing benchmark: Small â†’ Enterprise codebase sizes","description":"Design benchmark that tests tools at multiple codebase scales: small (10k-100k LOC), medium (100k-1M LOC), large (1M-10M LOC), enterprise (10M+ LOC). Simulate scale using combinations of open-source projects. Test code search, navigation, cross-module changes. Reference: Google 2B LOC, Stripe 20M LOC, Uber 100M+ LOC. Key metrics: search latency, comprehension time, multi-file edit correctness.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:44.417793-05:00","updated_at":"2025-12-20T17:55:44.417793-05:00","dependencies":[{"issue_id":"CodeContextBench-jdm","depends_on_id":"CodeContextBench-1wi","type":"blocks","created_at":"2025-12-20T17:58:20.186812-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-k5s","title":"Establish baseline metrics: What does good tool usage look like?","description":"Define metrics: deepsearch relevance, tokens-per-task, retrieval latency impact. Run successful trajectories to extract patterns. Creates reference for evaluating future agent implementations.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-20T13:49:29.295903-05:00","updated_at":"2025-12-20T13:49:29.295903-05:00"}
{"id":"CodeContextBench-k70","title":"Populate pre_fix_rev and ground_truth_rev in task definitions","description":"CRITICAL: Task definitions (task.toml files) have placeholder values for pre_fix_rev and ground_truth_rev. Currently all tasks use generic 'HEAD~1' instead of actual PR commit info. This blocks proper benchmark validation because agent changes can't be compared against known ground truth.\n\nROOT CAUSE: src/task_mining/mine_tasks.py line 84 uses hardcoded 'HEAD~1' placeholder. The GitHub mining analysis doesn't extract actual parent commit before PR merge.\n\nREQUIRED FIX:\n1. Analyze mine_tasks.py to understand what PR metadata is available (GitHubPullRequest object)\n2. Extract merge_commit_sha and parent commit SHA from PR\n3. For each PR: pre_fix_rev = parent commit before merge, ground_truth_rev = merge commit SHA\n4. Update mining script to compute real values instead of placeholders\n5. Regenerate task definitions or update existing ones with correct commits\n6. Validate: Each task.toml should have actual git commits, not 'HEAD~1'\n\nVALIDATION: After fix, tasks should have commits like: pre_fix_rev=abc123def456, ground_truth_rev=def456ghi789 (from actual GitHub PR data)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:26:50.659314-05:00","updated_at":"2025-12-19T10:29:06.790608-05:00","closed_at":"2025-12-19T10:29:06.790608-05:00"}
{"id":"CodeContextBench-ky2","title":"Context switching cost simulation in benchmarks","description":"Design benchmark simulating interruptions and task switching (23min recovery time). Test: partial task completion â†’ context switch â†’ resume. Measure: ability to recover context, redundant work after switch, notes/documentation quality. Simulate microservice 'context-switching hell' with multi-tool workflows (code + logs + docs).","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:45.600765-05:00","updated_at":"2025-12-20T17:55:45.600765-05:00"}
{"id":"CodeContextBench-l52","title":"Task Dockerfiles missing repository clones - must checkout pre_fix_rev and run real tests","description":"Task Dockerfiles are incomplete stubs without repository clones. Blocks all task validation. Related to CodeContextBench-7wq. Must add: (1) git clone of target repo, (2) git checkout pre_fix_rev, (3) build setup, (4) real test commands. Discovered from baseline test execution: workspace is empty, test can't run, no reward.txt generated.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-19T11:21:09.345723-05:00","updated_at":"2025-12-19T11:23:38.118901-05:00","closed_at":"2025-12-19T11:23:38.118901-05:00","dependencies":[{"issue_id":"CodeContextBench-l52","depends_on_id":"CodeContextBench-gh5","type":"discovered-from","created_at":"2025-12-19T11:21:09.346474-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-lxl","title":"Experiment: Scale impact on tool effectiveness (10kâ†’1M LOC)","description":"Run controlled experiment testing tool performance at different scales:\n\nEXPERIMENT DESIGN:\n- Same task type across different repo sizes\n- Sizes: 10k, 50k, 100k, 500k, 1M LOC\n- Both agents: baseline and MCP\n- Measure degradation curves\n\nHYPOTHESIS:\n- Baseline: Performance degrades faster with scale\n- MCP: More stable across scales\n- Crossover point: where MCP becomes essential (not just helpful)\n\nTASKS:\n- 'Find function implementing X'\n- 'Fix bug in module Y'\n- 'Update API usage across codebase'\n- 'Refactor pattern Z'\n\nMETRICS:\n- Search time and success rate\n- Navigation efficiency\n- Correctness at each scale\n- Token usage / cost\n\nREFERENCE POINTS:\n- Google: 2B LOC, thousands of daily searches\n- Stripe: 20M LOC monorepo\n- Small projects: 10k-100k LOC\n\nDELIVERABLE:\n- Scale performance curves\n- Crossover analysis\n- ROI model: at what scale does MCP pay for itself?\n\nDEPENDENCIES: Enterprise-scale test repository (1M+ LOC)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T18:16:28.727478-05:00","updated_at":"2025-12-20T18:16:28.727478-05:00","dependencies":[{"issue_id":"CodeContextBench-lxl","depends_on_id":"CodeContextBench-cvk","type":"blocks","created_at":"2025-12-20T18:16:40.551643-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-m7j","title":"Capture full multi-turn agent conversations, not just final summary","description":"Current traces only capture final result. Need to capture ALL turns:\n- Each turn = request + response pair\n- For 44-turn conversation: save all 44 interactions\n- Show agent's reasoning evolution (why did it ask tool X, then tool Y, then make change Z?)\n- Identify where MCP (Deep Search) was actually helpful\n- Track when agent gets stuck vs makes progress\n\nEnable via:\n1. Claude API streaming with events\n2. Parse and capture each turn separately\n3. Group into conversation tree/narrative\n4. Analyze decision points where Deep Search changed reasoning","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-18T08:50:08.075036-05:00","updated_at":"2025-12-18T08:50:08.075036-05:00"}
{"id":"CodeContextBench-mk9","title":"Create ClaudeCodeSourcegraphMCPAgent for Harbor","description":"Implement claude_code_sg_mcp_agent.py with Sourcegraph MCP tools enabled. Requires SRC_ACCESS_TOKEN. Treatment condition for A/B testing.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:50.420936-05:00","updated_at":"2025-12-17T15:56:26.950123-05:00","closed_at":"2025-12-17T15:56:26.950123-05:00","dependencies":[{"issue_id":"CodeContextBench-mk9","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:26:50.421346-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-mqz","title":"Fix task environment checkouts to pre-fix commits for proper benchmark validation","description":"CRITICAL: All benchmark tasks currently clone code with fixes already applied. For sgt-001, the Dockerfile clones PyTorch HEAD which includes commit 9d0d198cb50 (thread safety fix). When agents run and try to implement the fix, 'git diff HEAD' shows empty because HEAD already has the changes. This makes all tests fail with 0.0 reward.\n\nROOT CAUSE: Each task's Dockerfile must checkout code to the commit state BEFORE the fix was merged, not after.\n\nREQUIRED FIX: For each task:\n1. Identify the merged PR commit (e.g., 9d0d198cb50 for sgt-001)\n2. Find the commit immediately BEFORE it was merged\n3. Update Dockerfile to checkout that pre-fix commit\n4. Agents will then implement the fix from a clean baseline\n5. 'git diff HEAD' will show their actual changes\n\nEVIDENCE: MCP agent made 2 lines of code changes (test output showed changes detected), proving autonomous env vars are working. But test failed because it can't find NCCLUtils.cpp modifications in git diff against HEAD which already has them.\n\nVALIDATION: After fixing environments, re-run Phase 4 tests. Both agents should show code changes and test results matching the fix implementation.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T10:07:17.933235-05:00","updated_at":"2025-12-19T10:46:53.204148-05:00","closed_at":"2025-12-19T10:46:53.204148-05:00"}
{"id":"CodeContextBench-mw8","title":"Implement tool_profiles.py for MCP tool configuration","description":"Create tool_profiles.py to manage MCP tool definitions, capabilities, and configuration. Support tool registration, validation, and runtime configuration. Integrate with agents for tool availability at execution time.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:49.18927-05:00","updated_at":"2025-12-17T19:40:49.18927-05:00"}
{"id":"CodeContextBench-n0t","title":"Index 150 DependEval repos into Sourcegraph","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T12:44:45.213428-05:00","updated_at":"2025-12-20T12:52:21.361946-05:00","closed_at":"2025-12-20T12:52:21.361946-05:00"}
{"id":"CodeContextBench-nar","title":"Implement build/test feedback loop instrumentation","description":"Add instrumentation to track build/test cycles in benchmark runs:\n\nMETRICS TO CAPTURE:\n- Number of build attempts before success\n- Number of test runs (full suite + partial)\n- Time in build/test feedback loop\n- Test failures: real vs flaky\n- Build errors: syntax vs logic\n\nIMPLEMENTATION:\n- Hook into Harbor's build/test execution\n- Timestamp each build/test invocation\n- Capture stdout/stderr for error analysis\n- Track code changes between attempts\n\nANALYSIS TARGETS:\n- How many cycles does each agent need?\n- Do tools reduce trial-and-error?\n- Correlation: better comprehension = fewer cycles?\n- Compare to enterprise baselines (PayPal 40% flaky)\n\nSIMULATE (optional):\n- Slow CI (add artificial delays)\n- Flaky tests (random failures)\n- Complex build requirements\n\nREFERENCE: ENTERPRISE_CODEBASES.md build/test complexity, 68% burnout from slow feedback","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T18:15:24.144694-05:00","updated_at":"2025-12-20T18:15:24.144694-05:00","dependencies":[{"issue_id":"CodeContextBench-nar","depends_on_id":"CodeContextBench-zez","type":"blocks","created_at":"2025-12-20T18:15:36.697172-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-ou2","title":"Onboarding simulation benchmark: New developer tasks","description":"Design tasks simulating new developer onboarding (takes months at Google scale). Test: unfamiliar codebase navigation, finding experts/docs, first contribution quality. Measure: time to locate relevant code, documentation consultation frequency, mentor interaction needs. Reference: Google's 3-6 week remote onboarding delay, top hindrances (new tech, poor docs, finding expertise).","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:45.845938-05:00","updated_at":"2025-12-20T17:55:45.845938-05:00"}
{"id":"CodeContextBench-q89","title":"Port GitHub task mining infrastructure from sg_benchmark","description":"Copy task_mining/ with GitHub API query builder. Enable generating real-world tasks from OSS repos (Firefox, Kubernetes, etc.).","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T15:27:23.079179-05:00","updated_at":"2025-12-17T19:26:01.542374-05:00","closed_at":"2025-12-17T19:26:01.542374-05:00","dependencies":[{"issue_id":"CodeContextBench-q89","depends_on_id":"CodeContextBench-6vn","type":"blocks","created_at":"2025-12-17T15:27:23.079635-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-qg9","title":"Expand SWE-Bench tasks with enterprise scenarios","description":"Expand existing SWE-Bench benchmark with enterprise-pattern tasks:\n\nTASK CATEGORIES (from ENTERPRISE_CODEBASES.md):\n1. Wide-impact changes: Select tasks affecting 10+ files (like Uber's 100+ service commits)\n2. Cross-service coordination: Multi-repo or multi-module tasks\n3. Monorepo patterns: Tasks requiring navigation of large unified codebases\n4. Performance optimization: Non-functional requirement fixes\n5. Onboarding simulation: Tasks in unfamiliar codebases\n\nSELECTION CRITERIA:\n- Tasks from repos with 100k+ LOC\n- Tasks requiring cross-file understanding\n- Tasks with build/test complexity\n- Mix of bug fixes, features, refactoring\n\nTARGET: 20-30 tasks reflecting enterprise patterns\n\nREFERENCE: docs/ROADMAP_ENTERPRISE_BENCHMARKS.md Phase 2","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-20T18:15:22.09943-05:00","updated_at":"2025-12-20T18:15:22.09943-05:00"}
{"id":"CodeContextBench-qp1","title":"Monorepo vs Multi-repo benchmark scenarios","description":"Design benchmarks testing both monorepo patterns (like Google/Stripe) and multi-repo microservice patterns (like pre-2017 Uber). Test cross-service changes, dependency management, code search across repos. Simulate wide-impact commits (affecting 100+ modules). Use DeathStarBench or d'Aragona microservice dataset. Measure: blast radius handling, cross-repo refactoring, version skew detection.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:44.654392-05:00","updated_at":"2025-12-20T17:55:44.654392-05:00","dependencies":[{"issue_id":"CodeContextBench-qp1","depends_on_id":"CodeContextBench-1wi","type":"blocks","created_at":"2025-12-20T17:58:20.408098-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-qxh","title":"Port infrastructure docs: PODMAN.md, docker-wrapper.sh, harbor-config.yaml","description":"Copy working Podman setup from sourcegraph-benchmarks. Include docker wrapper script and Harbor configuration.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T15:27:10.050801-05:00","updated_at":"2025-12-17T17:54:10.832954-05:00","closed_at":"2025-12-17T17:54:10.832954-05:00","dependencies":[{"issue_id":"CodeContextBench-qxh","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:10.051232-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-rqg","title":"Integrate NeMo-Agent-Toolkit for structured execution tracing","description":"NeMo-Agent-Toolkit provides structured instrumentation (per-call tracing, token counts, latency, success/failure). Refactor observability modules to consume NeMo traces instead of regex-parsing logs. Extract from NeMo: failed_tool_calls, token_usage, per_tool_latency, operation timeline.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T16:59:09.347198-05:00","updated_at":"2025-12-17T17:42:54.46463-05:00","closed_at":"2025-12-17T17:42:54.46463-05:00","dependencies":[{"issue_id":"CodeContextBench-rqg","depends_on_id":"CodeContextBench-4re","type":"discovered-from","created_at":"2025-12-17T16:59:09.347861-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-rvk","title":"Document github_mined benchmark limitations and scope","description":"Add README to benchmarks/github_mined/ explaining: sample size (25 tasks), single repo (PyTorch), task types, what it validates, known limitations. Clarify what this benchmark does/doesn't prove. Reference: history/benchmark-design-review-20251220.md","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:49:28.379858-05:00","updated_at":"2025-12-20T13:52:13.685288-05:00","closed_at":"2025-12-20T13:52:13.685288-05:00"}
{"id":"CodeContextBench-s0c","title":"Redesign big_code_mcp: Pre-clone repos for both baseline and MCP agents","description":"Deep Search found that Phase 3 comparison was invalid: baseline agents saw empty directory stubs while MCP agents could use Sourcegraph to clone repos. Redesign so both agents have identical pre-cloned repository access. This isolates comparison to search strategy effectiveness rather than file access capability. See history/benchmark-design-review-20251220.md","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:49:28.22345-05:00","updated_at":"2025-12-20T13:52:28.821594-05:00","closed_at":"2025-12-20T13:52:28.821594-05:00"}
{"id":"CodeContextBench-sbh","title":"Create comparative visualization dashboard for enterprise metrics","description":"Build dashboard visualizing enterprise-informed benchmark metrics:\n\nVISUALIZATIONS:\n1. Time allocation breakdown (58% comprehension, 35% navigation)\n   - Show baseline vs MCP comparison\n   - Per-task and aggregate views\n2. Code search patterns\n   - Frequency heatmaps\n   - Success rate trends\n   - Query refinement flows\n3. Navigation efficiency\n   - Files read vs files changed scatter\n   - Exploration path graphs\n4. Build/test cycles\n   - Cycles to success distribution\n   - Time in feedback loop\n5. Process quality scores\n   - Multi-metric radar charts\n   - Baseline vs MCP side-by-side\n\nTECHNOLOGY:\n- Python: matplotlib/seaborn for static\n- Optional: Plotly/Dash for interactive\n- Export: PNG for reports, HTML for exploration\n\nOUTPUT:\n- Auto-generated after each benchmark run\n- Comparison across benchmark suites\n- Export for presentations/papers\n\nREFERENCE: docs/ROADMAP_ENTERPRISE_BENCHMARKS.md KPI section","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T18:15:24.402151-05:00","updated_at":"2025-12-20T18:15:24.402151-05:00","dependencies":[{"issue_id":"CodeContextBench-sbh","depends_on_id":"CodeContextBench-trg","type":"blocks","created_at":"2025-12-20T18:15:36.493711-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-swl","title":"Phase 1: Create CodeContextBench directory skeleton","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:26:40.440777-05:00","updated_at":"2025-12-17T15:31:50.10868-05:00","closed_at":"2025-12-17T15:31:50.10868-05:00"}
{"id":"CodeContextBench-trg","title":"Run baseline vs MCP comparison with enterprise metrics","description":"Execute benchmark comparison capturing enterprise productivity metrics:\n\nEXPERIMENT SETUP:\n- 20-30 tasks from expanded SWE-Bench\n- Both agents: baseline (no MCP) and big_code_mcp\n- Equal file access (pre-cloned repos)\n\nMETRICS TO COMPARE:\n1. Outcome: Correctness, test pass rate\n2. Process (NEW):\n   - Code search frequency and patterns\n   - Files read before understanding\n   - Navigation path efficiency\n   - Build/test cycles to success\n   - Time to comprehension\n\nANALYSIS:\n- Where does MCP show 2x+ improvement?\n- Which enterprise patterns benefit most?\n- Tool usage correlation with success\n- Identify failure modes for both approaches\n\nDELIVERABLE: Comparative report with enterprise lens\n\nDEPENDENCIES: Metrics infrastructure, expanded task set","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-20T18:15:22.362919-05:00","updated_at":"2025-12-20T18:15:22.362919-05:00","dependencies":[{"issue_id":"CodeContextBench-trg","depends_on_id":"CodeContextBench-zez","type":"blocks","created_at":"2025-12-20T18:15:35.225191-05:00","created_by":"daemon"},{"issue_id":"CodeContextBench-trg","depends_on_id":"CodeContextBench-qg9","type":"blocks","created_at":"2025-12-20T18:15:35.411322-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-tsd","title":"Cross-service change benchmark: Multi-module edits","description":"Design tasks requiring changes across multiple services/modules (like Uber's commits affecting 100-1000+ services). Test: API contract changes propagating through call chain, shared library updates, cross-cutting refactors. Measure: completeness (all affected modules found), correctness (changes are compatible), efficiency (minimal exploration overhead).","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:45.134149-05:00","updated_at":"2025-12-20T17:55:45.134149-05:00","dependencies":[{"issue_id":"CodeContextBench-tsd","depends_on_id":"CodeContextBench-2wz","type":"blocks","created_at":"2025-12-20T17:58:20.812488-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-uzn","title":"Port 10Figure task generator and Harbor integration","description":"Copy gen_harbor_tasks.py, test.sh.j2 template, and Harbor dataset configs from harbor-10figure-dataset. Create benchmarks/10figure/ structure.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T15:27:00.302054-05:00","updated_at":"2025-12-17T19:26:01.643071-05:00","closed_at":"2025-12-17T19:26:01.643071-05:00","dependencies":[{"issue_id":"CodeContextBench-uzn","depends_on_id":"CodeContextBench-swl","type":"blocks","created_at":"2025-12-17T15:27:00.302463-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-v9j","title":"Enhance big_code_mcp task instructions to require MCP tool usage","description":"Current instructions let agents work without Sourcegraph tools. Redesign to REQUIRE deepsearch for understanding architecture before changes. Makes MCP value visible. Document expected tool usage patterns.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T13:49:28.991995-05:00","updated_at":"2025-12-20T13:49:28.991995-05:00"}
{"id":"CodeContextBench-var","title":"Run baseline vs MCP comparison on DependEval subset","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T12:44:49.538232-05:00","updated_at":"2025-12-20T12:54:34.136229-05:00","closed_at":"2025-12-20T12:54:34.136229-05:00"}
{"id":"CodeContextBench-von","title":"Analyze benchmark results \u0026 generate comparative report","description":"Execute Phase 2c: Aggregate benchmark results \u0026 test hypothesis per MINING_PLAN.md Phase 2c. Hypothesis: Sourcegraph code search improves agent success on multi-file tasks. Aggregate metrics (success rate, efficiency, cost) across task categories, difficulty, language. Stratified analysis revealing which task types benefit most from search. Generate HTML/JSON comparative report. Validate H1: +MCP success \u003e baseline (+10-15% expected).","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T19:40:47.139209-05:00","updated_at":"2025-12-17T19:45:49.021383-05:00","dependencies":[{"issue_id":"CodeContextBench-von","depends_on_id":"CodeContextBench-cy6","type":"discovered-from","created_at":"2025-12-17T19:40:47.140584-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-vqr","title":"Experiment: Task type performance matrix (7 enterprise scenarios)","description":"Comprehensive experiment across 7 enterprise task types:\n\nTASK TYPES (from ENTERPRISE_CODEBASES.md and CodeContextBench-2wz):\n1. Bug fixing across multiple services\n2. Feature implementation with cross-cutting changes\n3. Refactoring and tech debt reduction\n4. Performance optimization\n5. Code review and comprehension\n6. Documentation and knowledge sharing\n7. Onboarding and learning new subsystems\n\nEXPERIMENT:\n- 5-10 tasks per type (35-70 total tasks)\n- Both agents on all tasks\n- Full metrics capture\n\nANALYSIS:\n- Performance matrix: task type Ã— agent type\n- Where does MCP excel? (hypothesis: comprehension, cross-file)\n- Where is baseline sufficient? (hypothesis: simple, isolated tasks)\n- Task difficulty correlation with success gap\n\nMETRICS BY TASK TYPE:\n- Success rate\n- Time to completion\n- Tool usage patterns\n- Navigation efficiency\n- Comprehension indicators\n\nDELIVERABLE:\n- Heat map: performance by task type\n- 'Sweet spot' analysis for each tool\n- Task type recommendations for tool selection\n- Validation of diverse task types benchmark design\n\nDEPENDENCIES: Expanded task set, metrics infrastructure\n\nREFERENCE: docs/ROADMAP_ENTERPRISE_BENCHMARKS.md - diverse task types","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T18:16:29.479379-05:00","updated_at":"2025-12-20T18:16:29.479379-05:00","dependencies":[{"issue_id":"CodeContextBench-vqr","depends_on_id":"CodeContextBench-qg9","type":"blocks","created_at":"2025-12-20T18:16:40.992058-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-wkb","title":"Mine 6 additional OSS repos \u0026 generate 100+ Harbor tasks","description":"Execute Phase 2a: Mine 6 repos (firefox, pytorch, vscode, ffmpeg, tensorrt_llm, servo) per MINING_PLAN.md. Generate 50-75 high-quality Harbor tasks. See history/MINING_PLAN.md for master strategy (MINING_STRATEGY.md for detailed pipeline, RESEARCH_ALIGNMENT.md for paper alignment). Requirements: multi-file (â‰¥2 files), deterministic verification, real GitHub work. Success: â‰¥80% validation pass, 5+ language coverage, balanced difficulty distribution.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T19:40:42.708461-05:00","updated_at":"2025-12-17T20:02:18.30501-05:00","closed_at":"2025-12-17T20:02:18.30501-05:00"}
{"id":"CodeContextBench-xiz","title":"Developer productivity metrics: Comprehension \u0026 navigation","description":"Design metrics capturing real developer workflows: time to comprehension (58% of dev time), navigation efficiency (35% of time), search success rate. Measure: time to locate relevant code, number of files read before understanding, search query refinement count. Compare to industry baselines (Google's internal metrics, Stripe surveys). Test onboarding scenarios (new dev ramp-up time).","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:55:44.907266-05:00","updated_at":"2025-12-20T17:55:44.907266-05:00","dependencies":[{"issue_id":"CodeContextBench-xiz","depends_on_id":"CodeContextBench-13j","type":"blocks","created_at":"2025-12-20T17:58:20.592868-05:00","created_by":"daemon"}]}
{"id":"CodeContextBench-yao","title":"Validate RepoQA adapter: run 2-task baseline vs MCP comparison","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T11:00:47.513831-05:00","updated_at":"2025-12-20T11:02:02.416848-05:00","closed_at":"2025-12-20T11:02:02.416848-05:00"}
{"id":"CodeContextBench-zez","title":"Implement enterprise metrics collection infrastructure","description":"Build infrastructure to capture enterprise-informed metrics during benchmark runs:\n\nMETRICS TO CAPTURE:\n- Code search patterns: frequency, success rate, query refinement\n- Navigation efficiency: files read vs files needed\n- Comprehension indicators: time spent reading, re-reads\n- Tool usage patterns: MCP deep search frequency, baseline context utilization\n- Build/test cycles: attempts to success, time in feedback loop\n\nIMPLEMENTATION:\n- Extend Harbor agent logging to capture these metrics\n- Create metrics collector module\n- Design JSON schema for metric storage\n- Add timestamps for time-based analysis\n\nOUTPUT:\n- metrics.json per task run\n- Aggregated metrics across benchmark suite\n- Visualization-ready data format\n\nREFERENCE: ENTERPRISE_CODEBASES.md sections on developer time allocation","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-20T18:15:21.835139-05:00","updated_at":"2025-12-20T18:15:21.835139-05:00"}
{"id":"CodeContextBench-zyq","title":"Create monorepo simulation benchmark from OSS projects","description":"Build monorepo benchmark by combining open-source projects:\n\nAPPROACH (from ENTERPRISE_CODEBASES.md recommendations):\n- Combine 10-20 top GitHub projects into single repo\n- Simulate Google/Stripe monorepo pattern\n- Total target: 1M+ LOC combined\n\nPROJECT SELECTION:\n- Popular projects with good test coverage\n- Mix of languages (Python, TypeScript, Go)\n- Complementary domains (web, CLI, data processing)\n\nTASKS TO CREATE:\n1. Cross-project dependency tasks (like shared library updates)\n2. Wide-impact refactors (affecting multiple projects)\n3. Code search across entire monorepo\n4. Consistent API pattern enforcement\n\nEVALUATION:\n- Can tools handle monorepo scale?\n- Search effectiveness across project boundaries\n- Multi-project change consistency\n\nREFERENCE: Stripe 20M LOC monorepo, Uber monorepo patterns","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T18:15:22.6254-05:00","updated_at":"2025-12-20T18:15:22.6254-05:00","dependencies":[{"issue_id":"CodeContextBench-zyq","depends_on_id":"CodeContextBench-zez","type":"blocks","created_at":"2025-12-20T18:15:35.624096-05:00","created_by":"daemon"}]}
