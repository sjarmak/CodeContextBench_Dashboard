# Harbor Evaluation Telemetry Schema

Complete reference for all metrics and telemetry captured during Harbor evaluations.

## Overview

Each Harbor evaluation captures comprehensive telemetry at multiple levels:
- **Job Level**: Aggregate across all trials
- **Trial Level**: Individual task execution
- **Step Level**: Agent step-by-step execution (trajectory)

---

## ğŸ“‹ Job-Level Telemetry

Location: `harbor_jobs/jobs/<job_id>/result.json`

```json
{
  "config": {
    "agent": {
      "name": "openhands",
      "model_name": "anthropic/claude-haiku-4-5"
    },
    "environment": {
      "type": "docker",
      "cpus": 2,
      "memory_mb": 4096
    }
  },
  "stats": {
    "n_trials": 5,
    "n_errors": 1,
    "evals": {
      "openhands__claude-haiku-4-5__swebench-verified": {
        "n_trials": 5,
        "n_errors": 1,
        "metrics": [
          {
            "reward": 0.4,
            "cost_usd": 0.087,
            "tokens": 64200
          }
        ]
      }
    }
  }
}
```

### Job Metrics

| Field | Type | Description |
|-------|------|-------------|
| `n_trials` | int | Total number of trials run |
| `n_errors` | int | Trials that failed to complete |
| `stats.evals` | dict | Results grouped by agent__model__dataset |
| `stats.evals[key].metrics` | list | Aggregate metrics for this configuration |

---

## ğŸ”¬ Trial-Level Telemetry

Location: `harbor_jobs/jobs/<job_id>/<task_id>/result.json`

```json
{
  "id": "uuid",
  "task_name": "django__forms__issue_123",
  "trial_name": "django__forms__issue_123__abc123",
  "trial_uri": "file:///path/to/trial",
  "source": "swebench-verified",
  "agent_info": {
    "name": "openhands",
    "version": "0.14.2",
    "model_info": {
      "name": "claude-haiku-4-5",
      "provider": "anthropic"
    }
  },
  "agent_result": {
    "n_input_tokens": 46138,
    "n_cache_tokens": 0,
    "n_output_tokens": 308,
    "cost_usd": 0.047678,
    "rollout_details": null,
    "metadata": null
  },
  "verifier_result": {
    "rewards": {
      "reward": 1.0
    },
    "metadata": {
      "test_output": "...",
      "passed_tests": 5,
      "failed_tests": 0
    }
  },
  "exception_info": null,
  "started_at": "2025-12-29T18:12:03.588006",
  "finished_at": "2025-12-29T18:13:29.707429",
  "environment_setup": {
    "started_at": "2025-12-29T18:12:03.588769",
    "finished_at": "2025-12-29T18:12:07.093605"
  },
  "agent_setup": {
    "started_at": "2025-12-29T18:12:07.093651",
    "finished_at": "2025-12-29T18:12:37.330663"
  },
  "agent_execution": {
    "started_at": "2025-12-29T18:12:37.330732",
    "finished_at": "2025-12-29T18:13:14.250127"
  },
  "verifier": {
    "started_at": "2025-12-29T18:13:14.250207",
    "finished_at": "2025-12-29T18:13:18.474603"
  }
}
```

### Agent Result Metrics

| Field | Type | Description | Notes |
|-------|------|-------------|-------|
| `n_input_tokens` | int | Tokens sent to LLM | Anthropic API |
| `n_cache_tokens` | int | Tokens from prompt cache | Anthropic cached responses |
| `n_output_tokens` | int | Tokens generated by LLM | |
| `cost_usd` | float | Estimated cost in USD | Calculated from token usage |
| `rollout_details` | obj | RL rollout metadata | For RL fine-tuning |
| `metadata` | obj | Custom agent metadata | Agent-specific data |

### Verifier Result Metrics

| Field | Type | Description |
|-------|------|-------------|
| `rewards` | dict | Task rewards (1.0 = pass, 0.0 = fail) |
| `rewards.reward` | float | Primary reward score |
| `metadata` | dict | Verification details |
| `metadata.test_output` | str | Test stdout/stderr |
| `metadata.passed_tests` | int | Number of passing tests |
| `metadata.failed_tests` | int | Number of failing tests |

### Timing Metrics

Calculated by difference between start and finish times:

```python
# Example calculation
setup_time = (agent_setup.finished_at - agent_setup.started_at).total_seconds()
execution_time = (agent_execution.finished_at - agent_execution.started_at).total_seconds()
total_time = (finished_at - started_at).total_seconds()
```

| Phase | Typical Duration | Notes |
|-------|------------------|-------|
| environment_setup | 3-10s | Docker build + pull |
| agent_setup | 30s | Install OpenHands, download models |
| agent_execution | 30-300s | Actual task solving |
| verifier | 5-30s | Run tests, verify solution |

---

## ğŸ“ Trajectory-Level Telemetry

Location: `harbor_jobs/jobs/<job_id>/<task_id>/agent/trajectory.json`

ATIF-compatible trajectory format with step-by-step execution:

```json
{
  "schema_version": "ATIF-v1.5",
  "session_id": "7121d2b7-02ca-4b-198473a63bccae6",
  "agent": {
    "name": "openhands",
    "version": "0.14.2",
    "tool_definitions": [
      {
        "name": "execute_bash",
        "description": "Execute bash command",
        "parameters": {...}
      }
    ]
  },
  "steps": [
    {
      "step_id": 1,
      "timestamp": "2025-12-29T18:12:37.330732",
      "source": "user",
      "message": "Fix the failing test in django/forms/widgets.py",
      "tool_calls": null,
      "observation": null,
      "metrics": null
    },
    {
      "step_id": 2,
      "timestamp": "2025-12-29T18:12:40.123456",
      "source": "agent",
      "message": "I'll start by exploring the repository structure...",
      "tool_calls": [
        {
          "tool_call_id": "call_123",
          "function_name": "execute_bash",
          "arguments": {
            "command": "find . -name '*.py' -path '*/forms/*' | head -20"
          }
        }
      ],
      "observation": {
        "results": [
          {
            "source_call_id": "call_123",
            "content": "./django/forms/widgets.py\n./django/forms/fields.py\n..."
          }
        ]
      },
      "metrics": {
        "prompt_tokens": 3200,
        "completion_tokens": 150,
        "cached_tokens": 0,
        "cost_usd": 0.005
      }
    },
    {
      "step_id": 3,
      "timestamp": "2025-12-29T18:12:45.234567",
      "source": "system",
      "message": "Tool execution completed",
      "tool_calls": null,
      "observation": null,
      "metrics": null
    }
  ],
  "final_metrics": {
    "total_prompt_tokens": 46138,
    "total_completion_tokens": 308,
    "total_cached_tokens": 0,
    "total_cost_usd": 0.047678
  }
}
```

### Step Structure

| Field | Type | Description |
|-------|------|-------------|
| `step_id` | int | Sequence number |
| `timestamp` | str | ISO 8601 timestamp |
| `source` | str | "user", "agent", or "system" |
| `message` | str | Text content of step |
| `tool_calls` | array | Agent tool invocations |
| `observation` | obj | Results of tool calls |
| `metrics` | obj | Token/cost for this step |

### Tool Call Structure

```json
{
  "tool_call_id": "call_123",
  "function_name": "execute_bash",
  "arguments": {
    "command": "ls -la /path/to/dir"
  }
}
```

### Observation Structure

```json
{
  "results": [
    {
      "source_call_id": "call_123",
      "content": "Tool output text here"
    }
  ]
}
```

---

## ğŸ’° Cost Calculation

### Formula

```python
# Anthropic pricing (as of 2025-12)
HAIKU_INPUT = 0.80 / 1_000_000        # $0.80 per 1M input tokens
HAIKU_OUTPUT = 4.00 / 1_000_000       # $4.00 per 1M output tokens

SONNET_INPUT = 3.00 / 1_000_000
SONNET_OUTPUT = 15.00 / 1_000_000

OPUS_INPUT = 15.00 / 1_000_000
OPUS_OUTPUT = 75.00 / 1_000_000

# Cost = (input_tokens * input_rate) + (output_tokens * output_rate)
cost = (n_input_tokens * rate_in) + (n_output_tokens * rate_out)
```

### Example

```
Model: Haiku
Input tokens: 46,138
Output tokens: 308

Cost = (46138 Ã— $0.80/1M) + (308 Ã— $4.00/1M)
     = $0.0369 + $0.0012
     = $0.0381
```

---

## ğŸ“Š IR-SDLC Specific Metrics

For information retrieval tasks from IR-SDLC-Factory:

### Standard IR Metrics

```json
{
  "retrieval_results": {
    "precision@10": 0.8,
    "recall@10": 0.65,
    "f1@10": 0.716,
    "mrr": 0.92,
    "ndcg@10": 0.85,
    "map": 0.78,
    "hit_rate@10": 1.0
  }
}
```

| Metric | Range | Meaning |
|--------|-------|---------|
| `precision@10` | 0-1 | Of top 10 results, how many were relevant? |
| `recall@10` | 0-1 | Of all relevant items, how many in top 10? |
| `f1@10` | 0-1 | Harmonic mean of precision and recall |
| `mrr` | 0-1 | Avg of 1/(rank of first relevant) |
| `ndcg@10` | 0-1 | Discounted relevance ranking |
| `map` | 0-1 | Mean Average Precision |
| `hit_rate@10` | 0-1 | Fraction with â‰¥1 relevant result |

### SDLC-Specific Metrics

```json
{
  "sdlc_metrics": {
    "file_level_recall": 0.75,
    "function_level_precision": 0.82,
    "cross_module_coverage": 0.68,
    "context_efficiency": 0.45
  }
}
```

| Metric | Meaning |
|--------|---------|
| `file_level_recall` | Found all affected files? |
| `function_level_precision` | Results at function granularity accurate? |
| `cross_module_coverage` | Covered all relevant modules? |
| `context_efficiency` | Relevant tokens / total tokens |

---

## ğŸ”§ Using the Data

### Example: Calculate Success Rate

```python
import json
from pathlib import Path

job_dir = Path("harbor_jobs/jobs/2025-12-29__18-12-02")

# Load job result
with open(job_dir / "result.json") as f:
    job = json.load(f)

# Get all trial results
trials = []
for trial_dir in job_dir.glob("*__*"):
    with open(trial_dir / "result.json") as f:
        trials.append(json.load(f))

# Calculate metrics
success_rate = sum(1 for t in trials if t["verifier_result"]["rewards"]["reward"] > 0) / len(trials)
total_cost = sum(t["agent_result"]["cost_usd"] for t in trials)
total_tokens = sum(
    t["agent_result"]["n_input_tokens"] + t["agent_result"]["n_output_tokens"]
    for t in trials
)

print(f"Success Rate: {success_rate:.1%}")
print(f"Total Cost: ${total_cost:.2f}")
print(f"Avg Tokens: {total_tokens // len(trials):,}")
```

### Example: Compare Models

```python
# Group by model
by_model = {}
for trial in trials:
    model = trial["agent_info"]["model_info"]["name"]
    if model not in by_model:
        by_model[model] = []
    by_model[model].append(trial)

# Compare
for model, model_trials in by_model.items():
    rewards = [t["verifier_result"]["rewards"]["reward"] for t in model_trials]
    costs = [t["agent_result"]["cost_usd"] for t in model_trials]
    
    print(f"\n{model}:")
    print(f"  Success Rate: {sum(rewards) / len(rewards):.1%}")
    print(f"  Avg Cost: ${sum(costs) / len(costs):.4f}")
    print(f"  Total Trials: {len(model_trials)}")
```

---

## ğŸ“ Logging Locations

All logs and telemetry saved to `harbor_jobs/jobs/<job_id>/`:

```
<job_id>/
â”œâ”€â”€ result.json                          # Job aggregate results
â”œâ”€â”€ config.json                          # Job configuration
â”œâ”€â”€ <task_id>/
â”‚   â”œâ”€â”€ result.json                      # Trial results
â”‚   â”œâ”€â”€ config.json                      # Trial config
â”‚   â”œâ”€â”€ trial.log                        # Trial log
â”‚   â”œâ”€â”€ exception.txt                    # If failed
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ trajectory.json              # Step-by-step (ATIF format)
â”‚   â”‚   â”œâ”€â”€ openhands.txt                # Agent stdout/stderr
â”‚   â”‚   â”œâ”€â”€ openhands.trajectory.json    # OpenHands native trajectory
â”‚   â”‚   â”œâ”€â”€ completions/                 # LLM responses (if saved)
â”‚   â”‚   â”‚   â””â”€â”€ *.json
â”‚   â”‚   â””â”€â”€ sessions/
â”‚   â”‚       â””â”€â”€ <session_id>/
â”‚   â”‚           â””â”€â”€ events/
â”‚   â”‚               â””â”€â”€ *.json           # Event logs
â”‚   â””â”€â”€ verifier/
â”‚       â”œâ”€â”€ reward.txt                   # Numeric reward
â”‚       â”œâ”€â”€ stdout                       # Test output
â”‚       â””â”€â”€ stderr                       # Test errors
â””â”€â”€ ...
```

---

## ğŸ¯ Key Takeaways

1. **Job Level**: Aggregate statistics across all trials
2. **Trial Level**: Per-task results with cost/token breakdown
3. **Step Level**: Agent decision-making and tool use (trajectory)
4. **Timing**: Clearly separated phases (setup, execution, verification)
5. **Cost**: Fully tracked per-step and aggregated
6. **Metrics**: Task-specific (reward) + IR-specific (precision, recall, etc.)

Use this schema to:
- Compare agent configurations
- Track cost vs performance
- Analyze tool usage patterns
- Identify failure modes
- Benchmark improvements
