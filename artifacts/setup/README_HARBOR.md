# ğŸš€ Harbor Evaluations Dashboard

Standalone, simple Streamlit app for benchmarking OpenHands with different MCP configurations.

## Launch

```bash
cd CodeContextBench
streamlit run harbor_dashboard.py
```

## What It Does

Run Harbor evaluations with:
- **3 Agent Configurations**: Baseline, Sourcegraph MCP, Deep Search MCP
- **Multiple Models**: Haiku, Sonnet, Opus
- **Multiple Benchmarks**: SWE-bench, IR-SDLC, Aider, etc.
- **Automatic Telemetry**: Tokens, cost, time, success rate

## Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸš€ Harbor Evaluations Dashboard        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  SIDEBAR: Configure                     â”‚
â”‚  â€¢ Agent (Baseline/SG/DS)               â”‚
â”‚  â€¢ Model (Haiku/Sonnet/Opus)            â”‚
â”‚  â€¢ Dataset (SWE-bench, IR-SDLC, etc)    â”‚
â”‚  â€¢ Advanced: filter, concurrency, etc   â”‚
â”‚                                         â”‚
â”‚  TABS:                                  â”‚
â”‚  â–¶ï¸  Run      â†’ Execute evaluations     â”‚
â”‚  ğŸ“Š Results  â†’ View metrics             â”‚
â”‚  ğŸ“ˆ Compare  â†’ Cross-eval analysis      â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

1. **Setup credentials** (`.env.local`):
   ```
   ANTHROPIC_API_KEY=...
   SOURCEGRAPH_URL=...
   SOURCEGRAPH_ACCESS_TOKEN=...
   ```

2. **Launch dashboard**:
   ```bash
   streamlit run harbor_dashboard.py
   ```

3. **Run a test**:
   - Baseline + Haiku + Hello World
   - Click START
   - Watch logs in real-time
   - View results in Results tab

## Files

| File | Purpose |
|------|---------|
| `harbor_dashboard.py` | Main app (380 lines) |
| `HARBOR_SETUP.md` | 2-minute setup guide |
| `HARBOR_QUICKSTART.md` | Detailed workflows |
| `TELEMETRY_SCHEMA.md` | Data reference |

## Key Features

âœ… **Three MCP Configurations**
- ğŸ”µ Baseline (no MCP)
- ğŸŸ¢ Sourcegraph MCP (code search/navigation)
- ğŸŸ£ Deep Search MCP (semantic analysis)

âœ… **Automatic Metrics**
- Reward (pass/fail)
- Cost (USD)
- Tokens (input+output)
- Time (execution)

âœ… **Simple Workflow**
- Configure in sidebar
- Click START
- View results
- Compare across runs

âœ… **Multiple Benchmarks**
- SWE-Bench Verified/Pro
- Aider Polyglot
- IR-SDLC (Multi-repo, Advanced, Gap-filling)
- Hello World (test)

## Example Workflows

### Baseline (5 min)
```
Baseline + Haiku + Hello World
â†’ Verify setup works
```

### Compare Agents (30 min)
```
Run: SWE-Bench + Python tasks
- Baseline
- Sourcegraph MCP
- Deep Search MCP
â†’ Measure MCP value
```

### Cost Analysis (20 min)
```
Run: Hello World with each model
- Haiku
- Sonnet
- Opus
â†’ Find cost/capability tradeoff
```

## Data

Results stored in `harbor_jobs/jobs/<timestamp>/`:
- Job aggregate results
- Per-trial metrics (tokens, cost, reward)
- Agent trajectories (step-by-step execution)
- Verification logs

All viewable in Results tab.

## Support

- **Setup issues**: See `HARBOR_SETUP.md`
- **How to use**: See `HARBOR_QUICKSTART.md`
- **Data schema**: See `TELEMETRY_SCHEMA.md`
- **Agent logs**: Check `harbor_jobs/jobs/<id>/<task>/agent/openhands.txt`

## That's It!

No complex integrations, no frankenstein code. Just:
1. `streamlit run harbor_dashboard.py`
2. Configure in sidebar
3. Click START
4. View results

Everything is self-contained and simple.
