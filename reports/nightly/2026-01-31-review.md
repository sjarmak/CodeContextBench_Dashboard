# Nightly Research Report — 2026-01-31

Previous reports covered: LoCoBench verifier weaknesses, agent variant gaps, dashboard missing views, scoring methodology (Jan 29); mock runner, lifecycle pipeline failures, dict.get() None bug, harbor parser pass/fail paths, stale model refs, hardcoded paths, dead code in orchestrator (Jan 30). This report focuses on **new findings** not previously surfaced.

---

## 1. Code & Architecture Review

### 1.1 Bare `except:` Blocks Across 11 Call Sites in `src/`

Seven files use bare `except: pass` or `except:` with a silent fallback, totaling 11 call sites:

| File | Count | Context |
|------|-------|---------|
| `src/analysis/statistical_analyzer.py` | 5 | Lines 279, 358, 432, 553, 594 — statistical tests fall back to `p_value = 1.0` on any exception |
| `src/analysis/time_series_analyzer.py` | 1 | Line 452 |
| `src/task_mining/task_generator.py` | 1 | Line 216 |
| `src/benchmark/metrics_extractor.py` | 1 | Line 143 |
| `src/benchmark/run_orchestrator.py` | 1 | Line 92 |
| `src/benchmark/llm_judge.py` | 1 | Line 780 |
| `src/benchmark/report_generator.py` | 1 | Line 235 |

The `statistical_analyzer.py` case is particularly concerning: five separate statistical tests (Fisher's exact, Mann-Whitney U, chi-squared, etc.) silently return `p_value = 1.0` on *any* exception including `TypeError` from malformed input data. This means corrupted data silently produces "no significant difference" instead of raising an error. A genuine data integrity issue would appear as a valid statistical non-finding.

**Recommendation:** Replace with specific exception types (`ValueError`, `scipy.linalg.LinAlgError`) and log warnings. For statistical tests, `p_value = None` (unknown) is more honest than `p_value = 1.0` (no difference).

### 1.2 Shell Injection Surface in `run_triggers.py`

`dashboard/views/run_triggers.py:17-37` uses `subprocess.run(command, shell=True)` where `command` is a string assembled from user-selected benchmark names and agent configs. While the inputs come from Streamlit dropdowns (not free text), the `run_command()` function is a general-purpose utility that accepts any string. If any caller passes unsanitized input, shell metacharacters execute arbitrary commands.

```python
def run_command(command: str, cwd: Optional[Path] = None) -> tuple[int, str, str]:
    result = subprocess.run(command, shell=True, ...)  # Line 27
```

**Recommendation:** Refactor to accept a `list[str]` of arguments and use `shell=False`. The function is only called from within the same module, making the refactor low-risk.

### 1.3 Test Suite Is Diagnostic-Heavy, Assertion-Light

The `tests/` directory contains 47 Python files, but 9 of them are diagnostic/debug scripts rather than proper test suites:

| File | Nature |
|------|--------|
| `tests/diagnose_dashboard.py` | Diagnostic script, not a test |
| `tests/diagnose_trace_display.py` | Diagnostic script |
| `tests/debug_multiselect.py` | Debug script |
| `tests/investigate_llm_judge.py` | Investigation script |
| `tests/show_task_multiselect.py` | UI debug helper |
| `tests/verify_fixes.py` | One-off verification |
| `tests/verify_all_fixes.py` | One-off verification |
| `tests/verify_swebench_integration.py` | Manual verification |
| `tests/smoke_test_10figure.py` | Smoke test (not pytest-compatible) |

These inflate apparent test coverage without providing regression protection. They should be moved to a `scripts/debug/` directory or deleted, leaving `tests/` clean for actual test suites.

**Key coverage gaps among the 38 real test files:**
- No tests for `dashboard/views/` (15 view modules, 9,141 LOC, zero test coverage)
- No tests for `agents/mcp_variants.py` (the 4 MCP agent variants that are the core experimental subjects)
- No tests for `src/analysis/statistical_analyzer.py` (despite it having 5 bare except blocks)
- `test_dashboard_e2e.py` exists but requires a running Streamlit instance — it's not CI-compatible

### 1.4 Ralph Archive Contains Unfinished Work Without Tracking

Two new directories appeared today in `scripts/ralph/archive/`:

- **`2026-01-31-benchmark-trace-reviewer/`**: PRD with 25/27 user stories complete. Missing: cost analysis GUI (US-026), time series analysis GUI (US-027).
- **`2026-01-31-harbor-benchmark-adapters/`**: PRD with 1/11 user stories complete. The remaining 10 stories implement the "Unified Experiment Comparison Pipeline" recommended as the #1 next feature in last night's report.

Neither has corresponding beads issues tracking the incomplete work. The `harbor-benchmark-adapters` session only completed adding `scipy` to requirements before stopping — the core comparison logic, bootstrap testing, and CLI wrapper were never started.

**Recommendation:** Create beads issues for the incomplete Ralph stories so they surface in `bd ready` and don't get lost.

### 1.5 10Figure Benchmark Has Infrastructure But No Cross-Repo Tasks

`benchmarks/10figure/` has 4 scored tasks — all targeting only Kubernetes. The PRD at `tasks/prd-10figure-benchmark-expansion.md` calls for expanding to 10 tasks across 4 repos (Kubernetes, Django, Envoy, TensorFlow) with at least 4 cross-repo tasks.

Current state:
- Base Docker image infrastructure is ready (`base/Dockerfile`, `base/build.sh`)
- Task generator exists (`scripts/gen_harbor_tasks.py`)
- All 4 existing tasks are single-repo Kubernetes tasks
- The base image requires a `~/10Figure-Codebases` corpus (~5GB) that must be manually assembled
- **No cross-repo tasks exist yet** — this is the benchmark's intended differentiator

The PRD has 4 open architectural questions (lines 127-133) that block progress: commit SHAs, cross-repo patch format, Docker image size strategy, and scoring rubric design.

---

## 2. Feature & UX Improvements

### 2.1 Centralized Exception Reporting for Statistical Analysis

The 5 bare excepts in `statistical_analyzer.py` represent a design gap: the analysis layer has no structured way to report partial failures. When a Fisher's exact test fails due to degenerate data (e.g., all-zero contingency table), the caller should know the test was *skipped* rather than receiving a fake `p_value = 1.0`.

**Proposed pattern:**
```python
@dataclass(frozen=True)
class StatResult:
    test_name: str
    p_value: float | None  # None = test could not be computed
    effect_size: float | None
    skipped_reason: str | None  # e.g., "Insufficient data for Fisher's exact test"
```

This feeds directly into the dashboard's statistical view — tasks where tests were skipped get a distinct visual treatment (grey/hatched) instead of being mixed in with genuine non-significant results.

### 2.2 Test Infrastructure for Dashboard Views

With 9,141 LOC and zero test coverage, the dashboard views are the largest untested surface in the project. Full E2E testing is expensive (requires Streamlit server), but **unit testing the data transformation logic** within views is feasible.

Most views follow a pattern: load data -> transform -> render. The transform step can be extracted and tested. For example, `analysis_comparison.py` computes deltas, alignment scores, and category breakdowns — all pure functions that can be tested without Streamlit.

**Proposed approach:**
1. Extract data transformation functions from views into `dashboard/utils/transforms/`
2. Write pytest tests for these functions using fixture data from `tests/fixtures/`
3. Keep rendering logic in views untested (acceptable — Streamlit rendering is declarative)

This gives ~60-70% effective coverage of the dashboard logic without any Streamlit testing infrastructure.

### 2.3 Cross-Repo Task Templates for 10Figure

The 10Figure PRD's biggest blocker is the open question about cross-repo task design. Propose resolving it with a concrete template:

**Cross-repo task structure:**
```
cross_repo_task_01/
  instruction.md      # References symbols in both repos
  task.toml           # Lists both repo paths
  task.yaml           # Defines expected changes in BOTH repos
  tests/
    test.sh           # Validates changes across both repos
    expected_changes.json  # Two-repo change spec
  environment/
    Dockerfile         # Mounts both repos from base image
```

**Scoring:** Separate scores per repo, weighted by change complexity. A task touching 3 files in Kubernetes and 1 file in Django would weight Kubernetes 0.75 and Django 0.25. This avoids the "easy repo inflates score" problem.

---

## 3. Research Recommendations

### 3.1 Structured Exception Handling in Statistical Libraries

The bare-except pattern in `statistical_analyzer.py` mirrors a common problem in scientific Python: scipy functions raise diverse exceptions depending on input data shape. The library `uncertainties` (Eric O. Lebigot) provides error-propagating wrappers that handle degenerate cases gracefully. More practically, wrapping scipy calls in a `safe_stat_test()` helper that catches `ValueError`, `ZeroDivisionError`, and `LinAlgError` specifically would eliminate all 5 bare excepts while preserving the fallback behavior for legitimate edge cases.

### 3.2 Streamlit Testing with `streamlit.testing`

Streamlit v1.28+ ships `streamlit.testing.v1` (AppTest), a headless testing framework that doesn't require a running server. This would make `test_dashboard_e2e.py` CI-compatible and enable regression tests for view rendering. The API:

```python
from streamlit.testing.v1 import AppTest
at = AppTest.from_file("dashboard/app.py")
at.run()
assert not at.exception
```

This is lower-lift than extracting transform functions and provides true integration coverage.

### 3.3 Pre-Commit Hook for Bare Except Detection

Add a `ruff` check to the pre-commit hook: rule `E722` (bare except) catches exactly the 11 call sites identified above. The project already has a pre-commit hook for secret detection; adding ruff extends it to code quality without a separate CI step.

```yaml
# .pre-commit-config.yaml
- repo: https://github.com/astral-sh/ruff-pre-commit
  hooks:
    - id: ruff
      args: [--select=E722, --fix]
```

### 3.4 Benchmark Trace Attribution via Embedding Similarity

The Jan 29 report recommended tool usage correlation; the Jan 30 report proposed a comparison pipeline. Neither addressed the **attribution** problem: given an MCP tool call that returned content X, did the agent actually use X in its solution?

Recent work on retrieval attribution (ALCE, 2024; AttrScore, 2025) provides lightweight methods: embed both the tool output chunk and the solution text with a sentence transformer (e.g., `all-MiniLM-L6-v2`), compute pairwise cosine similarity, and flag tool calls where no solution segment exceeds a 0.7 similarity threshold as "unused." This converts the binary "MCP was available" into a continuous "MCP was utilized" metric — a stronger signal for MCP value assessment.

---

## 4. Recommended Next Feature

### 10Figure Cross-Repo Benchmark Tasks (4-6 new tasks)

**Problem:** The 10Figure benchmark currently has 4 tasks, all targeting a single repository (Kubernetes). The benchmark's stated purpose — measuring whether MCP tools help with cross-repository understanding — cannot be evaluated because no cross-repo tasks exist. The PRD (`tasks/prd-10figure-benchmark-expansion.md`) has been written but is blocked on 4 unresolved design questions.

**Why this is the most impactful feature:**

1. **Cross-repo is MCP's strongest differentiator.** Sourcegraph's value proposition is cross-repo code intelligence. The Jan 25 benchmark results showed 0% delta on SWE-bench (single-repo) and only +0.9% on LoCoBench (synthetic). Cross-repo tasks are where the delta *should* be largest — but we can't measure it yet.

2. **The infrastructure is ready.** The base Docker image, task generator (`scripts/gen_harbor_tasks.py`), and Harbor adapter framework all exist. The remaining work is task *design*, not engineering.

3. **It unblocks the comparison pipeline.** The Jan 30 report recommended a statistical comparison pipeline as the top feature — but running it on 4 single-repo Kubernetes tasks won't produce meaningful results. Cross-repo tasks provide the input data that makes statistical comparison worthwhile.

4. **Concrete deliverable for the project's thesis.** The project exists to answer "does Sourcegraph code intelligence improve agent performance?" Without cross-repo tasks, the answer is "marginally, on tasks that don't require cross-repo understanding" — which is an uninteresting finding.

**What to build:**

1. **Resolve the 4 open PRD questions:**
   - Commit SHAs: Pin to the commits already mirrored in `sg-benchmarks` org
   - Cross-repo patches: Produce one combined diff with repo prefixes (like `git format-patch` with submodules)
   - Docker image size: Use multi-stage build — full repos in build stage, only relevant source trees in runtime
   - Scoring: Per-repo weighted scoring (see Section 2.3 above)

2. **Design 4 cross-repo tasks spanning these categories:**
   - **API contract tracing** (Kubernetes + Envoy): Trace how a K8s API field propagates through Envoy's proxy config. Agent must find definitions in both repos.
   - **Dependency impact analysis** (Django + TensorFlow): Given a Django model change, identify TensorFlow serving code that reads that model format.
   - **Cross-project bug localization** (Kubernetes + Django): A bug manifests in K8s but the root cause is in a shared utility pattern also present in Django. Agent must identify the pattern across repos.
   - **Multi-repo refactor** (Envoy + Kubernetes): Rename a protobuf field used in both projects. Agent must find and update all references.

3. **Build and validate:**
   - Generate task TOMLs and expected_changes.json for each
   - Build multi-repo base image
   - Run nop agent (framework validation), then baseline agent, then MCP agent
   - Verify scoring produces differentiated results

**Files to create/modify:**
- `benchmarks/10figure/cross_api_tracing_01/` through `cross_refactor_01/` (4 new task directories)
- `benchmarks/10figure/base/Dockerfile` (multi-stage build for size optimization)
- `benchmarks/10figure/templates/cross_repo_test.sh.j2` (cross-repo validation template)
- `tasks/prd-10figure-benchmark-expansion.md` (resolve open questions, mark completed stories)

**Success criteria:** MCP agent achieves measurably higher reward (>15% delta, p<0.05 on bootstrap test) on cross-repo tasks compared to baseline.
