# Nightly Research Report — 2026-01-30

Previous report (2026-01-29) covered: LoCoBench verifier weaknesses, agent variant gaps, dashboard missing views, scoring methodology. This report focuses on **new findings** not previously surfaced.

---

## 1. Code & Architecture Review

### 1.1 `run_benchmark.py` is a Mock — Not a Real Runner

`runners/run_benchmark.py` lines 114-140 return **hardcoded fake data** instead of executing real tasks:

```python
result = {
    "success": True,          # always True
    "tokens_input": 5000,     # fake
    "tokens_output": 2000,    # fake
    "search_queries": 0 if self.agent_name == "claude-baseline" else 3,  # fake
}
```

Agent classes are imported (lines 28-29) but never instantiated. Any code path that calls `run_benchmark.py` directly produces completely invalid results. The real execution path goes through Harbor CLI (`runners/harbor_benchmark.sh`), so this file is dead code masquerading as a runner. It should be **deleted or clearly marked as a scaffold** to prevent accidental use.

### 1.2 Lifecycle Pipeline Silently Swallows Phase Failures

`src/benchmark/lifecycle_pipeline.py` lines 117-138: if the creation phase fails (e.g., repo not cloned), the validation phase runs anyway on incomplete state. Failures are recorded in the manifest but **never halt execution**. The `run()` method (line 95) returns manifests regardless of failure count. Downstream processes work with incomplete benchmarks without any signal that something went wrong.

**Fix:** Add early-exit logic after creation phase:
```python
if any(r["status"] == "failed" for r in creation_results):
    raise PipelineError(f"Creation failed for {benchmark_id}")
```

### 1.3 Systematic `dict.get()` None Bug Across Runners

The CLAUDE.md documents this pattern but it's **still unfixed** in runner code:

| File | Line | Code |
|------|------|------|
| `runners/aggregator.py` | 52 | `data.get('verifier_result', {})` |
| `runners/compare_results.py` | 46 | `data.get('verifier_result', {})` |
| `src/ingest/database.py` | 307 | `result.verifier_result.reward` truthy check |

When Harbor result.json contains `"verifier_result": null`, these crash with `AttributeError: 'NoneType' object has no attribute 'get'`. The fix is `data.get('verifier_result') or {}` everywhere.

### 1.4 Harbor Parser Has 5 Unordered Pass/Fail Inference Paths

`src/ingest/harbor_parser.py` lines 294-316 determine task success through 5 different code paths with no defined precedence:
1. `reward["reward"] > 0`
2. Explicit `passed` field
3. `mrr > 0`
4. File-based reward override
5. `reward.txt` raw content

A result.json with both `"reward": 0` and `"passed": true` produces different outcomes depending on which field is checked first. This should be a **single, documented decision function** with clear priority order.

### 1.5 Stale Model References in 8 Files

The project standard is `anthropic/claude-haiku-4-5-20251001`, but old models are hardcoded as defaults:

| File | Line | Stale Default |
|------|------|---------------|
| `src/evaluation/agent_judge.py` | 206 | `claude-sonnet-4-20250514` |
| `src/evaluation/agent_judge_verifier.py` | 43 | `claude-sonnet-4-20250514` |
| `src/adapters/base_templates.py` | 349 | `claude-sonnet-4-20250514` |
| `dashboard/utils/judge_config.py` | 23 | `claude-sonnet-4-20250514` |
| `dashboard/views/analysis_llm_judge.py` | 123 | `claude-opus-4-20250514` |
| `dashboard/views/run_results.py` | 2894 | `claude-sonnet-4-5-20251022` |
| `dashboard/views/run_triggers.py` | 233 | `claude-sonnet-4-5-20250929` |

**Note:** Using a stronger model for LLM judge evaluation is a valid design choice (Sonnet/Opus for judge quality). But the inconsistency should be **intentional and documented**, not accidental. Consider a `MODEL_DEFAULTS` constant dict in a single config module.

### 1.6 Hardcoded User Paths Still Present

| File | Path | Issue |
|------|------|-------|
| `scripts/ralph/verify_fixes.py:22` | `/home/stephanie_jarmak/CodeContextBench/...` | Wrong user, wrong machine |
| `scripts/ralph/orchestrate_fixes.py:37` | `/home/stephanie_jarmak/CodeContextBench/...` | Same |
| `scripts/compound/toggle.sh:12` | `/Users/sjarmak/Library/LaunchAgents/...` | Should use `$HOME` |
| `scripts/close_beads.py:38` | `/Users/sjarmak/CodeContextBench/.beads/...` | Should use relative path |

The ralph scripts reference a different machine entirely (`/home/stephanie_jarmak`) and will fail silently.

### 1.7 Duplicate MCP Config Upload in Agent Setup

Both `claude_baseline_agent.py` and `mcp_variants.py` upload `.mcp.json` to **two** locations (`/app/.mcp.json` and `/root/.mcp.json`), but per CLAUDE.md, Harbor's `CLAUDE_CONFIG_DIR` resolves to `/logs/agent/sessions/.mcp.json`. The dual upload is cargo-culted from debugging sessions and adds confusion. Should upload to the documented path only.

### 1.8 Ingestion Orchestrator Has Dead Code in Job ID Extraction

`src/ingest/orchestrator.py` lines 216-224:
```python
def _extract_job_id(self, result_file: Path) -> Optional[str]:
    if result_file.parent.parent.name == "runs":
        return result_file.parent.name
    return result_file.parent.name  # Fallback: identical to above
```

Both branches return the same value. The conditional is dead code.

---

## 2. Feature & UX Improvements

### 2.1 Unified Result Comparison CLI Tool

Currently, comparing two experiment runs requires:
1. Manually identifying common tasks across runs
2. Running `runners/compare_results.py` (which lacks statistical significance testing)
3. Eyeballing deltas without confidence intervals

**Proposed improvement:** A `scripts/compare_experiments.py` CLI that:
- Takes two run directories as arguments
- Automatically finds common tasks (the Jan-25 "statistical artifact" issue)
- Computes paired Wilcoxon signed-rank test for reward deltas
- Reports effect size (Cohen's d) and confidence intervals
- Outputs a markdown summary table suitable for pasting into reports
- Flags tasks where the delta exceeds a configurable threshold

This directly addresses the CLAUDE.md learning: "When baseline and MCP runs have different task sets, raw mean reward comparisons produce misleading deltas."

### 2.2 Model Constants Module

Create `src/config/models.py` with:
```python
AGENT_MODEL = "anthropic/claude-haiku-4-5-20251001"
JUDGE_MODEL = "anthropic/claude-sonnet-4-20250514"  # Intentionally stronger for judge
MODEL_PRICING = { ... }  # Single source of truth
```

All files currently hardcoding model strings would import from this module. Prevents the 8-file inconsistency problem from recurring.

### 2.3 Reward Extraction as a Single Function

Extract harbor_parser.py's 5 pass/fail paths into one documented function:
```python
def determine_task_success(verifier_result: dict) -> tuple[bool, float, str]:
    """Returns (passed, reward_value, source_field) with documented priority."""
```

All consumers (aggregator, compare_results, database, dashboard) call this instead of reimplementing reward logic.

---

## 3. Research Recommendations

### 3.1 Adopt Pairwise Bootstrap for Statistical Comparison

The current comparison infrastructure computes raw deltas without significance testing. The **pairwise bootstrap** method (used by SacreBLEU for MT evaluation) is well-suited:
- Handles non-normal distributions (reward scores cluster at 0 and 1)
- Works with small N (50 tasks is sufficient)
- Provides confidence intervals directly
- Implementation: ~50 lines with numpy, or use `scipy.stats.bootstrap`

This is lower-effort than the paired t-test suggested in the previous report and more statistically appropriate for bounded scores.

### 3.2 Consider EvalPlus-Style Mutation Testing for Code Modification Tasks

The previous report noted that LoCoBench code modification tasks lack execution-based verification. **EvalPlus** (NTU, 2023-2025) augments pass/fail tests with mutation-generated edge cases. For LoCoBench's synthetic projects:
- Generate ground-truth patches for code modification tasks
- Apply the agent's diff to a clean project copy
- Run the project's test suite (synthetic projects have tests in `data/generated/`)
- Score based on test pass rate, not text similarity

This converts ~20% of LoCoBench tasks from text-matching to execution-based evaluation without writing new test cases.

### 3.3 Tool Usage Attribution via Trace Analysis

The core research question — "does MCP help?" — requires linking specific tool calls to solution quality. Current trace parsing (`src/benchmark/trace_parser.py`) extracts tool calls but doesn't correlate them with solution content.

**Proposed approach:** For each MCP tool call in the trace, extract the returned content. Then measure whether that content appears (verbatim or paraphrased) in the agent's solution. This gives a **tool utilization rate**: of N MCP calls, how many actually contributed to the answer? A high MCP call count with low utilization suggests the agent is using MCP indiscriminately, not strategically.

Libraries: `rouge-score` for paraphrase detection, or embed both tool output and solution chunks with a sentence transformer and measure cosine similarity above threshold.

### 3.4 Replace Keyword Verifier with Lightweight Judge (Reinforcing Previous Report)

The previous report recommended this as P0. Reinforcing with a specific implementation path:
- Use the existing `src/benchmark/llm_judge.py` `EnhancedLLMJudge` class
- Call it from `verify.py` with the task's `evaluation_criteria` and `ground_truth` from the scenario JSON
- Use 3-point scoring (already proven more reliable in CLAUDE.md learnings)
- Fallback to keyword scoring if API call fails (network issues in container)
- Estimated change: ~40 lines in `verify.py`, 0 new dependencies

---

## 4. Recommended Next Feature

### Unified Experiment Comparison Pipeline with Statistical Rigor

**Problem:** The project's core deliverable is comparing agent performance with and without MCP tools. But the comparison infrastructure is fragmented, statistically naive, and produces misleading results (documented in CLAUDE.md: "raw mean reward comparisons produce misleading deltas").

**What to build:** A single `compare_experiments` module that:

1. **Input:** Two experiment directories (baseline and treatment)
2. **Task alignment:** Automatically match tasks across runs by task ID, filtering to the common set. Report how many tasks were excluded and why.
3. **Reward normalization:** Apply per-benchmark normalization (0-1 scale) so cross-benchmark aggregation is valid. Currently big_code_mcp, LoCoBench, and SWE-bench use incompatible scales.
4. **Statistical testing:** Pairwise bootstrap with 10,000 resamples. Report: mean delta, 95% CI, p-value, effect size (Cohen's d).
5. **Per-category breakdown:** Group results by task category (architectural_understanding, bug_investigation, etc.) since MCP value likely varies by task type.
6. **Tool usage correlation:** For MCP runs, correlate number of tool calls with reward delta per task. Scatter plot + Spearman correlation coefficient.
7. **Output:** Markdown report + JSON data suitable for dashboard ingestion.

**Why this is the most impactful feature:**
- Every benchmark run currently requires manual analysis to draw conclusions
- The Jan-25 results showed 0% delta on common SWE-bench tasks but +0.9% on LoCoBench — but without confidence intervals, we can't distinguish signal from noise
- This is the missing link between "run benchmarks" and "make research claims"
- Enables the nightly compound loop to automatically assess whether code changes improved agent performance

**Files to create/modify:**
- `src/analysis/experiment_comparator.py` (new, ~300 lines) — core comparison logic
- `scripts/compare_experiments.py` (new, ~100 lines) — CLI wrapper
- `runners/compare_results.py` (deprecate or redirect to new module)
- `dashboard/views/analysis_comparison.py` (extend to consume new JSON output)

**Dependencies:** `scipy` (already in requirements), `numpy` (already available).

---

## 5. Quick-Fix Summary

Items that can be resolved in minutes, not sessions:

| Fix | File | Effort |
|-----|------|--------|
| Delete or mark `run_benchmark.py` as scaffold | `runners/run_benchmark.py` | 5 min |
| Fix `dict.get() or {}` pattern in 3 files | aggregator.py, compare_results.py, database.py | 15 min |
| Remove dead code in job ID extraction | `src/ingest/orchestrator.py:216-224` | 5 min |
| Update ralph script paths from `/home/stephanie_jarmak` | `scripts/ralph/verify_fixes.py`, `orchestrate_fixes.py` | 10 min |
| Replace hardcoded paths in compound scripts with `$HOME` | `scripts/compound/toggle.sh` | 5 min |
| Add early-exit on creation phase failure | `src/benchmark/lifecycle_pipeline.py` | 15 min |
