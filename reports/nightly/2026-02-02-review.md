# Nightly Research Report — 2026-02-02

Previous reports covered: LoCoBench verifier weaknesses, agent variant gaps, dashboard missing views (Jan 29); mock runner, lifecycle pipeline failures, dict.get() None bug, harbor parser pass/fail paths, stale model refs, hardcoded paths, dead code (Jan 30); bare except blocks, shell injection, test suite diagnostic bloat, Ralph archive gaps, 10Figure missing cross-repo tasks (Jan 31). This report focuses on **new findings** and **progress assessment** since the last report.

---

## 0. Progress Since Jan 31

### Unified Experiment Comparison Pipeline — MERGED (PR #3)

The Jan 30 report's #1 recommended feature — a statistical comparison pipeline — was fully implemented and merged as PR #3 (`ralph/unified-experiment-comparison`). Delivered:

- `src/analysis/experiment_comparator.py` (1,147 lines): Bootstrap testing, confidence intervals, Cohen's d, task alignment, per-category breakdown, tool usage correlation
- `tests/test_experiment_comparator.py` (1,904 lines, 109 tests)
- `scripts/compare_experiments.py`: CLI wrapper
- `runners/compare_results.py`: Deprecated with migration docstring

This is high-quality code — zero bare excepts, proper edge-case handling, full test coverage. Sets a good pattern for future statistical modules.

### Two Ralph Branches Remain Unmerged

| Branch | Commits Ahead | Stories Done | Status |
|--------|--------------|-------------|--------|
| `ralph/10figure-cross-repo-tasks` | 10 | 9/11 (2 blocked on Harbor) | 4 cross-repo tasks ready, 80 smoke tests pass |
| `ralph/benchmark-trace-reviewer` | 2 | 25/27 | GUI-driven cost/failure/timeseries analysis configs |

Both are feature-complete for their unblocked stories. The 10figure branch contains the Jan 31 report's recommended next feature (cross-repo tasks). Neither has been reviewed or merged.

---

## 1. Code & Architecture Review

### 1.1 Dashboard Dependencies Are Critically Incomplete

`dashboard/requirements.txt` declares only 4 packages:

```
streamlit>=1.29.0
plotly>=5.18.0
pandas>=2.1.0
scipy>=1.11.0
```

Three undeclared dependencies will cause `ImportError` on a fresh install:

| Import | Used By | Package |
|--------|---------|---------|
| `import yaml` | `dashboard/views/run_triggers.py:130` | `pyyaml` |
| `import psutil` | `dashboard/utils/run_tracker.py:8` | `psutil` |
| `import anthropic` | `dashboard/utils/judge_test_prompt.py:396`, `src/benchmark/llm_judge.py:25` | `anthropic` |

A fresh `pip install -r dashboard/requirements.txt && streamlit run dashboard/app.py` will fail. This is a blocking issue for any new contributor.

The `anthropic` SDK is particularly risky without version pinning — breaking changes between minor versions (parameter renames, return type changes) have occurred regularly throughout 2025-2026.

**Recommendation:** Add `pyyaml>=6.0`, `psutil>=5.9.0`, `anthropic>=0.27.0,<1.0.0` to `dashboard/requirements.txt`.

### 1.2 Untracked Production Data at Risk of Accidental Loss

Two untracked files contain production-grade work:

**`data/selected_benchmark_tasks.json`** (2,332 lines): 125 stratified-sampled tasks with enriched metadata (SDLC phase, MCP benefit score, difficulty, language). Generated by a script with random sampling — if re-run with a different seed, historical analysis becomes irreproducible.

**`dashboard/utils/ccb_task_registry.py`** (115 lines): Fully implemented loader with `@st.cache_data` decorator, `CCBTaskMetadata` dataclass, and error handling. Not imported by any committed code — orphaned work-in-progress.

**Risk:** A `git clean -fd` or new clone loses both files permanently. The task selection methodology encoded in the JSON cannot be reconstructed without the exact random seed.

**Recommendation:** Either commit both files (preferred — they're production-ready), or delete them if the task selection approach has been superseded.

### 1.3 Agent Prompt Contradiction Between Strategic and Focused Variants

`agents/mcp_variants.py` defines two Deep Search variants with contradictory guidance:

**StrategicDeepSearchAgent** (explicit "when NOT to use"):
> "Do NOT use Deep Search for every small question — leverage already-gathered context"

**DeepSearchFocusedAgent** (explicit "ALWAYS use first"):
> "ALWAYS use Deep Search BEFORE any local search tools when understanding code architecture, finding all usages, answering 'where is X used?'"

The intent is to test different usage frequencies, but the prompts don't just vary the *threshold* — they give contradictory *rules*. An agent following the Strategic prompt would correctly skip a Deep Search call after already gathering context. The same agent following the Focused prompt would make the call anyway because the ALWAYS rule overrides context awareness.

This means performance differences between the two variants conflate **tool usage frequency** with **agent prompt-following quality**. If the Focused variant scores lower, is it because more Deep Search calls hurt, or because contradictory rules confused the agent?

**Recommendation:** Rewrite both prompts with the same rule structure, varying only the threshold parameter: "Use Deep Search when your confidence in having sufficient context is below X%" (Strategic: 70%, Focused: 30%).

### 1.4 Cost Calculator Missing Haiku 4.5 Alias and Using Haiku 3.5 Pricing

`src/benchmark/cost_calculator.py:75-80` prices `anthropic/claude-haiku-4-5-20251001` at `$0.80/MTok input`. This is the Haiku 3.5 price. The model alias at line 111 maps `claude-haiku-4-5` correctly, but:

1. The pricing for Haiku 4.5 should be verified against current Anthropic pricing. The code reuses the Haiku 3.5 price without indicating whether this was intentional.
2. No `claude-haiku-4-5-20251001` alias (without provider prefix) exists in `ANTHROPIC_PRICING` dict — only the `anthropic/` prefixed version. If Harbor passes the model name without prefix, `get_model_pricing()` returns `None` and cost is reported as $0.

**Recommendation:** Add the unprefixed key `"claude-haiku-4-5-20251001"` to `ANTHROPIC_PRICING` (matching the pattern used for Opus and Sonnet), and add a date comment for when pricing was last verified.

### 1.5 Database Schema Has No Migration Tracking

`src/ingest/database.py` applies schema migrations via ad-hoc `PRAGMA table_info()` checks on every connection:

```python
cursor.execute("PRAGMA table_info(harbor_results)")
columns = [row[1] for row in cursor.fetchall()]
if "task_language" not in columns:
    cursor.execute("ALTER TABLE harbor_results ADD COLUMN task_language TEXT DEFAULT 'unknown'")
```

There is no `schema_migrations` table tracking which migrations have run. Problems:
- Migrations execute on every DB open (adds latency)
- If a migration partially fails (disk full, interrupted), the database enters an undefined state with no recovery path
- No way to know which schema version a given database file is at

The new `experiment_comparator.py` avoids this entirely by using file-based JSON output. But the dashboard still reads from SQLite via `database.py`.

---

## 2. Feature & UX Improvements

### 2.1 Task Registry Integration for Benchmark Selection

The untracked `ccb_task_registry.py` and `selected_benchmark_tasks.json` implement a useful but disconnected feature: enriched task metadata with MCP benefit scores. If committed and wired into the dashboard, this enables:

- **Task filtering by MCP benefit potential**: Show only tasks where MCP is expected to help (benefit_score > 0.7), enabling targeted analysis of where the tool actually delivers value
- **SDLC phase breakdown**: Compare agent performance on "bug fix" vs "feature implementation" vs "architectural analysis" tasks — more actionable than raw category labels
- **Benchmark selection UI**: Instead of running all 125 tasks, select a subset optimized for a specific research question (e.g., "only cross-repo tasks where MCP should help most")

**Sketch:** Add a "Task Selection" panel to the Experiment Runner view that loads the registry, shows task metadata in a filterable table, and exports the selected task IDs for use with `harbor run --task-name`.

### 2.2 Branch Merge Dashboard

With 3 active Ralph branches (one merged, two pending), the project needs visibility into what work is ready to merge. Currently this requires manual `git log` comparisons.

**Sketch:** A lightweight "Development Status" section in the dashboard landing page:

| Branch | Stories | Tests | Status | Action |
|--------|---------|-------|--------|--------|
| unified-experiment-comparison | 11/11 | 109 pass | Merged (PR #3) | — |
| 10figure-cross-repo-tasks | 9/11 | 80 pass | Ready for review | Create PR |
| benchmark-trace-reviewer | 25/27 | — | Ready for review | Create PR |

This could be generated from `git log` + `bd list` output rather than requiring dashboard code.

### 2.3 Experiment Comparator Dashboard Integration

The newly merged `experiment_comparator.py` produces JSON and Markdown reports, but the dashboard doesn't consume them yet. `dashboard/views/analysis_comparison.py` still uses its own filesystem-scanning approach rather than the new statistical module.

**Sketch:** Wire `ExperimentComparison.from_directories()` into the comparison view:
- Replace the manual delta computation with `experiment_comparator.compare()`
- Display bootstrap confidence intervals alongside raw deltas
- Show the tool usage correlation scatter plot (Spearman rho)
- Add a "Statistical Summary" card with p-value, effect size, and interpretation

This connects the CLI pipeline to the visual dashboard, so the same statistical rigor is available in both interfaces.

---

## 3. Research Recommendations

### 3.1 Adopt a Centralized Configuration Module

The project has no centralized config. Paths like `~/evals/custom_agents/agents/claudecode/runs` are hardcoded in 15+ files across `dashboard/` and `scripts/`. Environment variable `CCB_EXTERNAL_RUNS_DIR` appears in 6 files independently.

The `experiment_comparator.py` module takes paths as constructor arguments — a good pattern. Extend this: create `src/config/paths.py` that resolves all project paths from environment variables with validated defaults. All modules import from there instead of independently calling `os.environ.get()`.

This also enables a `~/.codecontextbench/config.toml` for user-specific overrides without code changes — important for the "new contributor" experience.

### 3.2 Pairwise LLM Judge for Direct A/B Comparison

The Jan 29 report recommended pairwise comparison over absolute scoring. The newly merged comparator computes statistical significance on reward scores, but the rewards themselves still come from the per-task verifier (keyword matching for LoCoBench, test-based for SWE-bench).

For LoCoBench tasks where the verifier is weak (keyword F1), a **pairwise judge** would be more reliable: given two agent solutions and the ground truth, ask "which solution better addresses the evaluation criteria?" This sidesteps the absolute scoring calibration problem entirely.

The infrastructure now exists to pair solutions across experiments (`TaskAlignment` in `experiment_comparator.py`). Adding a pairwise judge call on top would give a second, independent comparison signal.

### 3.3 Regression Testing for Statistical Modules

The `experiment_comparator.py` has 109 tests — excellent. But the older `statistical_analyzer.py` (5 bare excepts, used by multiple dashboard views) has zero tests. And the bare excepts mean bugs manifest as "no significant difference" rather than errors.

**Recommendation:** Before touching `statistical_analyzer.py`'s bare excepts, add regression tests using known-output fixtures. This prevents the fix from changing behavior in unexpected ways. Use the `experiment_comparator.py` test patterns as a template.

### 3.4 SWE-bench Lite / Verified Subset for Faster Iteration

The Jan 25 benchmark results showed 0% delta on 29 common SWE-bench Pro tasks. Running the full SWE-bench suite is expensive (5-12 min per task × 5 agents × N tasks). For iteration speed, use SWE-bench Verified (human-validated, higher signal) and filter to tasks under 500 lines of context. This gives a faster feedback loop for agent prompt changes while maintaining evaluation quality.

---

## 4. Recommended Next Feature

### Dashboard Integration of the Experiment Comparator

**Problem:** The project's most significant recent deliverable — the unified experiment comparison pipeline (PR #3, 1,147 lines, 109 tests) — is only accessible via CLI. The dashboard's comparison view (`dashboard/views/analysis_comparison.py`) still uses its own ad-hoc filesystem scanning and manual delta computation without bootstrap confidence intervals, effect sizes, or tool usage correlation.

This means the dashboard shows weaker statistical analysis than what `scripts/compare_experiments.py` produces from the command line. Users viewing results in the dashboard get raw deltas; users running the CLI get proper significance testing. The two should be equivalent.

**What to build:**

1. **Wire `ExperimentComparison` into the comparison view.** Replace the manual delta computation in `analysis_comparison.py` with a call to `ExperimentComparator.compare()`. This immediately gives the dashboard: bootstrap CIs, Cohen's d, p-values, task alignment stats.

2. **Add a Statistical Summary card.** Show the headline numbers: "Baseline mean: 0.45, MCP mean: 0.47, delta: +0.02 [95% CI: -0.03, +0.07], p=0.42, effect size: 0.11 (negligible)." This is the single most informative view for the research question.

3. **Add the tool usage correlation scatter plot.** The comparator already computes Spearman rho between MCP call count and reward delta per task. Render this as a Plotly scatter with a trend line. This directly answers: "do more MCP calls correlate with better performance?"

4. **Add per-category breakdown table.** Group tasks by category (architectural_understanding, bug_investigation, etc.) and show the comparison stats for each group. MCP value likely varies by task type — this reveals which task categories benefit most.

5. **JSON report caching.** After the first comparison, save the JSON report to `.dashboard_cache/`. On subsequent page loads, display cached results instantly and offer a "Recompute" button.

**Why this is the most impactful next feature:**

- The comparison pipeline is already built and tested — this is integration work, not new algorithm development
- It directly improves the dashboard's analytical value from "raw deltas" to "statistically rigorous comparison"
- It connects the two halves of the project (CLI analysis + dashboard visualization) that currently operate independently
- Every future benchmark run benefits from this — it's a permanent multiplier on analysis quality

**Files to modify:**
- `dashboard/views/analysis_comparison.py` — import and call `ExperimentComparator`
- `dashboard/utils/comparison_config.py` — add config options for bootstrap parameters (n_bootstrap, confidence_level)
- New: `dashboard/utils/comparator_cache.py` (~50 lines) — JSON cache layer

**Dependencies:** None new. `scipy` and `numpy` are already available. The comparator module has no external dependencies beyond what's installed.

**Success criteria:** Dashboard comparison view shows identical statistical results to `scripts/compare_experiments.py --format markdown` for the same input directories.
