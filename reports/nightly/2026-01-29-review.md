# Nightly Research Report — 2026-01-29

## 1. Benchmark Setup Review

### Adapter Configs

**LoCoBench-Agent** (`benchmarks/locobench_agent/adapter.py`) is the most actively iterated benchmark. The Jan-25 run surfaced several issues that were partially addressed:

- **Verifier weights updated** — keyword_overlap reduced from 0.50 to 0.35, file_references raised to 0.30, code_blocks to 0.25. This is an improvement, but the verifier still has fundamental limitations (see Section 3).
- **Model upgrade to Opus 4.5** — The comparison configs (`locobench_50_tasks_comparison.sh/yaml`) were updated from `claude-sonnet-4-20250514` to `claude-opus-4-5-20251101`. This is fine for capability ceiling testing, but the project-wide standard is `claude-haiku-4-5-20251001` per `AGENTS.md:24`. Running different models across benchmarks makes cross-benchmark comparison unreliable. **Recommendation:** Run LoCoBench with Haiku 4.5 as well for apples-to-apples comparison with big_code_mcp and github_mined results.
- **CLAUDE.md template** was created to inject Sourcegraph skill content because skills don't load in headless mode (`"skills": []` in agent init). This is a pragmatic workaround, but it means the MCP agent gets additional prompt context the baseline doesn't. This is a confound — the CLAUDE.md improvements may inflate MCP agent scores independent of actual tool usage.

**Stale references identified:**

| File | Issue |
|------|-------|
| `src/benchmark/cost_calculator.py` | Legacy Claude 3 model pricing still defined (harmless but noisy) |
| `HANDOFF_SUMMARY.md:118` | MCP example uses `claude-sonnet-4-20250514`, inconsistent with the config update to Opus 4.5 |
| `benchmarks/locobench_agent/adapter.py` | Hard-coded UID 1001/GID 1002 for `stephanie_jarmak`; breaks on any other machine |
| `README.md:233-234` | GCP VM sections still marked TODO — no infrastructure-as-code exists |

### Agent Variant Differentiation

The five agent variants (`agents/mcp_variants.py`) are well-designed for ablation:

| Variant | Independent Variable | Control |
|---------|---------------------|---------|
| Baseline | No MCP at all | Clean control |
| Strategic Deep Search | Selective DS at key moments | Prompt-guided DS frequency |
| Deep Search Focused | Aggressive DS-first | DS call volume |
| No Deep Search | Keyword/NLS only, no DS | Isolates DS value |
| Full Toolkit | All tools, neutral prompt | Agent's natural tool choice |

**Gap:** There's no variant that tests **MCP with read-only tools** (sg_read_file only, no search). This would isolate whether the value comes from search capabilities vs. just having indexed file access. Consider adding a `ReadOnlyMCPAgent`.

**Gap:** The Strategic vs. Focused distinction is entirely prompt-based — both have identical tool access. Without measuring actual Deep Search call counts per task and correlating with score delta, we can't validate whether the prompt guidance actually changes behavior. The dashboard doesn't currently surface per-task tool call breakdowns by agent variant.

---

## 2. Dashboard & UI Improvements

### Current State

The dashboard (`dashboard/`) is extensive with 15+ view modules covering:
- Experiment comparison, statistical significance, time-series trends
- Cost analysis, failure patterns, LLM judge assessment
- Trace viewer (timeline, diffs, file tree, cards, filters)
- Run triggers, benchmark manager

### Missing Views & Features

**1. Per-Task Tool Usage Drill-Down (HIGH PRIORITY)**

No view currently shows: "For task X, agent A made 3 Deep Search calls and scored 0.7; agent B made 0 and scored 0.3." This is the core question CodeContextBench exists to answer.

*Sketch:* Add a `views/tool_impact.py` view with:
- Scatter plot: X = number of MCP calls, Y = task reward, color = agent variant
- Table: task_id | baseline_reward | mcp_reward | delta | ds_calls | nls_calls | total_mcp
- Filter: by benchmark, language, difficulty, task category
- Drill-down: click task → see full tool usage timeline

**2. Run Triggering Flow (MEDIUM PRIORITY)**

`views/run_triggers.py` exists but there's no evidence it actually triggers runs. The Priority 5 item in `reports/priorities.md` describes SSH-based remote triggering.

*Sketch:* Minimal viable flow:
1. Select benchmark + agent variant from dropdowns
2. Show estimated cost (tokens × price from `cost_calculator.py`)
3. Button: "Start Run" → executes `harbor run` via subprocess or SSH
4. Progress: poll `jobs/` directory for new result.json files
5. Auto-refresh results table when run completes

**3. Verifier Diagnostic View (HIGH PRIORITY)**

When a task scores low, there's no way to understand *why* from the dashboard. The verifier outputs component scores (keyword_overlap, file_references, code_blocks, length_score) but these aren't surfaced.

*Sketch:* Add to task detail view:
- Bar chart of 4 verifier component scores
- Side-by-side: agent solution excerpt vs. ground truth keywords
- Highlight: which ground truth keywords were matched/missed
- Link to raw solution.md and ground_truth.json

**4. A/B Comparison Overlay (MEDIUM PRIORITY)**

`views/comparison.py` shows aggregate stats. Missing: paired per-task comparison.

*Sketch:*
- Select two agents → show per-task delta table (reward_A - reward_B)
- Highlight tasks where delta > 0.2 (MCP clearly helped) or < -0.2 (MCP hurt)
- Statistical test: paired t-test across tasks, show p-value and effect size

**5. Experiment History Timeline**

`views/analysis_timeseries.py` exists but the time-series analyzer needs multiple experiments over time. There's no view showing: "Here's how our benchmark scores have changed across the last 10 runs."

---

## 3. Evaluation Quality

### Verifier Blind Spots (LoCoBench)

The LoCoBench verifier (`benchmarks/locobench_agent/templates/tests/verify.py`) has significant limitations:

**1. Keyword F1 is a poor proxy for correctness.**
The verifier computes F1 between extracted keywords from the solution and a JSON-serialized ground truth. This means:
- Paraphrasing is penalized (e.g., "function" vs "method" vs "callable")
- Structural understanding without exact terminology scores low
- JSON serialization artifacts (key names like `context_files`) pollute the keyword set — `json.dumps(ground_truth)` at line 134 includes dict keys as "keywords"

**Recommendation:** Replace keyword overlap with an LLM judge call. The infrastructure already exists in `src/analysis/llm_judge_analyzer.py` and `src/benchmark/llm_judge.py`. Use a rubric-based judge that scores against evaluation_criteria from the task metadata rather than keyword matching.

**2. Code block scoring is binary and task-agnostic.**
`check_code_blocks()` returns 1.0 for any code block, 0.5 for inline code, 0.0 for none. An analysis task that correctly identifies architectural patterns without code examples gets penalized 25% of its score. Conversely, a wrong answer with random code blocks gets full credit on this dimension.

**Recommendation:** Make code_blocks weight conditional on task_category. For "Architectural Understanding" tasks, reduce to 0.10. For "Cross-File Refactoring" tasks, increase to 0.35.

**3. File reference scoring rewards mentioning filenames, not understanding them.**
An agent that dumps `ls -la` output and lists every file gets a perfect file_references score without any actual analysis.

**Recommendation:** Weight file references by whether they appear in analytical context (near explanation text) vs. bare listings.

**4. No evaluation of code modification tasks.**
The NEXT_RUN_RECOMMENDATIONS.md mentions adding "IMPLEMENT the code changes directly in /app/project/" guidance, but the verifier only checks `/logs/agent/solution.md`. If the task requires code changes, the verifier should also diff the project directory against an expected patch.

### How Other Benchmarks Evaluate

**SWE-bench:** Uses test-based evaluation — the agent's code changes must pass fail-to-pass tests. This is the gold standard for correctness but requires curated test cases per task.

**SWE-bench Pro** (already in this project at `benchmarks/swebench_pro/`): Same test-based approach with multi-language support.

**SWE-Perf** (`benchmarks/sweperf/`): Runtime reduction metric — a continuous, objective measure. Well-designed: correctness-gated (tests must pass first), then measures optimization quality.

**TheAgentCompany (TAC)**: Checkpoint-based scoring with deterministic + LLM-based grading. The mixed approach handles tasks that can't be fully automated.

**Lesson:** The project's strongest verifiers (SWE-Perf, SWE-bench Pro) use **execution-based evaluation**. LoCoBench's text-matching verifier is the weakest link. Either:
1. Add execution-based verification for code modification LoCoBench tasks, or
2. Replace text matching with LLM judge scoring (lower engineering cost, still a major improvement)

### Scoring Improvements

1. **Normalize across benchmarks.** big_code_mcp, LoCoBench, github_mined, and SWE-Perf all use different scoring scales and methods. The dashboard should normalize to a common 0-1 scale with benchmark-specific calibration (e.g., oracle score = 0.70 for LoCoBench but 1.0 for SWE-bench).

2. **Add confidence intervals.** Single-run scores are noisy. Run each task N≥3 times and report mean ± stderr. The statistical analyzer supports this but the run configs default to `-n 1`.

3. **Track MCP tool ROI.** For each MCP call, measure: (a) latency added, (b) tokens consumed, (c) whether the information was used in the solution. This requires trace-level analysis linking tool calls to solution content.

---

## 4. Research Recommendations

### Recent Relevant Work

**Agent Evaluation Frameworks:**
- **SWE-bench Verified** (2024-2025): Human-validated subset with higher signal. Consider filtering swebench_pro tasks to verified-only instances for cleaner signal.
- **DevBench** (2024): Multi-role software development benchmark. Tests PM, architect, developer, QA roles separately. Could inspire role-specific evaluation for TAC tasks.
- **CORE-Bench** (2024): Computational reproducibility benchmark. Relevant for measuring whether agent solutions are deterministic/reproducible.

**Tool-Augmented Agent Evaluation:**
- **ToolBench / API-Bank**: Benchmarks for tool selection and usage. The pattern of measuring tool selection quality (did the agent pick the right tool?) vs. tool execution quality (did it use the tool well?) maps directly to the Strategic vs. Full Toolkit comparison.
- **WebArena / OSWorld**: Real-environment benchmarks. The Daytona VM approach in this project is aligned with this direction.

**Evaluation Methodology:**
- **LLM-as-Judge with calibration**: Use reference solutions to calibrate judge scoring. Compare judge scores against human ratings on a sample to establish judge reliability. The `judge_human_alignment.py` module in the dashboard suggests this was planned but may not be fully implemented.
- **Pairwise comparison > absolute scoring**: Instead of "rate this solution 0-1", ask "which solution is better, A or B?" This is more reliable for LLM judges and directly answers the MCP vs. baseline question.

### New Benchmark Task Ideas

1. **Cross-Repository Tasks:** Tasks requiring information from 2+ repositories (e.g., "update this library's client code in repo B to match the API change in repo A"). This is where MCP/Sourcegraph should excel — cross-repo search is a differentiator that local grep can't match.

2. **Codebase Navigation Tasks:** Pure information retrieval: "Find all callers of function X across the monorepo" or "What is the dependency chain from module A to module B?" Score based on recall of known call sites. This directly measures search tool quality.

3. **Stale Documentation Detection:** Given a codebase with intentionally outdated docs, the agent must identify discrepancies between code and documentation. MCP-enhanced agents should find more discrepancies by searching more broadly.

4. **Large-Scale Refactoring:** Tasks requiring changes across 10+ files with consistent patterns (rename a type, update an API signature). Tests whether MCP helps find all usage sites. Verifiable by diffing against a known-correct patch.

---

## 5. Action Items

### Quick Wins (1-2 sessions each)

| Priority | Item | Files | Impact |
|----------|------|-------|--------|
| **P0** | Replace LoCoBench keyword verifier with LLM judge call | `benchmarks/locobench_agent/templates/tests/verify.py`, `src/benchmark/llm_judge.py` | Fixes the biggest evaluation blind spot; infrastructure exists |
| **P0** | Run LoCoBench with Haiku 4.5 for cross-benchmark consistency | `configs_v2/examples/locobench_50_tasks_comparison.sh` | Enables apples-to-apples comparison |
| **P1** | Add per-task tool usage to dashboard | `dashboard/views/` (new view), `src/ingest/harbor_parser.py` | Answers the core research question |
| **P1** | Add verifier component scores to task detail view | `dashboard/utils/task_detail.py` | Immediate debugging value |
| **P2** | Clean stale model references in cost_calculator.py | `src/benchmark/cost_calculator.py` | Reduces confusion |
| **P2** | Fix hard-coded UID/GID in locobench adapter | `benchmarks/locobench_agent/adapter.py` | Portability |

### Larger Efforts (multi-session)

| Priority | Item | Scope | Impact |
|----------|------|-------|--------|
| **P1** | Implement paired per-task A/B comparison view | New dashboard view + statistical backend | Core analysis capability |
| **P1** | Add execution-based verification for LoCoBench code modification tasks | Verifier rewrite for modification task category | Correctness for ~20% of tasks |
| **P2** | Add cross-repository benchmark tasks | New benchmark suite design + adapter | Tests MCP's strongest differentiator |
| **P2** | Implement pairwise LLM judge (A vs B, not absolute scoring) | `src/benchmark/llm_judge.py` refactor | More reliable evaluation |
| **P3** | Add `ReadOnlyMCPAgent` variant | `agents/mcp_variants.py` | Finer-grained ablation |
| **P3** | Multi-run confidence intervals (N≥3 per task) | Run config + dashboard aggregation | Statistical rigor |
| **P4** | Remote run triggering from dashboard | `dashboard/views/run_triggers.py` | Workflow convenience |

### Immediate Next Session Recommendation

Start with **P0: Replace LoCoBench keyword verifier with LLM judge**. The current verifier produced 0% pass rate at ≥0.8 threshold despite agents writing substantive 1,900-word solutions. The keyword F1 approach demonstrably under-measures actual agent capability. The LLM judge infrastructure (`src/benchmark/llm_judge.py`, `src/analysis/llm_judge_analyzer.py`) already exists — wire it into the verify.py template with a fallback to keyword scoring if the API call fails.
